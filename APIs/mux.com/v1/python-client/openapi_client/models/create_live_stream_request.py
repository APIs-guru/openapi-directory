# coding: utf-8

"""
    Mux API

    Mux is how developers build online video. This API encompasses both Mux Video and Mux Data functionality to help you build your video-related projects better and faster than ever before.

    The version of the OpenAPI document: v1
    Contact: devex@mux.com
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional, Union
from typing_extensions import Annotated
from openapi_client.models.create_asset_request import CreateAssetRequest
from openapi_client.models.create_simulcast_target_request import CreateSimulcastTargetRequest
from openapi_client.models.live_stream_embedded_subtitle_settings import LiveStreamEmbeddedSubtitleSettings
from openapi_client.models.live_stream_generated_subtitle_settings import LiveStreamGeneratedSubtitleSettings
from openapi_client.models.playback_policy import PlaybackPolicy
from typing import Optional, Set
from typing_extensions import Self

class CreateLiveStreamRequest(BaseModel):
    """
    CreateLiveStreamRequest
    """ # noqa: E501
    audio_only: Optional[StrictBool] = Field(default=None, description="Force the live stream to only process the audio track when the value is set to true. Mux drops the video track if broadcasted.")
    embedded_subtitles: Optional[List[LiveStreamEmbeddedSubtitleSettings]] = Field(default=None, description="Describe the embedded closed caption contents of the incoming live stream.")
    generated_subtitles: Optional[List[LiveStreamGeneratedSubtitleSettings]] = Field(default=None, description="Configure the incoming live stream to include subtitles created with automatic speech recognition. Each Asset created from a live stream with `generated_subtitles` configured will automatically receive two text tracks. The first of these will have a `text_source` value of `generated_live`, and will be available with `ready` status as soon as the stream is live. The second text track will have a `text_source` value of `generated_live_final` and will contain subtitles with improved accuracy, timing, and formatting. However, `generated_live_final` tracks will not be available in `ready` status until the live stream ends. If an Asset has both `generated_live` and `generated_live_final` tracks that are `ready`, then only the `generated_live_final` track will be included during playback.")
    latency_mode: Optional[StrictStr] = Field(default=None, description="Latency is the time from when the streamer transmits a frame of video to when you see it in the player. Set this as an alternative to setting low latency or reduced latency flags. The Low Latency value is a beta feature. Read more here: https://mux.com/blog/introducing-low-latency-live-streaming/")
    low_latency: Optional[StrictBool] = Field(default=None, description="This field is deprecated. Please use `latency_mode` instead. Latency is the time from when the streamer transmits a frame of video to when you see it in the player. Setting this option will enable compatibility with the LL-HLS specification for low-latency streaming. This typically has lower latency than Reduced Latency streams, and cannot be combined with Reduced Latency.")
    max_continuous_duration: Optional[Annotated[int, Field(le=43200, strict=True, ge=60)]] = Field(default=43200, description="The time in seconds a live stream may be continuously active before being disconnected. Defaults to 12 hours.")
    new_asset_settings: Optional[CreateAssetRequest] = None
    passthrough: Optional[StrictStr] = None
    playback_policy: Optional[List[PlaybackPolicy]] = None
    reconnect_slate_url: Optional[StrictStr] = Field(default=None, description="The URL of the image file that Mux should download and use as slate media during interruptions of the live stream media. This file will be downloaded each time a new recorded asset is created from the live stream. If this is not set, the default slate media will be used.")
    reconnect_window: Optional[Union[Annotated[float, Field(le=1800, strict=True, ge=0)], Annotated[int, Field(le=1800, strict=True, ge=0)]]] = Field(default=60, description="When live streaming software disconnects from Mux, either intentionally or due to a drop in the network, the Reconnect Window is the time in seconds that Mux should wait for the streaming software to reconnect before considering the live stream finished and completing the recorded asset. Defaults to 60 seconds on the API if not specified.  If not specified directly, Standard Latency streams have a Reconnect Window of 60 seconds; Reduced and Low Latency streams have a default of 0 seconds, or no Reconnect Window. For that reason, we suggest specifying a value other than zero for Reduced and Low Latency streams.  Reduced and Low Latency streams with a Reconnect Window greater than zero will insert slate media into the recorded asset while waiting for the streaming software to reconnect or when there are brief interruptions in the live stream media. When using a Reconnect Window setting higher than 60 seconds with a Standard Latency stream, we highly recommend enabling slate with the `use_slate_for_standard_latency` option. ")
    reduced_latency: Optional[StrictBool] = Field(default=None, description="This field is deprecated. Please use `latency_mode` instead. Latency is the time from when the streamer transmits a frame of video to when you see it in the player. Set this if you want lower latency for your live stream. Read more here: https://mux.com/blog/reduced-latency-for-mux-live-streaming-now-available/")
    simulcast_targets: Optional[List[CreateSimulcastTargetRequest]] = None
    test: Optional[StrictBool] = Field(default=None, description="Marks the live stream as a test live stream when the value is set to true. A test live stream can help evaluate the Mux Video APIs without incurring any cost. There is no limit on number of test live streams created. Test live streams are watermarked with the Mux logo and limited to 5 minutes. The test live stream is disabled after the stream is active for 5 mins and the recorded asset also deleted after 24 hours.")
    use_slate_for_standard_latency: Optional[StrictBool] = Field(default=False, description="By default, Standard Latency live streams do not have slate media inserted while waiting for live streaming software to reconnect to Mux. Setting this to true enables slate insertion on a Standard Latency stream.")
    __properties: ClassVar[List[str]] = ["audio_only", "embedded_subtitles", "generated_subtitles", "latency_mode", "low_latency", "max_continuous_duration", "new_asset_settings", "passthrough", "playback_policy", "reconnect_slate_url", "reconnect_window", "reduced_latency", "simulcast_targets", "test", "use_slate_for_standard_latency"]

    @field_validator('latency_mode')
    def latency_mode_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['low', 'reduced', 'standard']):
            raise ValueError("must be one of enum values ('low', 'reduced', 'standard')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of CreateLiveStreamRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in embedded_subtitles (list)
        _items = []
        if self.embedded_subtitles:
            for _item_embedded_subtitles in self.embedded_subtitles:
                if _item_embedded_subtitles:
                    _items.append(_item_embedded_subtitles.to_dict())
            _dict['embedded_subtitles'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in generated_subtitles (list)
        _items = []
        if self.generated_subtitles:
            for _item_generated_subtitles in self.generated_subtitles:
                if _item_generated_subtitles:
                    _items.append(_item_generated_subtitles.to_dict())
            _dict['generated_subtitles'] = _items
        # override the default output from pydantic by calling `to_dict()` of new_asset_settings
        if self.new_asset_settings:
            _dict['new_asset_settings'] = self.new_asset_settings.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in simulcast_targets (list)
        _items = []
        if self.simulcast_targets:
            for _item_simulcast_targets in self.simulcast_targets:
                if _item_simulcast_targets:
                    _items.append(_item_simulcast_targets.to_dict())
            _dict['simulcast_targets'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of CreateLiveStreamRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "audio_only": obj.get("audio_only"),
            "embedded_subtitles": [LiveStreamEmbeddedSubtitleSettings.from_dict(_item) for _item in obj["embedded_subtitles"]] if obj.get("embedded_subtitles") is not None else None,
            "generated_subtitles": [LiveStreamGeneratedSubtitleSettings.from_dict(_item) for _item in obj["generated_subtitles"]] if obj.get("generated_subtitles") is not None else None,
            "latency_mode": obj.get("latency_mode"),
            "low_latency": obj.get("low_latency"),
            "max_continuous_duration": obj.get("max_continuous_duration") if obj.get("max_continuous_duration") is not None else 43200,
            "new_asset_settings": CreateAssetRequest.from_dict(obj["new_asset_settings"]) if obj.get("new_asset_settings") is not None else None,
            "passthrough": obj.get("passthrough"),
            "playback_policy": obj.get("playback_policy"),
            "reconnect_slate_url": obj.get("reconnect_slate_url"),
            "reconnect_window": obj.get("reconnect_window") if obj.get("reconnect_window") is not None else 60,
            "reduced_latency": obj.get("reduced_latency"),
            "simulcast_targets": [CreateSimulcastTargetRequest.from_dict(_item) for _item in obj["simulcast_targets"]] if obj.get("simulcast_targets") is not None else None,
            "test": obj.get("test"),
            "use_slate_for_standard_latency": obj.get("use_slate_for_standard_latency") if obj.get("use_slate_for_standard_latency") is not None else False
        })
        return _obj


