# coding: utf-8

"""
    Cloud Pub/Sub API

    Provides reliable, many-to-many, asynchronous messaging between applications. 

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from typing import Optional, Set
from typing_extensions import Self

class BigQueryConfig(BaseModel):
    """
    Configuration for a BigQuery subscription.
    """ # noqa: E501
    drop_unknown_fields: Optional[StrictBool] = Field(default=None, description="Optional. When true and use_topic_schema is true, any fields that are a part of the topic schema that are not part of the BigQuery table schema are dropped when writing to BigQuery. Otherwise, the schemas must be kept in sync and any messages with extra fields are not written and remain in the subscription's backlog.", alias="dropUnknownFields")
    service_account_email: Optional[StrictStr] = Field(default=None, description="Optional. The service account to use to write to BigQuery. The subscription creator or updater that specifies this field must have `iam.serviceAccounts.actAs` permission on the service account. If not specified, the Pub/Sub [service agent](https://cloud.google.com/iam/docs/service-agents), service-{project_number}@gcp-sa-pubsub.iam.gserviceaccount.com, is used.", alias="serviceAccountEmail")
    state: Optional[StrictStr] = Field(default=None, description="Output only. An output-only field that indicates whether or not the subscription can receive messages.")
    table: Optional[StrictStr] = Field(default=None, description="Optional. The name of the table to which to write data, of the form {projectId}.{datasetId}.{tableId}")
    use_table_schema: Optional[StrictBool] = Field(default=None, description="Optional. When true, use the BigQuery table's schema as the columns to write to in BigQuery. `use_table_schema` and `use_topic_schema` cannot be enabled at the same time.", alias="useTableSchema")
    use_topic_schema: Optional[StrictBool] = Field(default=None, description="Optional. When true, use the topic's schema as the columns to write to in BigQuery, if it exists. `use_topic_schema` and `use_table_schema` cannot be enabled at the same time.", alias="useTopicSchema")
    write_metadata: Optional[StrictBool] = Field(default=None, description="Optional. When true, write the subscription name, message_id, publish_time, attributes, and ordering_key to additional columns in the table. The subscription name, message_id, and publish_time fields are put in their own columns while all other message properties (other than data) are written to a JSON object in the attributes column.", alias="writeMetadata")
    __properties: ClassVar[List[str]] = ["dropUnknownFields", "serviceAccountEmail", "state", "table", "useTableSchema", "useTopicSchema", "writeMetadata"]

    @field_validator('state')
    def state_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['STATE_UNSPECIFIED', 'ACTIVE', 'PERMISSION_DENIED', 'NOT_FOUND', 'SCHEMA_MISMATCH', 'IN_TRANSIT_LOCATION_RESTRICTION']):
            raise ValueError("must be one of enum values ('STATE_UNSPECIFIED', 'ACTIVE', 'PERMISSION_DENIED', 'NOT_FOUND', 'SCHEMA_MISMATCH', 'IN_TRANSIT_LOCATION_RESTRICTION')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of BigQueryConfig from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "state",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of BigQueryConfig from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "dropUnknownFields": obj.get("dropUnknownFields"),
            "serviceAccountEmail": obj.get("serviceAccountEmail"),
            "state": obj.get("state"),
            "table": obj.get("table"),
            "useTableSchema": obj.get("useTableSchema"),
            "useTopicSchema": obj.get("useTopicSchema"),
            "writeMetadata": obj.get("writeMetadata")
        })
        return _obj


