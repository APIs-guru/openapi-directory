# coding: utf-8

"""
    Vertex AI API

    Train high-quality custom machine learning models with minimal machine learning expertise and effort.

    The version of the OpenAPI document: v1beta1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_cloud_aiplatform_v1beta1_encryption_spec import GoogleCloudAiplatformV1beta1EncryptionSpec
from openapi_client.models.google_cloud_aiplatform_v1beta1_gcs_destination import GoogleCloudAiplatformV1beta1GcsDestination
from openapi_client.models.google_cloud_aiplatform_v1beta1_model_deployment_monitoring_big_query_table import GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringBigQueryTable
from openapi_client.models.google_cloud_aiplatform_v1beta1_model_deployment_monitoring_job_latest_monitoring_pipeline_metadata import GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadata
from openapi_client.models.google_cloud_aiplatform_v1beta1_model_deployment_monitoring_objective_config import GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfig
from openapi_client.models.google_cloud_aiplatform_v1beta1_model_deployment_monitoring_schedule_config import GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfig
from openapi_client.models.google_cloud_aiplatform_v1beta1_model_monitoring_alert_config import GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfig
from openapi_client.models.google_cloud_aiplatform_v1beta1_sampling_strategy import GoogleCloudAiplatformV1beta1SamplingStrategy
from openapi_client.models.google_rpc_status import GoogleRpcStatus
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringJob(BaseModel):
    """
    Represents a job that runs periodically to monitor the deployed models in an endpoint. It will analyze the logged training & prediction data to detect any abnormal behaviors.
    """ # noqa: E501
    analysis_instance_schema_uri: Optional[StrictStr] = Field(default=None, description="YAML schema file uri describing the format of a single instance that you want Tensorflow Data Validation (TFDV) to analyze. If this field is empty, all the feature data types are inferred from predict_instance_schema_uri, meaning that TFDV will use the data in the exact format(data type) as prediction request/response. If there are any data type differences between predict instance and TFDV instance, this field can be used to override the schema. For models trained with Vertex AI, this field must be set as all the fields in predict instance formatted as string.", alias="analysisInstanceSchemaUri")
    bigquery_tables: Optional[List[GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringBigQueryTable]] = Field(default=None, description="Output only. The created bigquery tables for the job under customer project. Customer could do their own query & analysis. There could be 4 log tables in maximum: 1. Training data logging predict request/response 2. Serving data logging predict request/response", alias="bigqueryTables")
    create_time: Optional[StrictStr] = Field(default=None, description="Output only. Timestamp when this ModelDeploymentMonitoringJob was created.", alias="createTime")
    display_name: Optional[StrictStr] = Field(default=None, description="Required. The user-defined name of the ModelDeploymentMonitoringJob. The name can be up to 128 characters long and can consist of any UTF-8 characters. Display name of a ModelDeploymentMonitoringJob.", alias="displayName")
    enable_monitoring_pipeline_logs: Optional[StrictBool] = Field(default=None, description="If true, the scheduled monitoring pipeline logs are sent to Google Cloud Logging, including pipeline status and anomalies detected. Please note the logs incur cost, which are subject to [Cloud Logging pricing](https://cloud.google.com/logging#pricing).", alias="enableMonitoringPipelineLogs")
    encryption_spec: Optional[GoogleCloudAiplatformV1beta1EncryptionSpec] = Field(default=None, alias="encryptionSpec")
    endpoint: Optional[StrictStr] = Field(default=None, description="Required. Endpoint resource name. Format: `projects/{project}/locations/{location}/endpoints/{endpoint}`")
    error: Optional[GoogleRpcStatus] = None
    labels: Optional[Dict[str, StrictStr]] = Field(default=None, description="The labels with user-defined metadata to organize your ModelDeploymentMonitoringJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.")
    latest_monitoring_pipeline_metadata: Optional[GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadata] = Field(default=None, alias="latestMonitoringPipelineMetadata")
    log_ttl: Optional[StrictStr] = Field(default=None, description="The TTL of BigQuery tables in user projects which stores logs. A day is the basic unit of the TTL and we take the ceil of TTL/86400(a day). e.g. { second: 3600} indicates ttl = 1 day.", alias="logTtl")
    logging_sampling_strategy: Optional[GoogleCloudAiplatformV1beta1SamplingStrategy] = Field(default=None, alias="loggingSamplingStrategy")
    model_deployment_monitoring_objective_configs: Optional[List[GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfig]] = Field(default=None, description="Required. The config for monitoring objectives. This is a per DeployedModel config. Each DeployedModel needs to be configured separately.", alias="modelDeploymentMonitoringObjectiveConfigs")
    model_deployment_monitoring_schedule_config: Optional[GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfig] = Field(default=None, alias="modelDeploymentMonitoringScheduleConfig")
    model_monitoring_alert_config: Optional[GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfig] = Field(default=None, alias="modelMonitoringAlertConfig")
    name: Optional[StrictStr] = Field(default=None, description="Output only. Resource name of a ModelDeploymentMonitoringJob.")
    next_schedule_time: Optional[StrictStr] = Field(default=None, description="Output only. Timestamp when this monitoring pipeline will be scheduled to run for the next round.", alias="nextScheduleTime")
    predict_instance_schema_uri: Optional[StrictStr] = Field(default=None, description="YAML schema file uri describing the format of a single instance, which are given to format this Endpoint's prediction (and explanation). If not set, we will generate predict schema from collected predict requests.", alias="predictInstanceSchemaUri")
    sample_predict_instance: Optional[Any] = Field(default=None, description="Sample Predict instance, same format as PredictRequest.instances, this can be set as a replacement of ModelDeploymentMonitoringJob.predict_instance_schema_uri. If not set, we will generate predict schema from collected predict requests.", alias="samplePredictInstance")
    schedule_state: Optional[StrictStr] = Field(default=None, description="Output only. Schedule state when the monitoring job is in Running state.", alias="scheduleState")
    state: Optional[StrictStr] = Field(default=None, description="Output only. The detailed state of the monitoring job. When the job is still creating, the state will be 'PENDING'. Once the job is successfully created, the state will be 'RUNNING'. Pause the job, the state will be 'PAUSED'. Resume the job, the state will return to 'RUNNING'.")
    stats_anomalies_base_directory: Optional[GoogleCloudAiplatformV1beta1GcsDestination] = Field(default=None, alias="statsAnomaliesBaseDirectory")
    update_time: Optional[StrictStr] = Field(default=None, description="Output only. Timestamp when this ModelDeploymentMonitoringJob was updated most recently.", alias="updateTime")
    __properties: ClassVar[List[str]] = ["analysisInstanceSchemaUri", "bigqueryTables", "createTime", "displayName", "enableMonitoringPipelineLogs", "encryptionSpec", "endpoint", "error", "labels", "latestMonitoringPipelineMetadata", "logTtl", "loggingSamplingStrategy", "modelDeploymentMonitoringObjectiveConfigs", "modelDeploymentMonitoringScheduleConfig", "modelMonitoringAlertConfig", "name", "nextScheduleTime", "predictInstanceSchemaUri", "samplePredictInstance", "scheduleState", "state", "statsAnomaliesBaseDirectory", "updateTime"]

    @field_validator('schedule_state')
    def schedule_state_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['MONITORING_SCHEDULE_STATE_UNSPECIFIED', 'PENDING', 'OFFLINE', 'RUNNING']):
            raise ValueError("must be one of enum values ('MONITORING_SCHEDULE_STATE_UNSPECIFIED', 'PENDING', 'OFFLINE', 'RUNNING')")
        return value

    @field_validator('state')
    def state_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['JOB_STATE_UNSPECIFIED', 'JOB_STATE_QUEUED', 'JOB_STATE_PENDING', 'JOB_STATE_RUNNING', 'JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLING', 'JOB_STATE_CANCELLED', 'JOB_STATE_PAUSED', 'JOB_STATE_EXPIRED', 'JOB_STATE_UPDATING', 'JOB_STATE_PARTIALLY_SUCCEEDED']):
            raise ValueError("must be one of enum values ('JOB_STATE_UNSPECIFIED', 'JOB_STATE_QUEUED', 'JOB_STATE_PENDING', 'JOB_STATE_RUNNING', 'JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLING', 'JOB_STATE_CANCELLED', 'JOB_STATE_PAUSED', 'JOB_STATE_EXPIRED', 'JOB_STATE_UPDATING', 'JOB_STATE_PARTIALLY_SUCCEEDED')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringJob from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "bigquery_tables",
            "create_time",
            "name",
            "next_schedule_time",
            "schedule_state",
            "state",
            "update_time",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in bigquery_tables (list)
        _items = []
        if self.bigquery_tables:
            for _item_bigquery_tables in self.bigquery_tables:
                if _item_bigquery_tables:
                    _items.append(_item_bigquery_tables.to_dict())
            _dict['bigqueryTables'] = _items
        # override the default output from pydantic by calling `to_dict()` of encryption_spec
        if self.encryption_spec:
            _dict['encryptionSpec'] = self.encryption_spec.to_dict()
        # override the default output from pydantic by calling `to_dict()` of error
        if self.error:
            _dict['error'] = self.error.to_dict()
        # override the default output from pydantic by calling `to_dict()` of latest_monitoring_pipeline_metadata
        if self.latest_monitoring_pipeline_metadata:
            _dict['latestMonitoringPipelineMetadata'] = self.latest_monitoring_pipeline_metadata.to_dict()
        # override the default output from pydantic by calling `to_dict()` of logging_sampling_strategy
        if self.logging_sampling_strategy:
            _dict['loggingSamplingStrategy'] = self.logging_sampling_strategy.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in model_deployment_monitoring_objective_configs (list)
        _items = []
        if self.model_deployment_monitoring_objective_configs:
            for _item_model_deployment_monitoring_objective_configs in self.model_deployment_monitoring_objective_configs:
                if _item_model_deployment_monitoring_objective_configs:
                    _items.append(_item_model_deployment_monitoring_objective_configs.to_dict())
            _dict['modelDeploymentMonitoringObjectiveConfigs'] = _items
        # override the default output from pydantic by calling `to_dict()` of model_deployment_monitoring_schedule_config
        if self.model_deployment_monitoring_schedule_config:
            _dict['modelDeploymentMonitoringScheduleConfig'] = self.model_deployment_monitoring_schedule_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of model_monitoring_alert_config
        if self.model_monitoring_alert_config:
            _dict['modelMonitoringAlertConfig'] = self.model_monitoring_alert_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of stats_anomalies_base_directory
        if self.stats_anomalies_base_directory:
            _dict['statsAnomaliesBaseDirectory'] = self.stats_anomalies_base_directory.to_dict()
        # set to None if sample_predict_instance (nullable) is None
        # and model_fields_set contains the field
        if self.sample_predict_instance is None and "sample_predict_instance" in self.model_fields_set:
            _dict['samplePredictInstance'] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringJob from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "analysisInstanceSchemaUri": obj.get("analysisInstanceSchemaUri"),
            "bigqueryTables": [GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringBigQueryTable.from_dict(_item) for _item in obj["bigqueryTables"]] if obj.get("bigqueryTables") is not None else None,
            "createTime": obj.get("createTime"),
            "displayName": obj.get("displayName"),
            "enableMonitoringPipelineLogs": obj.get("enableMonitoringPipelineLogs"),
            "encryptionSpec": GoogleCloudAiplatformV1beta1EncryptionSpec.from_dict(obj["encryptionSpec"]) if obj.get("encryptionSpec") is not None else None,
            "endpoint": obj.get("endpoint"),
            "error": GoogleRpcStatus.from_dict(obj["error"]) if obj.get("error") is not None else None,
            "labels": obj.get("labels"),
            "latestMonitoringPipelineMetadata": GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadata.from_dict(obj["latestMonitoringPipelineMetadata"]) if obj.get("latestMonitoringPipelineMetadata") is not None else None,
            "logTtl": obj.get("logTtl"),
            "loggingSamplingStrategy": GoogleCloudAiplatformV1beta1SamplingStrategy.from_dict(obj["loggingSamplingStrategy"]) if obj.get("loggingSamplingStrategy") is not None else None,
            "modelDeploymentMonitoringObjectiveConfigs": [GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfig.from_dict(_item) for _item in obj["modelDeploymentMonitoringObjectiveConfigs"]] if obj.get("modelDeploymentMonitoringObjectiveConfigs") is not None else None,
            "modelDeploymentMonitoringScheduleConfig": GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfig.from_dict(obj["modelDeploymentMonitoringScheduleConfig"]) if obj.get("modelDeploymentMonitoringScheduleConfig") is not None else None,
            "modelMonitoringAlertConfig": GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfig.from_dict(obj["modelMonitoringAlertConfig"]) if obj.get("modelMonitoringAlertConfig") is not None else None,
            "name": obj.get("name"),
            "nextScheduleTime": obj.get("nextScheduleTime"),
            "predictInstanceSchemaUri": obj.get("predictInstanceSchemaUri"),
            "samplePredictInstance": obj.get("samplePredictInstance"),
            "scheduleState": obj.get("scheduleState"),
            "state": obj.get("state"),
            "statsAnomaliesBaseDirectory": GoogleCloudAiplatformV1beta1GcsDestination.from_dict(obj["statsAnomaliesBaseDirectory"]) if obj.get("statsAnomaliesBaseDirectory") is not None else None,
            "updateTime": obj.get("updateTime")
        })
        return _obj


