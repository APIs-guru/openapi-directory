# coding: utf-8

"""
    Vertex AI API

    Train high-quality custom machine learning models with minimal machine learning expertise and effort.

    The version of the OpenAPI document: v1beta1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.cloud_ai_nl_llm_proto_service_rai_result import CloudAiNlLlmProtoServiceRaiResult
from openapi_client.models.learning_genai_recitation_recitation_result import LearningGenaiRecitationRecitationResult
from openapi_client.models.learning_genai_root_classifier_output_summary import LearningGenaiRootClassifierOutputSummary
from openapi_client.models.learning_genai_root_codey_output import LearningGenaiRootCodeyOutput
from openapi_client.models.learning_genai_root_filter_metadata import LearningGenaiRootFilterMetadata
from openapi_client.models.learning_genai_root_grounding_metadata import LearningGenaiRootGroundingMetadata
from openapi_client.models.learning_genai_root_rai_output import LearningGenaiRootRAIOutput
from openapi_client.models.learning_genai_root_score import LearningGenaiRootScore
from openapi_client.models.nlp_saft_lang_id_result import NlpSaftLangIdResult
from typing import Optional, Set
from typing_extensions import Self

class LearningServingLlmMessageMetadata(BaseModel):
    """
    LINT.IfChange This metadata contains additional information required for debugging.
    """ # noqa: E501
    classifier_summary: Optional[LearningGenaiRootClassifierOutputSummary] = Field(default=None, alias="classifierSummary")
    codey_output: Optional[LearningGenaiRootCodeyOutput] = Field(default=None, alias="codeyOutput")
    current_stream_text_length: Optional[StrictInt] = Field(default=None, alias="currentStreamTextLength")
    deleted: Optional[StrictBool] = Field(default=None, description="Whether the corresponding message has been deleted.")
    filter_meta: Optional[List[LearningGenaiRootFilterMetadata]] = Field(default=None, description="Metadata for filters that triggered.", alias="filterMeta")
    final_message_score: Optional[LearningGenaiRootScore] = Field(default=None, alias="finalMessageScore")
    finish_reason: Optional[StrictStr] = Field(default=None, description="NOT YET IMPLEMENTED.", alias="finishReason")
    grounding_metadata: Optional[LearningGenaiRootGroundingMetadata] = Field(default=None, alias="groundingMetadata")
    is_code: Optional[StrictBool] = Field(default=None, description="Applies to streaming response message only. Whether the message is a code.", alias="isCode")
    is_fallback: Optional[StrictBool] = Field(default=None, description="Applies to Response message only. Indicates whether the message is a fallback and the response would have otherwise been empty.", alias="isFallback")
    langid_result: Optional[NlpSaftLangIdResult] = Field(default=None, alias="langidResult")
    language: Optional[StrictStr] = Field(default=None, description="Detected language.")
    lm_prefix: Optional[StrictStr] = Field(default=None, description="The LM prefix used to generate this response.", alias="lmPrefix")
    original_text: Optional[StrictStr] = Field(default=None, description="The original text generated by LLM. This is the raw output for debugging purposes.", alias="originalText")
    per_stream_decoded_token_count: Optional[StrictInt] = Field(default=None, description="NOT YET IMPLEMENTED. Applies to streaming only. Number of tokens decoded / emitted by the model as part of this stream. This may be different from token_count, which contains number of tokens returned in this response after any response rewriting / truncation.", alias="perStreamDecodedTokenCount")
    rai_outputs: Optional[List[LearningGenaiRootRAIOutput]] = Field(default=None, description="Results of running RAI on the query or this response candidate. One output per rai_config. It will be populated regardless of whether the threshold is exceeded or not.", alias="raiOutputs")
    recitation_result: Optional[LearningGenaiRecitationRecitationResult] = Field(default=None, alias="recitationResult")
    return_token_count: Optional[StrictInt] = Field(default=None, description="NOT YET IMPLEMENTED. Number of tokens returned as part of this candidate.", alias="returnTokenCount")
    scores: Optional[List[LearningGenaiRootScore]] = Field(default=None, description="All the different scores for a message are logged here.")
    stream_terminated: Optional[StrictBool] = Field(default=None, description="Whether the response is terminated during streaming return. Only used for streaming requests.", alias="streamTerminated")
    total_decoded_token_count: Optional[StrictInt] = Field(default=None, description="NOT YET IMPLEMENTED. Aggregated number of total tokens decoded so far. For streaming, this is sum of all the tokens decoded so far i.e. aggregated count.", alias="totalDecodedTokenCount")
    translated_user_prompts: Optional[List[StrictStr]] = Field(default=None, description="Translated user-prompt used for RAI post processing. This is for internal processing only. We will translate in pre-processor and pass the translated text to the post processor using this field. It will be empty if non of the signals requested need translation.", alias="translatedUserPrompts")
    vertex_rai_result: Optional[CloudAiNlLlmProtoServiceRaiResult] = Field(default=None, alias="vertexRaiResult")
    __properties: ClassVar[List[str]] = ["classifierSummary", "codeyOutput", "currentStreamTextLength", "deleted", "filterMeta", "finalMessageScore", "finishReason", "groundingMetadata", "isCode", "isFallback", "langidResult", "language", "lmPrefix", "originalText", "perStreamDecodedTokenCount", "raiOutputs", "recitationResult", "returnTokenCount", "scores", "streamTerminated", "totalDecodedTokenCount", "translatedUserPrompts", "vertexRaiResult"]

    @field_validator('finish_reason')
    def finish_reason_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['UNSPECIFIED', 'RETURN', 'STOP', 'MAX_TOKENS', 'FILTER']):
            raise ValueError("must be one of enum values ('UNSPECIFIED', 'RETURN', 'STOP', 'MAX_TOKENS', 'FILTER')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of LearningServingLlmMessageMetadata from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of classifier_summary
        if self.classifier_summary:
            _dict['classifierSummary'] = self.classifier_summary.to_dict()
        # override the default output from pydantic by calling `to_dict()` of codey_output
        if self.codey_output:
            _dict['codeyOutput'] = self.codey_output.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in filter_meta (list)
        _items = []
        if self.filter_meta:
            for _item_filter_meta in self.filter_meta:
                if _item_filter_meta:
                    _items.append(_item_filter_meta.to_dict())
            _dict['filterMeta'] = _items
        # override the default output from pydantic by calling `to_dict()` of final_message_score
        if self.final_message_score:
            _dict['finalMessageScore'] = self.final_message_score.to_dict()
        # override the default output from pydantic by calling `to_dict()` of grounding_metadata
        if self.grounding_metadata:
            _dict['groundingMetadata'] = self.grounding_metadata.to_dict()
        # override the default output from pydantic by calling `to_dict()` of langid_result
        if self.langid_result:
            _dict['langidResult'] = self.langid_result.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in rai_outputs (list)
        _items = []
        if self.rai_outputs:
            for _item_rai_outputs in self.rai_outputs:
                if _item_rai_outputs:
                    _items.append(_item_rai_outputs.to_dict())
            _dict['raiOutputs'] = _items
        # override the default output from pydantic by calling `to_dict()` of recitation_result
        if self.recitation_result:
            _dict['recitationResult'] = self.recitation_result.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in scores (list)
        _items = []
        if self.scores:
            for _item_scores in self.scores:
                if _item_scores:
                    _items.append(_item_scores.to_dict())
            _dict['scores'] = _items
        # override the default output from pydantic by calling `to_dict()` of vertex_rai_result
        if self.vertex_rai_result:
            _dict['vertexRaiResult'] = self.vertex_rai_result.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of LearningServingLlmMessageMetadata from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "classifierSummary": LearningGenaiRootClassifierOutputSummary.from_dict(obj["classifierSummary"]) if obj.get("classifierSummary") is not None else None,
            "codeyOutput": LearningGenaiRootCodeyOutput.from_dict(obj["codeyOutput"]) if obj.get("codeyOutput") is not None else None,
            "currentStreamTextLength": obj.get("currentStreamTextLength"),
            "deleted": obj.get("deleted"),
            "filterMeta": [LearningGenaiRootFilterMetadata.from_dict(_item) for _item in obj["filterMeta"]] if obj.get("filterMeta") is not None else None,
            "finalMessageScore": LearningGenaiRootScore.from_dict(obj["finalMessageScore"]) if obj.get("finalMessageScore") is not None else None,
            "finishReason": obj.get("finishReason"),
            "groundingMetadata": LearningGenaiRootGroundingMetadata.from_dict(obj["groundingMetadata"]) if obj.get("groundingMetadata") is not None else None,
            "isCode": obj.get("isCode"),
            "isFallback": obj.get("isFallback"),
            "langidResult": NlpSaftLangIdResult.from_dict(obj["langidResult"]) if obj.get("langidResult") is not None else None,
            "language": obj.get("language"),
            "lmPrefix": obj.get("lmPrefix"),
            "originalText": obj.get("originalText"),
            "perStreamDecodedTokenCount": obj.get("perStreamDecodedTokenCount"),
            "raiOutputs": [LearningGenaiRootRAIOutput.from_dict(_item) for _item in obj["raiOutputs"]] if obj.get("raiOutputs") is not None else None,
            "recitationResult": LearningGenaiRecitationRecitationResult.from_dict(obj["recitationResult"]) if obj.get("recitationResult") is not None else None,
            "returnTokenCount": obj.get("returnTokenCount"),
            "scores": [LearningGenaiRootScore.from_dict(_item) for _item in obj["scores"]] if obj.get("scores") is not None else None,
            "streamTerminated": obj.get("streamTerminated"),
            "totalDecodedTokenCount": obj.get("totalDecodedTokenCount"),
            "translatedUserPrompts": obj.get("translatedUserPrompts"),
            "vertexRaiResult": CloudAiNlLlmProtoServiceRaiResult.from_dict(obj["vertexRaiResult"]) if obj.get("vertexRaiResult") is not None else None
        })
        return _obj


