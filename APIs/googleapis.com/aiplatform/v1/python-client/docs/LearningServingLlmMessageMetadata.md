# LearningServingLlmMessageMetadata

LINT.IfChange This metadata contains additional information required for debugging.

## Properties

Name | Type | Description | Notes
------------ | ------------- | ------------- | -------------
**classifier_summary** | [**LearningGenaiRootClassifierOutputSummary**](LearningGenaiRootClassifierOutputSummary.md) |  | [optional] 
**codey_output** | [**LearningGenaiRootCodeyOutput**](LearningGenaiRootCodeyOutput.md) |  | [optional] 
**current_stream_text_length** | **int** |  | [optional] 
**deleted** | **bool** | Whether the corresponding message has been deleted. | [optional] 
**filter_meta** | [**List[LearningGenaiRootFilterMetadata]**](LearningGenaiRootFilterMetadata.md) | Metadata for filters that triggered. | [optional] 
**final_message_score** | [**LearningGenaiRootScore**](LearningGenaiRootScore.md) |  | [optional] 
**finish_reason** | **str** | NOT YET IMPLEMENTED. | [optional] 
**grounding_metadata** | [**LearningGenaiRootGroundingMetadata**](LearningGenaiRootGroundingMetadata.md) |  | [optional] 
**is_code** | **bool** | Applies to streaming response message only. Whether the message is a code. | [optional] 
**is_fallback** | **bool** | Applies to Response message only. Indicates whether the message is a fallback and the response would have otherwise been empty. | [optional] 
**langid_result** | [**NlpSaftLangIdResult**](NlpSaftLangIdResult.md) |  | [optional] 
**language** | **str** | Detected language. | [optional] 
**lm_prefix** | **str** | The LM prefix used to generate this response. | [optional] 
**original_text** | **str** | The original text generated by LLM. This is the raw output for debugging purposes. | [optional] 
**per_stream_decoded_token_count** | **int** | NOT YET IMPLEMENTED. Applies to streaming only. Number of tokens decoded / emitted by the model as part of this stream. This may be different from token_count, which contains number of tokens returned in this response after any response rewriting / truncation. | [optional] 
**rai_outputs** | [**List[LearningGenaiRootRAIOutput]**](LearningGenaiRootRAIOutput.md) | Results of running RAI on the query or this response candidate. One output per rai_config. It will be populated regardless of whether the threshold is exceeded or not. | [optional] 
**recitation_result** | [**LearningGenaiRecitationRecitationResult**](LearningGenaiRecitationRecitationResult.md) |  | [optional] 
**return_token_count** | **int** | NOT YET IMPLEMENTED. Number of tokens returned as part of this candidate. | [optional] 
**scores** | [**List[LearningGenaiRootScore]**](LearningGenaiRootScore.md) | All the different scores for a message are logged here. | [optional] 
**stream_terminated** | **bool** | Whether the response is terminated during streaming return. Only used for streaming requests. | [optional] 
**total_decoded_token_count** | **int** | NOT YET IMPLEMENTED. Aggregated number of total tokens decoded so far. For streaming, this is sum of all the tokens decoded so far i.e. aggregated count. | [optional] 
**translated_user_prompts** | **List[str]** | Translated user-prompt used for RAI post processing. This is for internal processing only. We will translate in pre-processor and pass the translated text to the post processor using this field. It will be empty if non of the signals requested need translation. | [optional] 
**vertex_rai_result** | [**CloudAiNlLlmProtoServiceRaiResult**](CloudAiNlLlmProtoServiceRaiResult.md) |  | [optional] 

## Example

```python
from openapi_client.models.learning_serving_llm_message_metadata import LearningServingLlmMessageMetadata

# TODO update the JSON string below
json = "{}"
# create an instance of LearningServingLlmMessageMetadata from a JSON string
learning_serving_llm_message_metadata_instance = LearningServingLlmMessageMetadata.from_json(json)
# print the JSON string representation of the object
print(LearningServingLlmMessageMetadata.to_json())

# convert the object into a dict
learning_serving_llm_message_metadata_dict = learning_serving_llm_message_metadata_instance.to_dict()
# create an instance of LearningServingLlmMessageMetadata from a dict
learning_serving_llm_message_metadata_from_dict = LearningServingLlmMessageMetadata.from_dict(learning_serving_llm_message_metadata_dict)
```
[[Back to Model list]](../README.md#documentation-for-models) [[Back to API list]](../README.md#documentation-for-api-endpoints) [[Back to README]](../README.md)


