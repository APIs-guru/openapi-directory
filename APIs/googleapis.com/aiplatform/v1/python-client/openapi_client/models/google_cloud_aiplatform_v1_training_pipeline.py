# coding: utf-8

"""
    Vertex AI API

    Train high-quality custom machine learning models with minimal machine learning expertise and effort.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_cloud_aiplatform_v1_encryption_spec import GoogleCloudAiplatformV1EncryptionSpec
from openapi_client.models.google_cloud_aiplatform_v1_input_data_config import GoogleCloudAiplatformV1InputDataConfig
from openapi_client.models.google_cloud_aiplatform_v1_model import GoogleCloudAiplatformV1Model
from openapi_client.models.google_rpc_status import GoogleRpcStatus
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudAiplatformV1TrainingPipeline(BaseModel):
    """
    The TrainingPipeline orchestrates tasks associated with training a Model. It always executes the training task, and optionally may also export data from Vertex AI's Dataset which becomes the training input, upload the Model to Vertex AI, and evaluate the Model.
    """ # noqa: E501
    create_time: Optional[StrictStr] = Field(default=None, description="Output only. Time when the TrainingPipeline was created.", alias="createTime")
    display_name: Optional[StrictStr] = Field(default=None, description="Required. The user-defined name of this TrainingPipeline.", alias="displayName")
    encryption_spec: Optional[GoogleCloudAiplatformV1EncryptionSpec] = Field(default=None, alias="encryptionSpec")
    end_time: Optional[StrictStr] = Field(default=None, description="Output only. Time when the TrainingPipeline entered any of the following states: `PIPELINE_STATE_SUCCEEDED`, `PIPELINE_STATE_FAILED`, `PIPELINE_STATE_CANCELLED`.", alias="endTime")
    error: Optional[GoogleRpcStatus] = None
    input_data_config: Optional[GoogleCloudAiplatformV1InputDataConfig] = Field(default=None, alias="inputDataConfig")
    labels: Optional[Dict[str, StrictStr]] = Field(default=None, description="The labels with user-defined metadata to organize TrainingPipelines. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.")
    model_id: Optional[StrictStr] = Field(default=None, description="Optional. The ID to use for the uploaded Model, which will become the final component of the model resource name. This value may be up to 63 characters, and valid characters are `[a-z0-9_-]`. The first character cannot be a number or hyphen.", alias="modelId")
    model_to_upload: Optional[GoogleCloudAiplatformV1Model] = Field(default=None, alias="modelToUpload")
    name: Optional[StrictStr] = Field(default=None, description="Output only. Resource name of the TrainingPipeline.")
    parent_model: Optional[StrictStr] = Field(default=None, description="Optional. When specify this field, the `model_to_upload` will not be uploaded as a new model, instead, it will become a new version of this `parent_model`.", alias="parentModel")
    start_time: Optional[StrictStr] = Field(default=None, description="Output only. Time when the TrainingPipeline for the first time entered the `PIPELINE_STATE_RUNNING` state.", alias="startTime")
    state: Optional[StrictStr] = Field(default=None, description="Output only. The detailed state of the pipeline.")
    training_task_definition: Optional[StrictStr] = Field(default=None, description="Required. A Google Cloud Storage path to the YAML file that defines the training task which is responsible for producing the model artifact, and may also include additional auxiliary work. The definition files that can be used here are found in gs://google-cloud-aiplatform/schema/trainingjob/definition/. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.", alias="trainingTaskDefinition")
    training_task_inputs: Optional[Any] = Field(default=None, description="Required. The training task's parameter(s), as specified in the training_task_definition's `inputs`.", alias="trainingTaskInputs")
    training_task_metadata: Optional[Any] = Field(default=None, description="Output only. The metadata information as specified in the training_task_definition's `metadata`. This metadata is an auxiliary runtime and final information about the training task. While the pipeline is running this information is populated only at a best effort basis. Only present if the pipeline's training_task_definition contains `metadata` object.", alias="trainingTaskMetadata")
    update_time: Optional[StrictStr] = Field(default=None, description="Output only. Time when the TrainingPipeline was most recently updated.", alias="updateTime")
    __properties: ClassVar[List[str]] = ["createTime", "displayName", "encryptionSpec", "endTime", "error", "inputDataConfig", "labels", "modelId", "modelToUpload", "name", "parentModel", "startTime", "state", "trainingTaskDefinition", "trainingTaskInputs", "trainingTaskMetadata", "updateTime"]

    @field_validator('state')
    def state_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['PIPELINE_STATE_UNSPECIFIED', 'PIPELINE_STATE_QUEUED', 'PIPELINE_STATE_PENDING', 'PIPELINE_STATE_RUNNING', 'PIPELINE_STATE_SUCCEEDED', 'PIPELINE_STATE_FAILED', 'PIPELINE_STATE_CANCELLING', 'PIPELINE_STATE_CANCELLED', 'PIPELINE_STATE_PAUSED']):
            raise ValueError("must be one of enum values ('PIPELINE_STATE_UNSPECIFIED', 'PIPELINE_STATE_QUEUED', 'PIPELINE_STATE_PENDING', 'PIPELINE_STATE_RUNNING', 'PIPELINE_STATE_SUCCEEDED', 'PIPELINE_STATE_FAILED', 'PIPELINE_STATE_CANCELLING', 'PIPELINE_STATE_CANCELLED', 'PIPELINE_STATE_PAUSED')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudAiplatformV1TrainingPipeline from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "create_time",
            "end_time",
            "name",
            "start_time",
            "state",
            "training_task_metadata",
            "update_time",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of encryption_spec
        if self.encryption_spec:
            _dict['encryptionSpec'] = self.encryption_spec.to_dict()
        # override the default output from pydantic by calling `to_dict()` of error
        if self.error:
            _dict['error'] = self.error.to_dict()
        # override the default output from pydantic by calling `to_dict()` of input_data_config
        if self.input_data_config:
            _dict['inputDataConfig'] = self.input_data_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of model_to_upload
        if self.model_to_upload:
            _dict['modelToUpload'] = self.model_to_upload.to_dict()
        # set to None if training_task_inputs (nullable) is None
        # and model_fields_set contains the field
        if self.training_task_inputs is None and "training_task_inputs" in self.model_fields_set:
            _dict['trainingTaskInputs'] = None

        # set to None if training_task_metadata (nullable) is None
        # and model_fields_set contains the field
        if self.training_task_metadata is None and "training_task_metadata" in self.model_fields_set:
            _dict['trainingTaskMetadata'] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudAiplatformV1TrainingPipeline from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "createTime": obj.get("createTime"),
            "displayName": obj.get("displayName"),
            "encryptionSpec": GoogleCloudAiplatformV1EncryptionSpec.from_dict(obj["encryptionSpec"]) if obj.get("encryptionSpec") is not None else None,
            "endTime": obj.get("endTime"),
            "error": GoogleRpcStatus.from_dict(obj["error"]) if obj.get("error") is not None else None,
            "inputDataConfig": GoogleCloudAiplatformV1InputDataConfig.from_dict(obj["inputDataConfig"]) if obj.get("inputDataConfig") is not None else None,
            "labels": obj.get("labels"),
            "modelId": obj.get("modelId"),
            "modelToUpload": GoogleCloudAiplatformV1Model.from_dict(obj["modelToUpload"]) if obj.get("modelToUpload") is not None else None,
            "name": obj.get("name"),
            "parentModel": obj.get("parentModel"),
            "startTime": obj.get("startTime"),
            "state": obj.get("state"),
            "trainingTaskDefinition": obj.get("trainingTaskDefinition"),
            "trainingTaskInputs": obj.get("trainingTaskInputs"),
            "trainingTaskMetadata": obj.get("trainingTaskMetadata"),
            "updateTime": obj.get("updateTime")
        })
        return _obj


