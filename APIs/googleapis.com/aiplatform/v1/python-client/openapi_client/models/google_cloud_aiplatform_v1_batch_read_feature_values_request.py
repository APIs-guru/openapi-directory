# coding: utf-8

"""
    Vertex AI API

    Train high-quality custom machine learning models with minimal machine learning expertise and effort.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_cloud_aiplatform_v1_batch_read_feature_values_request_entity_type_spec import GoogleCloudAiplatformV1BatchReadFeatureValuesRequestEntityTypeSpec
from openapi_client.models.google_cloud_aiplatform_v1_batch_read_feature_values_request_pass_through_field import GoogleCloudAiplatformV1BatchReadFeatureValuesRequestPassThroughField
from openapi_client.models.google_cloud_aiplatform_v1_big_query_source import GoogleCloudAiplatformV1BigQuerySource
from openapi_client.models.google_cloud_aiplatform_v1_csv_source import GoogleCloudAiplatformV1CsvSource
from openapi_client.models.google_cloud_aiplatform_v1_feature_value_destination import GoogleCloudAiplatformV1FeatureValueDestination
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudAiplatformV1BatchReadFeatureValuesRequest(BaseModel):
    """
    Request message for FeaturestoreService.BatchReadFeatureValues.
    """ # noqa: E501
    bigquery_read_instances: Optional[GoogleCloudAiplatformV1BigQuerySource] = Field(default=None, alias="bigqueryReadInstances")
    csv_read_instances: Optional[GoogleCloudAiplatformV1CsvSource] = Field(default=None, alias="csvReadInstances")
    destination: Optional[GoogleCloudAiplatformV1FeatureValueDestination] = None
    entity_type_specs: Optional[List[GoogleCloudAiplatformV1BatchReadFeatureValuesRequestEntityTypeSpec]] = Field(default=None, description="Required. Specifies EntityType grouping Features to read values of and settings.", alias="entityTypeSpecs")
    pass_through_fields: Optional[List[GoogleCloudAiplatformV1BatchReadFeatureValuesRequestPassThroughField]] = Field(default=None, description="When not empty, the specified fields in the *_read_instances source will be joined as-is in the output, in addition to those fields from the Featurestore Entity. For BigQuery source, the type of the pass-through values will be automatically inferred. For CSV source, the pass-through values will be passed as opaque bytes.", alias="passThroughFields")
    start_time: Optional[StrictStr] = Field(default=None, description="Optional. Excludes Feature values with feature generation timestamp before this timestamp. If not set, retrieve oldest values kept in Feature Store. Timestamp, if present, must not have higher than millisecond precision.", alias="startTime")
    __properties: ClassVar[List[str]] = ["bigqueryReadInstances", "csvReadInstances", "destination", "entityTypeSpecs", "passThroughFields", "startTime"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudAiplatformV1BatchReadFeatureValuesRequest from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of bigquery_read_instances
        if self.bigquery_read_instances:
            _dict['bigqueryReadInstances'] = self.bigquery_read_instances.to_dict()
        # override the default output from pydantic by calling `to_dict()` of csv_read_instances
        if self.csv_read_instances:
            _dict['csvReadInstances'] = self.csv_read_instances.to_dict()
        # override the default output from pydantic by calling `to_dict()` of destination
        if self.destination:
            _dict['destination'] = self.destination.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in entity_type_specs (list)
        _items = []
        if self.entity_type_specs:
            for _item_entity_type_specs in self.entity_type_specs:
                if _item_entity_type_specs:
                    _items.append(_item_entity_type_specs.to_dict())
            _dict['entityTypeSpecs'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in pass_through_fields (list)
        _items = []
        if self.pass_through_fields:
            for _item_pass_through_fields in self.pass_through_fields:
                if _item_pass_through_fields:
                    _items.append(_item_pass_through_fields.to_dict())
            _dict['passThroughFields'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudAiplatformV1BatchReadFeatureValuesRequest from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "bigqueryReadInstances": GoogleCloudAiplatformV1BigQuerySource.from_dict(obj["bigqueryReadInstances"]) if obj.get("bigqueryReadInstances") is not None else None,
            "csvReadInstances": GoogleCloudAiplatformV1CsvSource.from_dict(obj["csvReadInstances"]) if obj.get("csvReadInstances") is not None else None,
            "destination": GoogleCloudAiplatformV1FeatureValueDestination.from_dict(obj["destination"]) if obj.get("destination") is not None else None,
            "entityTypeSpecs": [GoogleCloudAiplatformV1BatchReadFeatureValuesRequestEntityTypeSpec.from_dict(_item) for _item in obj["entityTypeSpecs"]] if obj.get("entityTypeSpecs") is not None else None,
            "passThroughFields": [GoogleCloudAiplatformV1BatchReadFeatureValuesRequestPassThroughField.from_dict(_item) for _item in obj["passThroughFields"]] if obj.get("passThroughFields") is not None else None,
            "startTime": obj.get("startTime")
        })
        return _obj


