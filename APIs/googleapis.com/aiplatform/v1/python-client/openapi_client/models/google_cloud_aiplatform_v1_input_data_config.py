# coding: utf-8

"""
    Vertex AI API

    Train high-quality custom machine learning models with minimal machine learning expertise and effort.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_cloud_aiplatform_v1_big_query_destination import GoogleCloudAiplatformV1BigQueryDestination
from openapi_client.models.google_cloud_aiplatform_v1_filter_split import GoogleCloudAiplatformV1FilterSplit
from openapi_client.models.google_cloud_aiplatform_v1_fraction_split import GoogleCloudAiplatformV1FractionSplit
from openapi_client.models.google_cloud_aiplatform_v1_gcs_destination import GoogleCloudAiplatformV1GcsDestination
from openapi_client.models.google_cloud_aiplatform_v1_predefined_split import GoogleCloudAiplatformV1PredefinedSplit
from openapi_client.models.google_cloud_aiplatform_v1_stratified_split import GoogleCloudAiplatformV1StratifiedSplit
from openapi_client.models.google_cloud_aiplatform_v1_timestamp_split import GoogleCloudAiplatformV1TimestampSplit
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudAiplatformV1InputDataConfig(BaseModel):
    """
    Specifies Vertex AI owned input data to be used for training, and possibly evaluating, the Model.
    """ # noqa: E501
    annotation_schema_uri: Optional[StrictStr] = Field(default=None, description="Applicable only to custom training with Datasets that have DataItems and Annotations. Cloud Storage URI that points to a YAML file describing the annotation schema. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). The schema files that can be used here are found in gs://google-cloud-aiplatform/schema/dataset/annotation/ , note that the chosen schema must be consistent with metadata of the Dataset specified by dataset_id. Only Annotations that both match this schema and belong to DataItems not ignored by the split method are used in respectively training, validation or test role, depending on the role of the DataItem they are on. When used in conjunction with annotations_filter, the Annotations used for training are filtered by both annotations_filter and annotation_schema_uri.", alias="annotationSchemaUri")
    annotations_filter: Optional[StrictStr] = Field(default=None, description="Applicable only to Datasets that have DataItems and Annotations. A filter on Annotations of the Dataset. Only Annotations that both match this filter and belong to DataItems not ignored by the split method are used in respectively training, validation or test role, depending on the role of the DataItem they are on (for the auto-assigned that role is decided by Vertex AI). A filter with same syntax as the one used in ListAnnotations may be used, but note here it filters across all Annotations of the Dataset, and not just within a single DataItem.", alias="annotationsFilter")
    bigquery_destination: Optional[GoogleCloudAiplatformV1BigQueryDestination] = Field(default=None, alias="bigqueryDestination")
    dataset_id: Optional[StrictStr] = Field(default=None, description="Required. The ID of the Dataset in the same Project and Location which data will be used to train the Model. The Dataset must use schema compatible with Model being trained, and what is compatible should be described in the used TrainingPipeline's training_task_definition. For tabular Datasets, all their data is exported to training, to pick and choose from.", alias="datasetId")
    filter_split: Optional[GoogleCloudAiplatformV1FilterSplit] = Field(default=None, alias="filterSplit")
    fraction_split: Optional[GoogleCloudAiplatformV1FractionSplit] = Field(default=None, alias="fractionSplit")
    gcs_destination: Optional[GoogleCloudAiplatformV1GcsDestination] = Field(default=None, alias="gcsDestination")
    persist_ml_use_assignment: Optional[StrictBool] = Field(default=None, description="Whether to persist the ML use assignment to data item system labels.", alias="persistMlUseAssignment")
    predefined_split: Optional[GoogleCloudAiplatformV1PredefinedSplit] = Field(default=None, alias="predefinedSplit")
    saved_query_id: Optional[StrictStr] = Field(default=None, description="Only applicable to Datasets that have SavedQueries. The ID of a SavedQuery (annotation set) under the Dataset specified by dataset_id used for filtering Annotations for training. Only Annotations that are associated with this SavedQuery are used in respectively training. When used in conjunction with annotations_filter, the Annotations used for training are filtered by both saved_query_id and annotations_filter. Only one of saved_query_id and annotation_schema_uri should be specified as both of them represent the same thing: problem type.", alias="savedQueryId")
    stratified_split: Optional[GoogleCloudAiplatformV1StratifiedSplit] = Field(default=None, alias="stratifiedSplit")
    timestamp_split: Optional[GoogleCloudAiplatformV1TimestampSplit] = Field(default=None, alias="timestampSplit")
    __properties: ClassVar[List[str]] = ["annotationSchemaUri", "annotationsFilter", "bigqueryDestination", "datasetId", "filterSplit", "fractionSplit", "gcsDestination", "persistMlUseAssignment", "predefinedSplit", "savedQueryId", "stratifiedSplit", "timestampSplit"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudAiplatformV1InputDataConfig from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of bigquery_destination
        if self.bigquery_destination:
            _dict['bigqueryDestination'] = self.bigquery_destination.to_dict()
        # override the default output from pydantic by calling `to_dict()` of filter_split
        if self.filter_split:
            _dict['filterSplit'] = self.filter_split.to_dict()
        # override the default output from pydantic by calling `to_dict()` of fraction_split
        if self.fraction_split:
            _dict['fractionSplit'] = self.fraction_split.to_dict()
        # override the default output from pydantic by calling `to_dict()` of gcs_destination
        if self.gcs_destination:
            _dict['gcsDestination'] = self.gcs_destination.to_dict()
        # override the default output from pydantic by calling `to_dict()` of predefined_split
        if self.predefined_split:
            _dict['predefinedSplit'] = self.predefined_split.to_dict()
        # override the default output from pydantic by calling `to_dict()` of stratified_split
        if self.stratified_split:
            _dict['stratifiedSplit'] = self.stratified_split.to_dict()
        # override the default output from pydantic by calling `to_dict()` of timestamp_split
        if self.timestamp_split:
            _dict['timestampSplit'] = self.timestamp_split.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudAiplatformV1InputDataConfig from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "annotationSchemaUri": obj.get("annotationSchemaUri"),
            "annotationsFilter": obj.get("annotationsFilter"),
            "bigqueryDestination": GoogleCloudAiplatformV1BigQueryDestination.from_dict(obj["bigqueryDestination"]) if obj.get("bigqueryDestination") is not None else None,
            "datasetId": obj.get("datasetId"),
            "filterSplit": GoogleCloudAiplatformV1FilterSplit.from_dict(obj["filterSplit"]) if obj.get("filterSplit") is not None else None,
            "fractionSplit": GoogleCloudAiplatformV1FractionSplit.from_dict(obj["fractionSplit"]) if obj.get("fractionSplit") is not None else None,
            "gcsDestination": GoogleCloudAiplatformV1GcsDestination.from_dict(obj["gcsDestination"]) if obj.get("gcsDestination") is not None else None,
            "persistMlUseAssignment": obj.get("persistMlUseAssignment"),
            "predefinedSplit": GoogleCloudAiplatformV1PredefinedSplit.from_dict(obj["predefinedSplit"]) if obj.get("predefinedSplit") is not None else None,
            "savedQueryId": obj.get("savedQueryId"),
            "stratifiedSplit": GoogleCloudAiplatformV1StratifiedSplit.from_dict(obj["stratifiedSplit"]) if obj.get("stratifiedSplit") is not None else None,
            "timestampSplit": GoogleCloudAiplatformV1TimestampSplit.from_dict(obj["timestampSplit"]) if obj.get("timestampSplit") is not None else None
        })
        return _obj


