# coding: utf-8

"""
    Vertex AI API

    Train high-quality custom machine learning models with minimal machine learning expertise and effort.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictFloat, StrictInt
from typing import Any, ClassVar, Dict, List, Optional, Union
from openapi_client.models.google_cloud_aiplatform_v1_schema_modelevaluation_metrics_bounding_box_metrics import GoogleCloudAiplatformV1SchemaModelevaluationMetricsBoundingBoxMetrics
from openapi_client.models.google_cloud_aiplatform_v1_schema_modelevaluation_metrics_track_metrics import GoogleCloudAiplatformV1SchemaModelevaluationMetricsTrackMetrics
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudAiplatformV1SchemaModelevaluationMetricsVideoObjectTrackingMetrics(BaseModel):
    """
    Model evaluation metrics for video object tracking problems. Evaluates prediction quality of both labeled bounding boxes and labeled tracks (i.e. series of bounding boxes sharing same label and instance ID).
    """ # noqa: E501
    bounding_box_mean_average_precision: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="The single metric for bounding boxes evaluation: the `meanAveragePrecision` averaged over all `boundingBoxMetrics`.", alias="boundingBoxMeanAveragePrecision")
    bounding_box_metrics: Optional[List[GoogleCloudAiplatformV1SchemaModelevaluationMetricsBoundingBoxMetrics]] = Field(default=None, description="The bounding boxes match metrics for each intersection-over-union threshold 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99 and each label confidence threshold 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99 pair.", alias="boundingBoxMetrics")
    evaluated_bounding_box_count: Optional[StrictInt] = Field(default=None, description="UNIMPLEMENTED. The total number of bounding boxes (i.e. summed over all frames) the ground truth used to create this evaluation had.", alias="evaluatedBoundingBoxCount")
    evaluated_frame_count: Optional[StrictInt] = Field(default=None, description="UNIMPLEMENTED. The number of video frames used to create this evaluation.", alias="evaluatedFrameCount")
    evaluated_track_count: Optional[StrictInt] = Field(default=None, description="UNIMPLEMENTED. The total number of tracks (i.e. as seen across all frames) the ground truth used to create this evaluation had.", alias="evaluatedTrackCount")
    track_mean_average_precision: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="UNIMPLEMENTED. The single metric for tracks accuracy evaluation: the `meanAveragePrecision` averaged over all `trackMetrics`.", alias="trackMeanAveragePrecision")
    track_mean_bounding_box_iou: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="UNIMPLEMENTED. The single metric for tracks bounding box iou evaluation: the `meanBoundingBoxIou` averaged over all `trackMetrics`.", alias="trackMeanBoundingBoxIou")
    track_mean_mismatch_rate: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="UNIMPLEMENTED. The single metric for tracking consistency evaluation: the `meanMismatchRate` averaged over all `trackMetrics`.", alias="trackMeanMismatchRate")
    track_metrics: Optional[List[GoogleCloudAiplatformV1SchemaModelevaluationMetricsTrackMetrics]] = Field(default=None, description="UNIMPLEMENTED. The tracks match metrics for each intersection-over-union threshold 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99 and each label confidence threshold 0.05,0.10,...,0.95,0.96,0.97,0.98,0.99 pair.", alias="trackMetrics")
    __properties: ClassVar[List[str]] = ["boundingBoxMeanAveragePrecision", "boundingBoxMetrics", "evaluatedBoundingBoxCount", "evaluatedFrameCount", "evaluatedTrackCount", "trackMeanAveragePrecision", "trackMeanBoundingBoxIou", "trackMeanMismatchRate", "trackMetrics"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudAiplatformV1SchemaModelevaluationMetricsVideoObjectTrackingMetrics from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in bounding_box_metrics (list)
        _items = []
        if self.bounding_box_metrics:
            for _item_bounding_box_metrics in self.bounding_box_metrics:
                if _item_bounding_box_metrics:
                    _items.append(_item_bounding_box_metrics.to_dict())
            _dict['boundingBoxMetrics'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in track_metrics (list)
        _items = []
        if self.track_metrics:
            for _item_track_metrics in self.track_metrics:
                if _item_track_metrics:
                    _items.append(_item_track_metrics.to_dict())
            _dict['trackMetrics'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudAiplatformV1SchemaModelevaluationMetricsVideoObjectTrackingMetrics from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "boundingBoxMeanAveragePrecision": obj.get("boundingBoxMeanAveragePrecision"),
            "boundingBoxMetrics": [GoogleCloudAiplatformV1SchemaModelevaluationMetricsBoundingBoxMetrics.from_dict(_item) for _item in obj["boundingBoxMetrics"]] if obj.get("boundingBoxMetrics") is not None else None,
            "evaluatedBoundingBoxCount": obj.get("evaluatedBoundingBoxCount"),
            "evaluatedFrameCount": obj.get("evaluatedFrameCount"),
            "evaluatedTrackCount": obj.get("evaluatedTrackCount"),
            "trackMeanAveragePrecision": obj.get("trackMeanAveragePrecision"),
            "trackMeanBoundingBoxIou": obj.get("trackMeanBoundingBoxIou"),
            "trackMeanMismatchRate": obj.get("trackMeanMismatchRate"),
            "trackMetrics": [GoogleCloudAiplatformV1SchemaModelevaluationMetricsTrackMetrics.from_dict(_item) for _item in obj["trackMetrics"]] if obj.get("trackMetrics") is not None else None
        })
        return _obj


