# coding: utf-8

"""
    Cloud Logging API

    Writes log entries and manages your Cloud Logging configuration.

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.big_query_options import BigQueryOptions
from openapi_client.models.log_exclusion import LogExclusion
from typing import Optional, Set
from typing_extensions import Self

class LogSink(BaseModel):
    """
    Describes a sink used to export log entries to one of the following destinations: a Cloud Logging log bucket, a Cloud Storage bucket, a BigQuery dataset, a Pub/Sub topic, a Cloud project.A logs filter controls which log entries are exported. The sink must be created within a project, organization, billing account, or folder.
    """ # noqa: E501
    bigquery_options: Optional[BigQueryOptions] = Field(default=None, alias="bigqueryOptions")
    create_time: Optional[StrictStr] = Field(default=None, description="Output only. The creation timestamp of the sink.This field may not be present for older sinks.", alias="createTime")
    description: Optional[StrictStr] = Field(default=None, description="Optional. A description of this sink.The maximum length of the description is 8000 characters.")
    destination: Optional[StrictStr] = Field(default=None, description="Required. The export destination: \"storage.googleapis.com/[GCS_BUCKET]\" \"bigquery.googleapis.com/projects/[PROJECT_ID]/datasets/[DATASET]\" \"pubsub.googleapis.com/projects/[PROJECT_ID]/topics/[TOPIC_ID]\" \"logging.googleapis.com/projects/[PROJECT_ID]\" \"logging.googleapis.com/projects/[PROJECT_ID]/locations/[LOCATION_ID]/buckets/[BUCKET_ID]\" The sink's writer_identity, set when the sink is created, must have permission to write to the destination or else the log entries are not exported. For more information, see Exporting Logs with Sinks (https://cloud.google.com/logging/docs/api/tasks/exporting-logs).")
    disabled: Optional[StrictBool] = Field(default=None, description="Optional. If set to true, then this sink is disabled and it does not export any log entries.")
    exclusions: Optional[List[LogExclusion]] = Field(default=None, description="Optional. Log entries that match any of these exclusion filters will not be exported.If a log entry is matched by both filter and one of exclusion_filters it will not be exported.")
    filter: Optional[StrictStr] = Field(default=None, description="Optional. An advanced logs filter (https://cloud.google.com/logging/docs/view/advanced-queries). The only exported log entries are those that are in the resource owning the sink and that match the filter.For example:logName=\"projects/[PROJECT_ID]/logs/[LOG_ID]\" AND severity>=ERROR")
    include_children: Optional[StrictBool] = Field(default=None, description="Optional. This field applies only to sinks owned by organizations and folders. If the field is false, the default, only the logs owned by the sink's parent resource are available for export. If the field is true, then log entries from all the projects, folders, and billing accounts contained in the sink's parent resource are also available for export. Whether a particular log entry from the children is exported depends on the sink's filter expression.For example, if this field is true, then the filter resource.type=gce_instance would export all Compute Engine VM instance log entries from all projects in the sink's parent.To only export entries from certain child projects, filter on the project part of the log name:logName:(\"projects/test-project1/\" OR \"projects/test-project2/\") AND resource.type=gce_instance", alias="includeChildren")
    name: Optional[StrictStr] = Field(default=None, description="Output only. The client-assigned sink identifier, unique within the project.For example: \"my-syslog-errors-to-pubsub\".Sink identifiers are limited to 100 characters and can include only the following characters: upper and lower-case alphanumeric characters, underscores, hyphens, periods.First character has to be alphanumeric.")
    output_version_format: Optional[StrictStr] = Field(default=None, description="Deprecated. This field is unused.", alias="outputVersionFormat")
    update_time: Optional[StrictStr] = Field(default=None, description="Output only. The last update timestamp of the sink.This field may not be present for older sinks.", alias="updateTime")
    writer_identity: Optional[StrictStr] = Field(default=None, description="Output only. An IAM identity—a service account or group—under which Cloud Logging writes the exported log entries to the sink's destination. This field is either set by specifying custom_writer_identity or set automatically by sinks.create and sinks.update based on the value of unique_writer_identity in those methods.Until you grant this identity write-access to the destination, log entry exports from this sink will fail. For more information, see Granting Access for a Resource (https://cloud.google.com/iam/docs/granting-roles-to-service-accounts#granting_access_to_a_service_account_for_a_resource). Consult the destination service's documentation to determine the appropriate IAM roles to assign to the identity.Sinks that have a destination that is a log bucket in the same project as the sink cannot have a writer_identity and no additional permissions are required.", alias="writerIdentity")
    __properties: ClassVar[List[str]] = ["bigqueryOptions", "createTime", "description", "destination", "disabled", "exclusions", "filter", "includeChildren", "name", "outputVersionFormat", "updateTime", "writerIdentity"]

    @field_validator('output_version_format')
    def output_version_format_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['VERSION_FORMAT_UNSPECIFIED', 'V2', 'V1']):
            raise ValueError("must be one of enum values ('VERSION_FORMAT_UNSPECIFIED', 'V2', 'V1')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of LogSink from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "create_time",
            "name",
            "update_time",
            "writer_identity",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of bigquery_options
        if self.bigquery_options:
            _dict['bigqueryOptions'] = self.bigquery_options.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in exclusions (list)
        _items = []
        if self.exclusions:
            for _item_exclusions in self.exclusions:
                if _item_exclusions:
                    _items.append(_item_exclusions.to_dict())
            _dict['exclusions'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of LogSink from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "bigqueryOptions": BigQueryOptions.from_dict(obj["bigqueryOptions"]) if obj.get("bigqueryOptions") is not None else None,
            "createTime": obj.get("createTime"),
            "description": obj.get("description"),
            "destination": obj.get("destination"),
            "disabled": obj.get("disabled"),
            "exclusions": [LogExclusion.from_dict(_item) for _item in obj["exclusions"]] if obj.get("exclusions") is not None else None,
            "filter": obj.get("filter"),
            "includeChildren": obj.get("includeChildren"),
            "name": obj.get("name"),
            "outputVersionFormat": obj.get("outputVersionFormat"),
            "updateTime": obj.get("updateTime"),
            "writerIdentity": obj.get("writerIdentity")
        })
        return _obj


