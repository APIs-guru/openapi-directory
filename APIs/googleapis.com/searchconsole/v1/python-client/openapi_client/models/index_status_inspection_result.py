# coding: utf-8

"""
    Google Search Console API

    The Search Console API provides access to both Search Console data (verified users only) and to public information on an URL basis (anyone)

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from typing import Optional, Set
from typing_extensions import Self

class IndexStatusInspectionResult(BaseModel):
    """
    Results of index status inspection for either the live page or the version in Google's index, depending on whether you requested a live inspection or not. For more information, see the [Index coverage report documentation](https://support.google.com/webmasters/answer/7440203).
    """ # noqa: E501
    coverage_state: Optional[StrictStr] = Field(default=None, description="Could Google find and index the page. More details about page indexing appear in 'indexing_state'.", alias="coverageState")
    crawled_as: Optional[StrictStr] = Field(default=None, description="Primary crawler that was used by Google to crawl your site.", alias="crawledAs")
    google_canonical: Optional[StrictStr] = Field(default=None, description="The URL of the page that Google selected as canonical. If the page was not indexed, this field is absent.", alias="googleCanonical")
    indexing_state: Optional[StrictStr] = Field(default=None, description="Whether or not the page blocks indexing through a noindex rule.", alias="indexingState")
    last_crawl_time: Optional[StrictStr] = Field(default=None, description="Last time this URL was crawled by Google using the [primary crawler](https://support.google.com/webmasters/answer/7440203#primary_crawler). Absent if the URL was never crawled successfully.", alias="lastCrawlTime")
    page_fetch_state: Optional[StrictStr] = Field(default=None, description="Whether or not Google could retrieve the page from your server. Equivalent to [\"page fetch\"](https://support.google.com/webmasters/answer/9012289#index_coverage) in the URL inspection report.", alias="pageFetchState")
    referring_urls: Optional[List[StrictStr]] = Field(default=None, description="URLs that link to the inspected URL, directly and indirectly.", alias="referringUrls")
    robots_txt_state: Optional[StrictStr] = Field(default=None, description="Whether or not the page is blocked to Google by a robots.txt rule.", alias="robotsTxtState")
    sitemap: Optional[List[StrictStr]] = Field(default=None, description="Any sitemaps that this URL was listed in, as known by Google. Not guaranteed to be an exhaustive list, especially if Google did not discover this URL through a sitemap. Absent if no sitemaps were found.")
    user_canonical: Optional[StrictStr] = Field(default=None, description="The URL that your page or site [declares as canonical](https://developers.google.com/search/docs/advanced/crawling/consolidate-duplicate-urls?#define-canonical). If you did not declare a canonical URL, this field is absent.", alias="userCanonical")
    verdict: Optional[StrictStr] = Field(default=None, description="High level verdict about whether the URL *is* indexed (indexed status), or *can be* indexed (live inspection).")
    __properties: ClassVar[List[str]] = ["coverageState", "crawledAs", "googleCanonical", "indexingState", "lastCrawlTime", "pageFetchState", "referringUrls", "robotsTxtState", "sitemap", "userCanonical", "verdict"]

    @field_validator('crawled_as')
    def crawled_as_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['CRAWLING_USER_AGENT_UNSPECIFIED', 'DESKTOP', 'MOBILE']):
            raise ValueError("must be one of enum values ('CRAWLING_USER_AGENT_UNSPECIFIED', 'DESKTOP', 'MOBILE')")
        return value

    @field_validator('indexing_state')
    def indexing_state_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['INDEXING_STATE_UNSPECIFIED', 'INDEXING_ALLOWED', 'BLOCKED_BY_META_TAG', 'BLOCKED_BY_HTTP_HEADER', 'BLOCKED_BY_ROBOTS_TXT']):
            raise ValueError("must be one of enum values ('INDEXING_STATE_UNSPECIFIED', 'INDEXING_ALLOWED', 'BLOCKED_BY_META_TAG', 'BLOCKED_BY_HTTP_HEADER', 'BLOCKED_BY_ROBOTS_TXT')")
        return value

    @field_validator('page_fetch_state')
    def page_fetch_state_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['PAGE_FETCH_STATE_UNSPECIFIED', 'SUCCESSFUL', 'SOFT_404', 'BLOCKED_ROBOTS_TXT', 'NOT_FOUND', 'ACCESS_DENIED', 'SERVER_ERROR', 'REDIRECT_ERROR', 'ACCESS_FORBIDDEN', 'BLOCKED_4XX', 'INTERNAL_CRAWL_ERROR', 'INVALID_URL']):
            raise ValueError("must be one of enum values ('PAGE_FETCH_STATE_UNSPECIFIED', 'SUCCESSFUL', 'SOFT_404', 'BLOCKED_ROBOTS_TXT', 'NOT_FOUND', 'ACCESS_DENIED', 'SERVER_ERROR', 'REDIRECT_ERROR', 'ACCESS_FORBIDDEN', 'BLOCKED_4XX', 'INTERNAL_CRAWL_ERROR', 'INVALID_URL')")
        return value

    @field_validator('robots_txt_state')
    def robots_txt_state_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['ROBOTS_TXT_STATE_UNSPECIFIED', 'ALLOWED', 'DISALLOWED']):
            raise ValueError("must be one of enum values ('ROBOTS_TXT_STATE_UNSPECIFIED', 'ALLOWED', 'DISALLOWED')")
        return value

    @field_validator('verdict')
    def verdict_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['VERDICT_UNSPECIFIED', 'PASS', 'PARTIAL', 'FAIL', 'NEUTRAL']):
            raise ValueError("must be one of enum values ('VERDICT_UNSPECIFIED', 'PASS', 'PARTIAL', 'FAIL', 'NEUTRAL')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of IndexStatusInspectionResult from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of IndexStatusInspectionResult from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "coverageState": obj.get("coverageState"),
            "crawledAs": obj.get("crawledAs"),
            "googleCanonical": obj.get("googleCanonical"),
            "indexingState": obj.get("indexingState"),
            "lastCrawlTime": obj.get("lastCrawlTime"),
            "pageFetchState": obj.get("pageFetchState"),
            "referringUrls": obj.get("referringUrls"),
            "robotsTxtState": obj.get("robotsTxtState"),
            "sitemap": obj.get("sitemap"),
            "userCanonical": obj.get("userCanonical"),
            "verdict": obj.get("verdict")
        })
        return _obj


