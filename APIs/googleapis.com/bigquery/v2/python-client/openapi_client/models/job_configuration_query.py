# coding: utf-8

"""
    BigQuery API

    A data platform for customers to create, manage, share and query data.

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.clustering import Clustering
from openapi_client.models.connection_property import ConnectionProperty
from openapi_client.models.dataset_reference import DatasetReference
from openapi_client.models.encryption_configuration import EncryptionConfiguration
from openapi_client.models.external_data_configuration import ExternalDataConfiguration
from openapi_client.models.query_parameter import QueryParameter
from openapi_client.models.range_partitioning import RangePartitioning
from openapi_client.models.script_options import ScriptOptions
from openapi_client.models.system_variables import SystemVariables
from openapi_client.models.table_reference import TableReference
from openapi_client.models.time_partitioning import TimePartitioning
from openapi_client.models.user_defined_function_resource import UserDefinedFunctionResource
from typing import Optional, Set
from typing_extensions import Self

class JobConfigurationQuery(BaseModel):
    """
    JobConfigurationQuery configures a BigQuery query job.
    """ # noqa: E501
    allow_large_results: Optional[StrictBool] = Field(default=False, description="Optional. If true and query uses legacy SQL dialect, allows the query to produce arbitrarily large result tables at a slight cost in performance. Requires destinationTable to be set. For GoogleSQL queries, this flag is ignored and large results are always allowed. However, you must still set destinationTable when result size exceeds the allowed maximum response size.", alias="allowLargeResults")
    clustering: Optional[Clustering] = None
    connection_properties: Optional[List[ConnectionProperty]] = Field(default=None, description="Connection properties which can modify the query behavior.", alias="connectionProperties")
    continuous: Optional[StrictBool] = Field(default=None, description="[Optional] Specifies whether the query should be executed as a continuous query. The default value is false.")
    create_disposition: Optional[StrictStr] = Field(default=None, description="Optional. Specifies whether the job is allowed to create new tables. The following values are supported: * CREATE_IF_NEEDED: If the table does not exist, BigQuery creates the table. * CREATE_NEVER: The table must already exist. If it does not, a 'notFound' error is returned in the job result. The default value is CREATE_IF_NEEDED. Creation, truncation and append actions occur as one atomic update upon job completion.", alias="createDisposition")
    create_session: Optional[StrictBool] = Field(default=None, description="If this property is true, the job creates a new session using a randomly generated session_id. To continue using a created session with subsequent queries, pass the existing session identifier as a `ConnectionProperty` value. The session identifier is returned as part of the `SessionInfo` message within the query statistics. The new session's location will be set to `Job.JobReference.location` if it is present, otherwise it's set to the default location based on existing routing logic.", alias="createSession")
    default_dataset: Optional[DatasetReference] = Field(default=None, alias="defaultDataset")
    destination_encryption_configuration: Optional[EncryptionConfiguration] = Field(default=None, alias="destinationEncryptionConfiguration")
    destination_table: Optional[TableReference] = Field(default=None, alias="destinationTable")
    flatten_results: Optional[StrictBool] = Field(default=True, description="Optional. If true and query uses legacy SQL dialect, flattens all nested and repeated fields in the query results. allowLargeResults must be true if this is set to false. For GoogleSQL queries, this flag is ignored and results are never flattened.", alias="flattenResults")
    maximum_billing_tier: Optional[StrictInt] = Field(default=1, description="Optional. [Deprecated] Maximum billing tier allowed for this query. The billing tier controls the amount of compute resources allotted to the query, and multiplies the on-demand cost of the query accordingly. A query that runs within its allotted resources will succeed and indicate its billing tier in statistics.query.billingTier, but if the query exceeds its allotted resources, it will fail with billingTierLimitExceeded. WARNING: The billed byte amount can be multiplied by an amount up to this number! Most users should not need to alter this setting, and we recommend that you avoid introducing new uses of it.", alias="maximumBillingTier")
    maximum_bytes_billed: Optional[StrictStr] = Field(default=None, description="Limits the bytes billed for this job. Queries that will have bytes billed beyond this limit will fail (without incurring a charge). If unspecified, this will be set to your project default.", alias="maximumBytesBilled")
    parameter_mode: Optional[StrictStr] = Field(default=None, description="GoogleSQL only. Set to POSITIONAL to use positional (?) query parameters or to NAMED to use named (@myparam) query parameters in this query.", alias="parameterMode")
    preserve_nulls: Optional[StrictBool] = Field(default=None, description="[Deprecated] This property is deprecated.", alias="preserveNulls")
    priority: Optional[StrictStr] = Field(default=None, description="Optional. Specifies a priority for the query. Possible values include INTERACTIVE and BATCH. The default value is INTERACTIVE.")
    query: Optional[StrictStr] = Field(default=None, description="[Required] SQL query text to execute. The useLegacySql field can be used to indicate whether the query uses legacy SQL or GoogleSQL.")
    query_parameters: Optional[List[QueryParameter]] = Field(default=None, description="Query parameters for GoogleSQL queries.", alias="queryParameters")
    range_partitioning: Optional[RangePartitioning] = Field(default=None, alias="rangePartitioning")
    schema_update_options: Optional[List[StrictStr]] = Field(default=None, description="Allows the schema of the destination table to be updated as a side effect of the query job. Schema update options are supported in two cases: when writeDisposition is WRITE_APPEND; when writeDisposition is WRITE_TRUNCATE and the destination table is a partition of a table, specified by partition decorators. For normal tables, WRITE_TRUNCATE will always overwrite the schema. One or more of the following values are specified: * ALLOW_FIELD_ADDITION: allow adding a nullable field to the schema. * ALLOW_FIELD_RELAXATION: allow relaxing a required field in the original schema to nullable.", alias="schemaUpdateOptions")
    script_options: Optional[ScriptOptions] = Field(default=None, alias="scriptOptions")
    system_variables: Optional[SystemVariables] = Field(default=None, alias="systemVariables")
    table_definitions: Optional[Dict[str, ExternalDataConfiguration]] = Field(default=None, description="Optional. You can specify external table definitions, which operate as ephemeral tables that can be queried. These definitions are configured using a JSON map, where the string key represents the table identifier, and the value is the corresponding external data configuration object.", alias="tableDefinitions")
    time_partitioning: Optional[TimePartitioning] = Field(default=None, alias="timePartitioning")
    use_legacy_sql: Optional[StrictBool] = Field(default=True, description="Optional. Specifies whether to use BigQuery's legacy SQL dialect for this query. The default value is true. If set to false, the query will use BigQuery's GoogleSQL: https://cloud.google.com/bigquery/sql-reference/ When useLegacySql is set to false, the value of flattenResults is ignored; query will be run as if flattenResults is false.", alias="useLegacySql")
    use_query_cache: Optional[StrictBool] = Field(default=True, description="Optional. Whether to look for the result in the query cache. The query cache is a best-effort cache that will be flushed whenever tables in the query are modified. Moreover, the query cache is only available when a query does not have a destination table specified. The default value is true.", alias="useQueryCache")
    user_defined_function_resources: Optional[List[UserDefinedFunctionResource]] = Field(default=None, description="Describes user-defined function resources used in the query.", alias="userDefinedFunctionResources")
    write_disposition: Optional[StrictStr] = Field(default=None, description="Optional. Specifies the action that occurs if the destination table already exists. The following values are supported: * WRITE_TRUNCATE: If the table already exists, BigQuery overwrites the data, removes the constraints, and uses the schema from the query result. * WRITE_APPEND: If the table already exists, BigQuery appends the data to the table. * WRITE_EMPTY: If the table already exists and contains data, a 'duplicate' error is returned in the job result. The default value is WRITE_EMPTY. Each action is atomic and only occurs if BigQuery is able to complete the job successfully. Creation, truncation and append actions occur as one atomic update upon job completion.", alias="writeDisposition")
    __properties: ClassVar[List[str]] = ["allowLargeResults", "clustering", "connectionProperties", "continuous", "createDisposition", "createSession", "defaultDataset", "destinationEncryptionConfiguration", "destinationTable", "flattenResults", "maximumBillingTier", "maximumBytesBilled", "parameterMode", "preserveNulls", "priority", "query", "queryParameters", "rangePartitioning", "schemaUpdateOptions", "scriptOptions", "systemVariables", "tableDefinitions", "timePartitioning", "useLegacySql", "useQueryCache", "userDefinedFunctionResources", "writeDisposition"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of JobConfigurationQuery from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of clustering
        if self.clustering:
            _dict['clustering'] = self.clustering.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in connection_properties (list)
        _items = []
        if self.connection_properties:
            for _item_connection_properties in self.connection_properties:
                if _item_connection_properties:
                    _items.append(_item_connection_properties.to_dict())
            _dict['connectionProperties'] = _items
        # override the default output from pydantic by calling `to_dict()` of default_dataset
        if self.default_dataset:
            _dict['defaultDataset'] = self.default_dataset.to_dict()
        # override the default output from pydantic by calling `to_dict()` of destination_encryption_configuration
        if self.destination_encryption_configuration:
            _dict['destinationEncryptionConfiguration'] = self.destination_encryption_configuration.to_dict()
        # override the default output from pydantic by calling `to_dict()` of destination_table
        if self.destination_table:
            _dict['destinationTable'] = self.destination_table.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in query_parameters (list)
        _items = []
        if self.query_parameters:
            for _item_query_parameters in self.query_parameters:
                if _item_query_parameters:
                    _items.append(_item_query_parameters.to_dict())
            _dict['queryParameters'] = _items
        # override the default output from pydantic by calling `to_dict()` of range_partitioning
        if self.range_partitioning:
            _dict['rangePartitioning'] = self.range_partitioning.to_dict()
        # override the default output from pydantic by calling `to_dict()` of script_options
        if self.script_options:
            _dict['scriptOptions'] = self.script_options.to_dict()
        # override the default output from pydantic by calling `to_dict()` of system_variables
        if self.system_variables:
            _dict['systemVariables'] = self.system_variables.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each value in table_definitions (dict)
        _field_dict = {}
        if self.table_definitions:
            for _key_table_definitions in self.table_definitions:
                if self.table_definitions[_key_table_definitions]:
                    _field_dict[_key_table_definitions] = self.table_definitions[_key_table_definitions].to_dict()
            _dict['tableDefinitions'] = _field_dict
        # override the default output from pydantic by calling `to_dict()` of time_partitioning
        if self.time_partitioning:
            _dict['timePartitioning'] = self.time_partitioning.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in user_defined_function_resources (list)
        _items = []
        if self.user_defined_function_resources:
            for _item_user_defined_function_resources in self.user_defined_function_resources:
                if _item_user_defined_function_resources:
                    _items.append(_item_user_defined_function_resources.to_dict())
            _dict['userDefinedFunctionResources'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of JobConfigurationQuery from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "allowLargeResults": obj.get("allowLargeResults") if obj.get("allowLargeResults") is not None else False,
            "clustering": Clustering.from_dict(obj["clustering"]) if obj.get("clustering") is not None else None,
            "connectionProperties": [ConnectionProperty.from_dict(_item) for _item in obj["connectionProperties"]] if obj.get("connectionProperties") is not None else None,
            "continuous": obj.get("continuous"),
            "createDisposition": obj.get("createDisposition"),
            "createSession": obj.get("createSession"),
            "defaultDataset": DatasetReference.from_dict(obj["defaultDataset"]) if obj.get("defaultDataset") is not None else None,
            "destinationEncryptionConfiguration": EncryptionConfiguration.from_dict(obj["destinationEncryptionConfiguration"]) if obj.get("destinationEncryptionConfiguration") is not None else None,
            "destinationTable": TableReference.from_dict(obj["destinationTable"]) if obj.get("destinationTable") is not None else None,
            "flattenResults": obj.get("flattenResults") if obj.get("flattenResults") is not None else True,
            "maximumBillingTier": obj.get("maximumBillingTier") if obj.get("maximumBillingTier") is not None else 1,
            "maximumBytesBilled": obj.get("maximumBytesBilled"),
            "parameterMode": obj.get("parameterMode"),
            "preserveNulls": obj.get("preserveNulls"),
            "priority": obj.get("priority"),
            "query": obj.get("query"),
            "queryParameters": [QueryParameter.from_dict(_item) for _item in obj["queryParameters"]] if obj.get("queryParameters") is not None else None,
            "rangePartitioning": RangePartitioning.from_dict(obj["rangePartitioning"]) if obj.get("rangePartitioning") is not None else None,
            "schemaUpdateOptions": obj.get("schemaUpdateOptions"),
            "scriptOptions": ScriptOptions.from_dict(obj["scriptOptions"]) if obj.get("scriptOptions") is not None else None,
            "systemVariables": SystemVariables.from_dict(obj["systemVariables"]) if obj.get("systemVariables") is not None else None,
            "tableDefinitions": dict(
                (_k, ExternalDataConfiguration.from_dict(_v))
                for _k, _v in obj["tableDefinitions"].items()
            )
            if obj.get("tableDefinitions") is not None
            else None,
            "timePartitioning": TimePartitioning.from_dict(obj["timePartitioning"]) if obj.get("timePartitioning") is not None else None,
            "useLegacySql": obj.get("useLegacySql") if obj.get("useLegacySql") is not None else True,
            "useQueryCache": obj.get("useQueryCache") if obj.get("useQueryCache") is not None else True,
            "userDefinedFunctionResources": [UserDefinedFunctionResource.from_dict(_item) for _item in obj["userDefinedFunctionResources"]] if obj.get("userDefinedFunctionResources") is not None else None,
            "writeDisposition": obj.get("writeDisposition")
        })
        return _obj


