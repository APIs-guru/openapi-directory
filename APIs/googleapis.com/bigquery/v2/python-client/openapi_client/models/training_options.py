# coding: utf-8

"""
    BigQuery API

    A data platform for customers to create, manage, share and query data.

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictFloat, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional, Union
from openapi_client.models.arima_order import ArimaOrder
from typing import Optional, Set
from typing_extensions import Self

class TrainingOptions(BaseModel):
    """
    Options used in model training.
    """ # noqa: E501
    activation_fn: Optional[StrictStr] = Field(default=None, description="Activation function of the neural nets.", alias="activationFn")
    adjust_step_changes: Optional[StrictBool] = Field(default=None, description="If true, detect step changes and make data adjustment in the input time series.", alias="adjustStepChanges")
    approx_global_feature_contrib: Optional[StrictBool] = Field(default=None, description="Whether to use approximate feature contribution method in XGBoost model explanation for global explain.", alias="approxGlobalFeatureContrib")
    auto_arima: Optional[StrictBool] = Field(default=None, description="Whether to enable auto ARIMA or not.", alias="autoArima")
    auto_arima_max_order: Optional[StrictStr] = Field(default=None, description="The max value of the sum of non-seasonal p and q.", alias="autoArimaMaxOrder")
    auto_arima_min_order: Optional[StrictStr] = Field(default=None, description="The min value of the sum of non-seasonal p and q.", alias="autoArimaMinOrder")
    auto_class_weights: Optional[StrictBool] = Field(default=None, description="Whether to calculate class weights automatically based on the popularity of each label.", alias="autoClassWeights")
    batch_size: Optional[StrictStr] = Field(default=None, description="Batch size for dnn models.", alias="batchSize")
    booster_type: Optional[StrictStr] = Field(default=None, description="Booster type for boosted tree models.", alias="boosterType")
    budget_hours: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Budget in hours for AutoML training.", alias="budgetHours")
    calculate_p_values: Optional[StrictBool] = Field(default=None, description="Whether or not p-value test should be computed for this model. Only available for linear and logistic regression models.", alias="calculatePValues")
    category_encoding_method: Optional[StrictStr] = Field(default=None, description="Categorical feature encoding method.", alias="categoryEncodingMethod")
    clean_spikes_and_dips: Optional[StrictBool] = Field(default=None, description="If true, clean spikes and dips in the input time series.", alias="cleanSpikesAndDips")
    color_space: Optional[StrictStr] = Field(default=None, description="Enums for color space, used for processing images in Object Table. See more details at https://www.tensorflow.org/io/tutorials/colorspace.", alias="colorSpace")
    colsample_bylevel: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Subsample ratio of columns for each level for boosted tree models.", alias="colsampleBylevel")
    colsample_bynode: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Subsample ratio of columns for each node(split) for boosted tree models.", alias="colsampleBynode")
    colsample_bytree: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Subsample ratio of columns when constructing each tree for boosted tree models.", alias="colsampleBytree")
    dart_normalize_type: Optional[StrictStr] = Field(default=None, description="Type of normalization algorithm for boosted tree models using dart booster.", alias="dartNormalizeType")
    data_frequency: Optional[StrictStr] = Field(default=None, description="The data frequency of a time series.", alias="dataFrequency")
    data_split_column: Optional[StrictStr] = Field(default=None, description="The column to split data with. This column won't be used as a feature. 1. When data_split_method is CUSTOM, the corresponding column should be boolean. The rows with true value tag are eval data, and the false are training data. 2. When data_split_method is SEQ, the first DATA_SPLIT_EVAL_FRACTION rows (from smallest to largest) in the corresponding column are used as training data, and the rest are eval data. It respects the order in Orderable data types: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#data-type-properties", alias="dataSplitColumn")
    data_split_eval_fraction: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="The fraction of evaluation data over the whole input data. The rest of data will be used as training data. The format should be double. Accurate to two decimal places. Default value is 0.2.", alias="dataSplitEvalFraction")
    data_split_method: Optional[StrictStr] = Field(default=None, description="The data split type for training and evaluation, e.g. RANDOM.", alias="dataSplitMethod")
    decompose_time_series: Optional[StrictBool] = Field(default=None, description="If true, perform decompose time series and save the results.", alias="decomposeTimeSeries")
    distance_type: Optional[StrictStr] = Field(default=None, description="Distance type for clustering models.", alias="distanceType")
    dropout: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Dropout probability for dnn models.")
    early_stop: Optional[StrictBool] = Field(default=None, description="Whether to stop early when the loss doesn't improve significantly any more (compared to min_relative_progress). Used only for iterative training algorithms.", alias="earlyStop")
    enable_global_explain: Optional[StrictBool] = Field(default=None, description="If true, enable global explanation during training.", alias="enableGlobalExplain")
    feedback_type: Optional[StrictStr] = Field(default=None, description="Feedback type that specifies which algorithm to run for matrix factorization.", alias="feedbackType")
    fit_intercept: Optional[StrictBool] = Field(default=None, description="Whether the model should include intercept during model training.", alias="fitIntercept")
    hidden_units: Optional[List[StrictStr]] = Field(default=None, description="Hidden units for dnn models.", alias="hiddenUnits")
    holiday_region: Optional[StrictStr] = Field(default=None, description="The geographical region based on which the holidays are considered in time series modeling. If a valid value is specified, then holiday effects modeling is enabled.", alias="holidayRegion")
    holiday_regions: Optional[List[StrictStr]] = Field(default=None, description="A list of geographical regions that are used for time series modeling.", alias="holidayRegions")
    horizon: Optional[StrictStr] = Field(default=None, description="The number of periods ahead that need to be forecasted.")
    hparam_tuning_objectives: Optional[List[StrictStr]] = Field(default=None, description="The target evaluation metrics to optimize the hyperparameters for.", alias="hparamTuningObjectives")
    include_drift: Optional[StrictBool] = Field(default=None, description="Include drift when fitting an ARIMA model.", alias="includeDrift")
    initial_learn_rate: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Specifies the initial learning rate for the line search learn rate strategy.", alias="initialLearnRate")
    input_label_columns: Optional[List[StrictStr]] = Field(default=None, description="Name of input label columns in training data.", alias="inputLabelColumns")
    instance_weight_column: Optional[StrictStr] = Field(default=None, description="Name of the instance weight column for training data. This column isn't be used as a feature.", alias="instanceWeightColumn")
    integrated_gradients_num_steps: Optional[StrictStr] = Field(default=None, description="Number of integral steps for the integrated gradients explain method.", alias="integratedGradientsNumSteps")
    item_column: Optional[StrictStr] = Field(default=None, description="Item column specified for matrix factorization models.", alias="itemColumn")
    kmeans_initialization_column: Optional[StrictStr] = Field(default=None, description="The column used to provide the initial centroids for kmeans algorithm when kmeans_initialization_method is CUSTOM.", alias="kmeansInitializationColumn")
    kmeans_initialization_method: Optional[StrictStr] = Field(default=None, description="The method used to initialize the centroids for kmeans algorithm.", alias="kmeansInitializationMethod")
    l1_reg_activation: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="L1 regularization coefficient to activations.", alias="l1RegActivation")
    l1_regularization: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="L1 regularization coefficient.", alias="l1Regularization")
    l2_regularization: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="L2 regularization coefficient.", alias="l2Regularization")
    label_class_weights: Optional[Dict[str, Union[StrictFloat, StrictInt]]] = Field(default=None, description="Weights associated with each label class, for rebalancing the training data. Only applicable for classification models.", alias="labelClassWeights")
    learn_rate: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Learning rate in training. Used only for iterative training algorithms.", alias="learnRate")
    learn_rate_strategy: Optional[StrictStr] = Field(default=None, description="The strategy to determine learn rate for the current iteration.", alias="learnRateStrategy")
    loss_type: Optional[StrictStr] = Field(default=None, description="Type of loss function used during training run.", alias="lossType")
    max_iterations: Optional[StrictStr] = Field(default=None, description="The maximum number of iterations in training. Used only for iterative training algorithms.", alias="maxIterations")
    max_parallel_trials: Optional[StrictStr] = Field(default=None, description="Maximum number of trials to run in parallel.", alias="maxParallelTrials")
    max_time_series_length: Optional[StrictStr] = Field(default=None, description="The maximum number of time points in a time series that can be used in modeling the trend component of the time series. Don't use this option with the `timeSeriesLengthFraction` or `minTimeSeriesLength` options.", alias="maxTimeSeriesLength")
    max_tree_depth: Optional[StrictStr] = Field(default=None, description="Maximum depth of a tree for boosted tree models.", alias="maxTreeDepth")
    min_relative_progress: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="When early_stop is true, stops training when accuracy improvement is less than 'min_relative_progress'. Used only for iterative training algorithms.", alias="minRelativeProgress")
    min_split_loss: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Minimum split loss for boosted tree models.", alias="minSplitLoss")
    min_time_series_length: Optional[StrictStr] = Field(default=None, description="The minimum number of time points in a time series that are used in modeling the trend component of the time series. If you use this option you must also set the `timeSeriesLengthFraction` option. This training option ensures that enough time points are available when you use `timeSeriesLengthFraction` in trend modeling. This is particularly important when forecasting multiple time series in a single query using `timeSeriesIdColumn`. If the total number of time points is less than the `minTimeSeriesLength` value, then the query uses all available time points.", alias="minTimeSeriesLength")
    min_tree_child_weight: Optional[StrictStr] = Field(default=None, description="Minimum sum of instance weight needed in a child for boosted tree models.", alias="minTreeChildWeight")
    model_registry: Optional[StrictStr] = Field(default=None, description="The model registry.", alias="modelRegistry")
    model_uri: Optional[StrictStr] = Field(default=None, description="Google Cloud Storage URI from which the model was imported. Only applicable for imported models.", alias="modelUri")
    non_seasonal_order: Optional[ArimaOrder] = Field(default=None, alias="nonSeasonalOrder")
    num_clusters: Optional[StrictStr] = Field(default=None, description="Number of clusters for clustering models.", alias="numClusters")
    num_factors: Optional[StrictStr] = Field(default=None, description="Num factors specified for matrix factorization models.", alias="numFactors")
    num_parallel_tree: Optional[StrictStr] = Field(default=None, description="Number of parallel trees constructed during each iteration for boosted tree models.", alias="numParallelTree")
    num_principal_components: Optional[StrictStr] = Field(default=None, description="Number of principal components to keep in the PCA model. Must be <= the number of features.", alias="numPrincipalComponents")
    num_trials: Optional[StrictStr] = Field(default=None, description="Number of trials to run this hyperparameter tuning job.", alias="numTrials")
    optimization_strategy: Optional[StrictStr] = Field(default=None, description="Optimization strategy for training linear regression models.", alias="optimizationStrategy")
    optimizer: Optional[StrictStr] = Field(default=None, description="Optimizer used for training the neural nets.")
    pca_explained_variance_ratio: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="The minimum ratio of cumulative explained variance that needs to be given by the PCA model.", alias="pcaExplainedVarianceRatio")
    pca_solver: Optional[StrictStr] = Field(default=None, description="The solver for PCA.", alias="pcaSolver")
    sampled_shapley_num_paths: Optional[StrictStr] = Field(default=None, description="Number of paths for the sampled Shapley explain method.", alias="sampledShapleyNumPaths")
    scale_features: Optional[StrictBool] = Field(default=None, description="If true, scale the feature values by dividing the feature standard deviation. Currently only apply to PCA.", alias="scaleFeatures")
    standardize_features: Optional[StrictBool] = Field(default=None, description="Whether to standardize numerical features. Default to true.", alias="standardizeFeatures")
    subsample: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Subsample fraction of the training data to grow tree to prevent overfitting for boosted tree models.")
    tf_version: Optional[StrictStr] = Field(default=None, description="Based on the selected TF version, the corresponding docker image is used to train external models.", alias="tfVersion")
    time_series_data_column: Optional[StrictStr] = Field(default=None, description="Column to be designated as time series data for ARIMA model.", alias="timeSeriesDataColumn")
    time_series_id_column: Optional[StrictStr] = Field(default=None, description="The time series id column that was used during ARIMA model training.", alias="timeSeriesIdColumn")
    time_series_id_columns: Optional[List[StrictStr]] = Field(default=None, description="The time series id columns that were used during ARIMA model training.", alias="timeSeriesIdColumns")
    time_series_length_fraction: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="The fraction of the interpolated length of the time series that's used to model the time series trend component. All of the time points of the time series are used to model the non-trend component. This training option accelerates modeling training without sacrificing much forecasting accuracy. You can use this option with `minTimeSeriesLength` but not with `maxTimeSeriesLength`.", alias="timeSeriesLengthFraction")
    time_series_timestamp_column: Optional[StrictStr] = Field(default=None, description="Column to be designated as time series timestamp for ARIMA model.", alias="timeSeriesTimestampColumn")
    tree_method: Optional[StrictStr] = Field(default=None, description="Tree construction algorithm for boosted tree models.", alias="treeMethod")
    trend_smoothing_window_size: Optional[StrictStr] = Field(default=None, description="Smoothing window size for the trend component. When a positive value is specified, a center moving average smoothing is applied on the history trend. When the smoothing window is out of the boundary at the beginning or the end of the trend, the first element or the last element is padded to fill the smoothing window before the average is applied.", alias="trendSmoothingWindowSize")
    user_column: Optional[StrictStr] = Field(default=None, description="User column specified for matrix factorization models.", alias="userColumn")
    vertex_ai_model_version_aliases: Optional[List[StrictStr]] = Field(default=None, description="The version aliases to apply in Vertex AI model registry. Always overwrite if the version aliases exists in a existing model.", alias="vertexAiModelVersionAliases")
    wals_alpha: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Hyperparameter for matrix factoration when implicit feedback type is specified.", alias="walsAlpha")
    warm_start: Optional[StrictBool] = Field(default=None, description="Whether to train a model from the last checkpoint.", alias="warmStart")
    xgboost_version: Optional[StrictStr] = Field(default=None, description="User-selected XGBoost versions for training of XGBoost models.", alias="xgboostVersion")
    __properties: ClassVar[List[str]] = ["activationFn", "adjustStepChanges", "approxGlobalFeatureContrib", "autoArima", "autoArimaMaxOrder", "autoArimaMinOrder", "autoClassWeights", "batchSize", "boosterType", "budgetHours", "calculatePValues", "categoryEncodingMethod", "cleanSpikesAndDips", "colorSpace", "colsampleBylevel", "colsampleBynode", "colsampleBytree", "dartNormalizeType", "dataFrequency", "dataSplitColumn", "dataSplitEvalFraction", "dataSplitMethod", "decomposeTimeSeries", "distanceType", "dropout", "earlyStop", "enableGlobalExplain", "feedbackType", "fitIntercept", "hiddenUnits", "holidayRegion", "holidayRegions", "horizon", "hparamTuningObjectives", "includeDrift", "initialLearnRate", "inputLabelColumns", "instanceWeightColumn", "integratedGradientsNumSteps", "itemColumn", "kmeansInitializationColumn", "kmeansInitializationMethod", "l1RegActivation", "l1Regularization", "l2Regularization", "labelClassWeights", "learnRate", "learnRateStrategy", "lossType", "maxIterations", "maxParallelTrials", "maxTimeSeriesLength", "maxTreeDepth", "minRelativeProgress", "minSplitLoss", "minTimeSeriesLength", "minTreeChildWeight", "modelRegistry", "modelUri", "nonSeasonalOrder", "numClusters", "numFactors", "numParallelTree", "numPrincipalComponents", "numTrials", "optimizationStrategy", "optimizer", "pcaExplainedVarianceRatio", "pcaSolver", "sampledShapleyNumPaths", "scaleFeatures", "standardizeFeatures", "subsample", "tfVersion", "timeSeriesDataColumn", "timeSeriesIdColumn", "timeSeriesIdColumns", "timeSeriesLengthFraction", "timeSeriesTimestampColumn", "treeMethod", "trendSmoothingWindowSize", "userColumn", "vertexAiModelVersionAliases", "walsAlpha", "warmStart", "xgboostVersion"]

    @field_validator('booster_type')
    def booster_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['BOOSTER_TYPE_UNSPECIFIED', 'GBTREE', 'DART']):
            raise ValueError("must be one of enum values ('BOOSTER_TYPE_UNSPECIFIED', 'GBTREE', 'DART')")
        return value

    @field_validator('category_encoding_method')
    def category_encoding_method_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['ENCODING_METHOD_UNSPECIFIED', 'ONE_HOT_ENCODING', 'LABEL_ENCODING', 'DUMMY_ENCODING']):
            raise ValueError("must be one of enum values ('ENCODING_METHOD_UNSPECIFIED', 'ONE_HOT_ENCODING', 'LABEL_ENCODING', 'DUMMY_ENCODING')")
        return value

    @field_validator('color_space')
    def color_space_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['COLOR_SPACE_UNSPECIFIED', 'RGB', 'HSV', 'YIQ', 'YUV', 'GRAYSCALE']):
            raise ValueError("must be one of enum values ('COLOR_SPACE_UNSPECIFIED', 'RGB', 'HSV', 'YIQ', 'YUV', 'GRAYSCALE')")
        return value

    @field_validator('dart_normalize_type')
    def dart_normalize_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['DART_NORMALIZE_TYPE_UNSPECIFIED', 'TREE', 'FOREST']):
            raise ValueError("must be one of enum values ('DART_NORMALIZE_TYPE_UNSPECIFIED', 'TREE', 'FOREST')")
        return value

    @field_validator('data_frequency')
    def data_frequency_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['DATA_FREQUENCY_UNSPECIFIED', 'AUTO_FREQUENCY', 'YEARLY', 'QUARTERLY', 'MONTHLY', 'WEEKLY', 'DAILY', 'HOURLY', 'PER_MINUTE']):
            raise ValueError("must be one of enum values ('DATA_FREQUENCY_UNSPECIFIED', 'AUTO_FREQUENCY', 'YEARLY', 'QUARTERLY', 'MONTHLY', 'WEEKLY', 'DAILY', 'HOURLY', 'PER_MINUTE')")
        return value

    @field_validator('data_split_method')
    def data_split_method_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['DATA_SPLIT_METHOD_UNSPECIFIED', 'RANDOM', 'CUSTOM', 'SEQUENTIAL', 'NO_SPLIT', 'AUTO_SPLIT']):
            raise ValueError("must be one of enum values ('DATA_SPLIT_METHOD_UNSPECIFIED', 'RANDOM', 'CUSTOM', 'SEQUENTIAL', 'NO_SPLIT', 'AUTO_SPLIT')")
        return value

    @field_validator('distance_type')
    def distance_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['DISTANCE_TYPE_UNSPECIFIED', 'EUCLIDEAN', 'COSINE']):
            raise ValueError("must be one of enum values ('DISTANCE_TYPE_UNSPECIFIED', 'EUCLIDEAN', 'COSINE')")
        return value

    @field_validator('feedback_type')
    def feedback_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['FEEDBACK_TYPE_UNSPECIFIED', 'IMPLICIT', 'EXPLICIT']):
            raise ValueError("must be one of enum values ('FEEDBACK_TYPE_UNSPECIFIED', 'IMPLICIT', 'EXPLICIT')")
        return value

    @field_validator('holiday_region')
    def holiday_region_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['HOLIDAY_REGION_UNSPECIFIED', 'GLOBAL', 'NA', 'JAPAC', 'EMEA', 'LAC', 'AE', 'AR', 'AT', 'AU', 'BE', 'BR', 'CA', 'CH', 'CL', 'CN', 'CO', 'CS', 'CZ', 'DE', 'DK', 'DZ', 'EC', 'EE', 'EG', 'ES', 'FI', 'FR', 'GB', 'GR', 'HK', 'HU', 'ID', 'IE', 'IL', 'IN', 'IR', 'IT', 'JP', 'KR', 'LV', 'MA', 'MX', 'MY', 'NG', 'NL', 'false', 'NZ', 'PE', 'PH', 'PK', 'PL', 'PT', 'RO', 'RS', 'RU', 'SA', 'SE', 'SG', 'SI', 'SK', 'TH', 'TR', 'TW', 'UA', 'US', 'VE', 'VN', 'ZA']):
            raise ValueError("must be one of enum values ('HOLIDAY_REGION_UNSPECIFIED', 'GLOBAL', 'NA', 'JAPAC', 'EMEA', 'LAC', 'AE', 'AR', 'AT', 'AU', 'BE', 'BR', 'CA', 'CH', 'CL', 'CN', 'CO', 'CS', 'CZ', 'DE', 'DK', 'DZ', 'EC', 'EE', 'EG', 'ES', 'FI', 'FR', 'GB', 'GR', 'HK', 'HU', 'ID', 'IE', 'IL', 'IN', 'IR', 'IT', 'JP', 'KR', 'LV', 'MA', 'MX', 'MY', 'NG', 'NL', 'false', 'NZ', 'PE', 'PH', 'PK', 'PL', 'PT', 'RO', 'RS', 'RU', 'SA', 'SE', 'SG', 'SI', 'SK', 'TH', 'TR', 'TW', 'UA', 'US', 'VE', 'VN', 'ZA')")
        return value

    @field_validator('holiday_regions')
    def holiday_regions_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        for i in value:
            if i not in set(['HOLIDAY_REGION_UNSPECIFIED', 'GLOBAL', 'NA', 'JAPAC', 'EMEA', 'LAC', 'AE', 'AR', 'AT', 'AU', 'BE', 'BR', 'CA', 'CH', 'CL', 'CN', 'CO', 'CS', 'CZ', 'DE', 'DK', 'DZ', 'EC', 'EE', 'EG', 'ES', 'FI', 'FR', 'GB', 'GR', 'HK', 'HU', 'ID', 'IE', 'IL', 'IN', 'IR', 'IT', 'JP', 'KR', 'LV', 'MA', 'MX', 'MY', 'NG', 'NL', 'false', 'NZ', 'PE', 'PH', 'PK', 'PL', 'PT', 'RO', 'RS', 'RU', 'SA', 'SE', 'SG', 'SI', 'SK', 'TH', 'TR', 'TW', 'UA', 'US', 'VE', 'VN', 'ZA']):
                raise ValueError("each list item must be one of ('HOLIDAY_REGION_UNSPECIFIED', 'GLOBAL', 'NA', 'JAPAC', 'EMEA', 'LAC', 'AE', 'AR', 'AT', 'AU', 'BE', 'BR', 'CA', 'CH', 'CL', 'CN', 'CO', 'CS', 'CZ', 'DE', 'DK', 'DZ', 'EC', 'EE', 'EG', 'ES', 'FI', 'FR', 'GB', 'GR', 'HK', 'HU', 'ID', 'IE', 'IL', 'IN', 'IR', 'IT', 'JP', 'KR', 'LV', 'MA', 'MX', 'MY', 'NG', 'NL', 'false', 'NZ', 'PE', 'PH', 'PK', 'PL', 'PT', 'RO', 'RS', 'RU', 'SA', 'SE', 'SG', 'SI', 'SK', 'TH', 'TR', 'TW', 'UA', 'US', 'VE', 'VN', 'ZA')")
        return value

    @field_validator('hparam_tuning_objectives')
    def hparam_tuning_objectives_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        for i in value:
            if i not in set(['HPARAM_TUNING_OBJECTIVE_UNSPECIFIED', 'MEAN_ABSOLUTE_ERROR', 'MEAN_SQUARED_ERROR', 'MEAN_SQUARED_LOG_ERROR', 'MEDIAN_ABSOLUTE_ERROR', 'R_SQUARED', 'EXPLAINED_VARIANCE', 'PRECISION', 'RECALL', 'ACCURACY', 'F1_SCORE', 'LOG_LOSS', 'ROC_AUC', 'DAVIES_BOULDIN_INDEX', 'MEAN_AVERAGE_PRECISION', 'NORMALIZED_DISCOUNTED_CUMULATIVE_GAIN', 'AVERAGE_RANK']):
                raise ValueError("each list item must be one of ('HPARAM_TUNING_OBJECTIVE_UNSPECIFIED', 'MEAN_ABSOLUTE_ERROR', 'MEAN_SQUARED_ERROR', 'MEAN_SQUARED_LOG_ERROR', 'MEDIAN_ABSOLUTE_ERROR', 'R_SQUARED', 'EXPLAINED_VARIANCE', 'PRECISION', 'RECALL', 'ACCURACY', 'F1_SCORE', 'LOG_LOSS', 'ROC_AUC', 'DAVIES_BOULDIN_INDEX', 'MEAN_AVERAGE_PRECISION', 'NORMALIZED_DISCOUNTED_CUMULATIVE_GAIN', 'AVERAGE_RANK')")
        return value

    @field_validator('kmeans_initialization_method')
    def kmeans_initialization_method_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['KMEANS_INITIALIZATION_METHOD_UNSPECIFIED', 'RANDOM', 'CUSTOM', 'KMEANS_PLUS_PLUS']):
            raise ValueError("must be one of enum values ('KMEANS_INITIALIZATION_METHOD_UNSPECIFIED', 'RANDOM', 'CUSTOM', 'KMEANS_PLUS_PLUS')")
        return value

    @field_validator('learn_rate_strategy')
    def learn_rate_strategy_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['LEARN_RATE_STRATEGY_UNSPECIFIED', 'LINE_SEARCH', 'CONSTANT']):
            raise ValueError("must be one of enum values ('LEARN_RATE_STRATEGY_UNSPECIFIED', 'LINE_SEARCH', 'CONSTANT')")
        return value

    @field_validator('loss_type')
    def loss_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['LOSS_TYPE_UNSPECIFIED', 'MEAN_SQUARED_LOSS', 'MEAN_LOG_LOSS']):
            raise ValueError("must be one of enum values ('LOSS_TYPE_UNSPECIFIED', 'MEAN_SQUARED_LOSS', 'MEAN_LOG_LOSS')")
        return value

    @field_validator('model_registry')
    def model_registry_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['MODEL_REGISTRY_UNSPECIFIED', 'VERTEX_AI']):
            raise ValueError("must be one of enum values ('MODEL_REGISTRY_UNSPECIFIED', 'VERTEX_AI')")
        return value

    @field_validator('optimization_strategy')
    def optimization_strategy_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['OPTIMIZATION_STRATEGY_UNSPECIFIED', 'BATCH_GRADIENT_DESCENT', 'NORMAL_EQUATION']):
            raise ValueError("must be one of enum values ('OPTIMIZATION_STRATEGY_UNSPECIFIED', 'BATCH_GRADIENT_DESCENT', 'NORMAL_EQUATION')")
        return value

    @field_validator('pca_solver')
    def pca_solver_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['UNSPECIFIED', 'FULL', 'RANDOMIZED', 'AUTO']):
            raise ValueError("must be one of enum values ('UNSPECIFIED', 'FULL', 'RANDOMIZED', 'AUTO')")
        return value

    @field_validator('tree_method')
    def tree_method_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['TREE_METHOD_UNSPECIFIED', 'AUTO', 'EXACT', 'APPROX', 'HIST']):
            raise ValueError("must be one of enum values ('TREE_METHOD_UNSPECIFIED', 'AUTO', 'EXACT', 'APPROX', 'HIST')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of TrainingOptions from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of non_seasonal_order
        if self.non_seasonal_order:
            _dict['nonSeasonalOrder'] = self.non_seasonal_order.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of TrainingOptions from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "activationFn": obj.get("activationFn"),
            "adjustStepChanges": obj.get("adjustStepChanges"),
            "approxGlobalFeatureContrib": obj.get("approxGlobalFeatureContrib"),
            "autoArima": obj.get("autoArima"),
            "autoArimaMaxOrder": obj.get("autoArimaMaxOrder"),
            "autoArimaMinOrder": obj.get("autoArimaMinOrder"),
            "autoClassWeights": obj.get("autoClassWeights"),
            "batchSize": obj.get("batchSize"),
            "boosterType": obj.get("boosterType"),
            "budgetHours": obj.get("budgetHours"),
            "calculatePValues": obj.get("calculatePValues"),
            "categoryEncodingMethod": obj.get("categoryEncodingMethod"),
            "cleanSpikesAndDips": obj.get("cleanSpikesAndDips"),
            "colorSpace": obj.get("colorSpace"),
            "colsampleBylevel": obj.get("colsampleBylevel"),
            "colsampleBynode": obj.get("colsampleBynode"),
            "colsampleBytree": obj.get("colsampleBytree"),
            "dartNormalizeType": obj.get("dartNormalizeType"),
            "dataFrequency": obj.get("dataFrequency"),
            "dataSplitColumn": obj.get("dataSplitColumn"),
            "dataSplitEvalFraction": obj.get("dataSplitEvalFraction"),
            "dataSplitMethod": obj.get("dataSplitMethod"),
            "decomposeTimeSeries": obj.get("decomposeTimeSeries"),
            "distanceType": obj.get("distanceType"),
            "dropout": obj.get("dropout"),
            "earlyStop": obj.get("earlyStop"),
            "enableGlobalExplain": obj.get("enableGlobalExplain"),
            "feedbackType": obj.get("feedbackType"),
            "fitIntercept": obj.get("fitIntercept"),
            "hiddenUnits": obj.get("hiddenUnits"),
            "holidayRegion": obj.get("holidayRegion"),
            "holidayRegions": obj.get("holidayRegions"),
            "horizon": obj.get("horizon"),
            "hparamTuningObjectives": obj.get("hparamTuningObjectives"),
            "includeDrift": obj.get("includeDrift"),
            "initialLearnRate": obj.get("initialLearnRate"),
            "inputLabelColumns": obj.get("inputLabelColumns"),
            "instanceWeightColumn": obj.get("instanceWeightColumn"),
            "integratedGradientsNumSteps": obj.get("integratedGradientsNumSteps"),
            "itemColumn": obj.get("itemColumn"),
            "kmeansInitializationColumn": obj.get("kmeansInitializationColumn"),
            "kmeansInitializationMethod": obj.get("kmeansInitializationMethod"),
            "l1RegActivation": obj.get("l1RegActivation"),
            "l1Regularization": obj.get("l1Regularization"),
            "l2Regularization": obj.get("l2Regularization"),
            "labelClassWeights": obj.get("labelClassWeights"),
            "learnRate": obj.get("learnRate"),
            "learnRateStrategy": obj.get("learnRateStrategy"),
            "lossType": obj.get("lossType"),
            "maxIterations": obj.get("maxIterations"),
            "maxParallelTrials": obj.get("maxParallelTrials"),
            "maxTimeSeriesLength": obj.get("maxTimeSeriesLength"),
            "maxTreeDepth": obj.get("maxTreeDepth"),
            "minRelativeProgress": obj.get("minRelativeProgress"),
            "minSplitLoss": obj.get("minSplitLoss"),
            "minTimeSeriesLength": obj.get("minTimeSeriesLength"),
            "minTreeChildWeight": obj.get("minTreeChildWeight"),
            "modelRegistry": obj.get("modelRegistry"),
            "modelUri": obj.get("modelUri"),
            "nonSeasonalOrder": ArimaOrder.from_dict(obj["nonSeasonalOrder"]) if obj.get("nonSeasonalOrder") is not None else None,
            "numClusters": obj.get("numClusters"),
            "numFactors": obj.get("numFactors"),
            "numParallelTree": obj.get("numParallelTree"),
            "numPrincipalComponents": obj.get("numPrincipalComponents"),
            "numTrials": obj.get("numTrials"),
            "optimizationStrategy": obj.get("optimizationStrategy"),
            "optimizer": obj.get("optimizer"),
            "pcaExplainedVarianceRatio": obj.get("pcaExplainedVarianceRatio"),
            "pcaSolver": obj.get("pcaSolver"),
            "sampledShapleyNumPaths": obj.get("sampledShapleyNumPaths"),
            "scaleFeatures": obj.get("scaleFeatures"),
            "standardizeFeatures": obj.get("standardizeFeatures"),
            "subsample": obj.get("subsample"),
            "tfVersion": obj.get("tfVersion"),
            "timeSeriesDataColumn": obj.get("timeSeriesDataColumn"),
            "timeSeriesIdColumn": obj.get("timeSeriesIdColumn"),
            "timeSeriesIdColumns": obj.get("timeSeriesIdColumns"),
            "timeSeriesLengthFraction": obj.get("timeSeriesLengthFraction"),
            "timeSeriesTimestampColumn": obj.get("timeSeriesTimestampColumn"),
            "treeMethod": obj.get("treeMethod"),
            "trendSmoothingWindowSize": obj.get("trendSmoothingWindowSize"),
            "userColumn": obj.get("userColumn"),
            "vertexAiModelVersionAliases": obj.get("vertexAiModelVersionAliases"),
            "walsAlpha": obj.get("walsAlpha"),
            "warmStart": obj.get("warmStart"),
            "xgboostVersion": obj.get("xgboostVersion")
        })
        return _obj


