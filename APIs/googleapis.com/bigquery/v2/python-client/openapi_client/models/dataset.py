# coding: utf-8

"""
    BigQuery API

    A data platform for customers to create, manage, share and query data.

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.dataset_access_inner import DatasetAccessInner
from openapi_client.models.dataset_reference import DatasetReference
from openapi_client.models.dataset_tags_inner import DatasetTagsInner
from openapi_client.models.encryption_configuration import EncryptionConfiguration
from openapi_client.models.external_dataset_reference import ExternalDatasetReference
from openapi_client.models.linked_dataset_source import LinkedDatasetSource
from typing import Optional, Set
from typing_extensions import Self

class Dataset(BaseModel):
    """
    Dataset
    """ # noqa: E501
    access: Optional[List[DatasetAccessInner]] = Field(default=None, description="Optional. An array of objects that define dataset access for one or more entities. You can set this property when inserting or updating a dataset in order to control who is allowed to access the data. If unspecified at dataset creation time, BigQuery adds default dataset access for the following entities: access.specialGroup: projectReaders; access.role: READER; access.specialGroup: projectWriters; access.role: WRITER; access.specialGroup: projectOwners; access.role: OWNER; access.userByEmail: [dataset creator email]; access.role: OWNER;")
    creation_time: Optional[StrictStr] = Field(default=None, description="Output only. The time when this dataset was created, in milliseconds since the epoch.", alias="creationTime")
    dataset_reference: Optional[DatasetReference] = Field(default=None, alias="datasetReference")
    default_collation: Optional[StrictStr] = Field(default=None, description="Optional. Defines the default collation specification of future tables created in the dataset. If a table is created in this dataset without table-level default collation, then the table inherits the dataset default collation, which is applied to the string fields that do not have explicit collation specified. A change to this field affects only tables created afterwards, and does not alter the existing tables. The following values are supported: * 'und:ci': undetermined locale, case insensitive. * '': empty string. Default to case-sensitive behavior.", alias="defaultCollation")
    default_encryption_configuration: Optional[EncryptionConfiguration] = Field(default=None, alias="defaultEncryptionConfiguration")
    default_partition_expiration_ms: Optional[StrictStr] = Field(default=None, description="This default partition expiration, expressed in milliseconds. When new time-partitioned tables are created in a dataset where this property is set, the table will inherit this value, propagated as the `TimePartitioning.expirationMs` property on the new table. If you set `TimePartitioning.expirationMs` explicitly when creating a table, the `defaultPartitionExpirationMs` of the containing dataset is ignored. When creating a partitioned table, if `defaultPartitionExpirationMs` is set, the `defaultTableExpirationMs` value is ignored and the table will not be inherit a table expiration deadline.", alias="defaultPartitionExpirationMs")
    default_rounding_mode: Optional[StrictStr] = Field(default=None, description="Optional. Defines the default rounding mode specification of new tables created within this dataset. During table creation, if this field is specified, the table within this dataset will inherit the default rounding mode of the dataset. Setting the default rounding mode on a table overrides this option. Existing tables in the dataset are unaffected. If columns are defined during that table creation, they will immediately inherit the table's default rounding mode, unless otherwise specified.", alias="defaultRoundingMode")
    default_table_expiration_ms: Optional[StrictStr] = Field(default=None, description="Optional. The default lifetime of all tables in the dataset, in milliseconds. The minimum lifetime value is 3600000 milliseconds (one hour). To clear an existing default expiration with a PATCH request, set to 0. Once this property is set, all newly-created tables in the dataset will have an expirationTime property set to the creation time plus the value in this property, and changing the value will only affect new tables, not existing ones. When the expirationTime for a given table is reached, that table will be deleted automatically. If a table's expirationTime is modified or removed before the table expires, or if you provide an explicit expirationTime when creating a table, that value takes precedence over the default expiration time indicated by this property.", alias="defaultTableExpirationMs")
    description: Optional[StrictStr] = Field(default=None, description="Optional. A user-friendly description of the dataset.")
    etag: Optional[StrictStr] = Field(default=None, description="Output only. A hash of the resource.")
    external_dataset_reference: Optional[ExternalDatasetReference] = Field(default=None, alias="externalDatasetReference")
    friendly_name: Optional[StrictStr] = Field(default=None, description="Optional. A descriptive name for the dataset.", alias="friendlyName")
    id: Optional[StrictStr] = Field(default=None, description="Output only. The fully-qualified unique name of the dataset in the format projectId:datasetId. The dataset name without the project name is given in the datasetId field. When creating a new dataset, leave this field blank, and instead specify the datasetId field.")
    is_case_insensitive: Optional[StrictBool] = Field(default=None, description="Optional. TRUE if the dataset and its table names are case-insensitive, otherwise FALSE. By default, this is FALSE, which means the dataset and its table names are case-sensitive. This field does not affect routine references.", alias="isCaseInsensitive")
    kind: Optional[StrictStr] = Field(default='bigquery#dataset', description="Output only. The resource type.")
    labels: Optional[Dict[str, StrictStr]] = Field(default=None, description="The labels associated with this dataset. You can use these to organize and group your datasets. You can set this property when inserting or updating a dataset. See Creating and Updating Dataset Labels for more information.")
    last_modified_time: Optional[StrictStr] = Field(default=None, description="Output only. The date when this dataset was last modified, in milliseconds since the epoch.", alias="lastModifiedTime")
    linked_dataset_source: Optional[LinkedDatasetSource] = Field(default=None, alias="linkedDatasetSource")
    location: Optional[StrictStr] = Field(default=None, description="The geographic location where the dataset should reside. See https://cloud.google.com/bigquery/docs/locations for supported locations.")
    max_time_travel_hours: Optional[StrictStr] = Field(default=None, description="Optional. Defines the time travel window in hours. The value can be from 48 to 168 hours (2 to 7 days). The default value is 168 hours if this is not set.", alias="maxTimeTravelHours")
    satisfies_pzi: Optional[StrictBool] = Field(default=None, description="Output only. Reserved for future use.", alias="satisfiesPzi")
    satisfies_pzs: Optional[StrictBool] = Field(default=None, description="Output only. Reserved for future use.", alias="satisfiesPzs")
    self_link: Optional[StrictStr] = Field(default=None, description="Output only. A URL that can be used to access the resource again. You can use this URL in Get or Update requests to the resource.", alias="selfLink")
    storage_billing_model: Optional[StrictStr] = Field(default=None, description="Optional. Updates storage_billing_model for the dataset.", alias="storageBillingModel")
    tags: Optional[List[DatasetTagsInner]] = Field(default=None, description="Output only. Tags for the Dataset.")
    type: Optional[StrictStr] = Field(default=None, description="Output only. Same as `type` in `ListFormatDataset`. The type of the dataset, one of: * DEFAULT - only accessible by owner and authorized accounts, * PUBLIC - accessible by everyone, * LINKED - linked dataset, * EXTERNAL - dataset with definition in external metadata catalog. -- *BIGLAKE_METASTORE - dataset that references a database created in BigLakeMetastore service. --")
    __properties: ClassVar[List[str]] = ["access", "creationTime", "datasetReference", "defaultCollation", "defaultEncryptionConfiguration", "defaultPartitionExpirationMs", "defaultRoundingMode", "defaultTableExpirationMs", "description", "etag", "externalDatasetReference", "friendlyName", "id", "isCaseInsensitive", "kind", "labels", "lastModifiedTime", "linkedDatasetSource", "location", "maxTimeTravelHours", "satisfiesPzi", "satisfiesPzs", "selfLink", "storageBillingModel", "tags", "type"]

    @field_validator('default_rounding_mode')
    def default_rounding_mode_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['ROUNDING_MODE_UNSPECIFIED', 'ROUND_HALF_AWAY_FROM_ZERO', 'ROUND_HALF_EVEN']):
            raise ValueError("must be one of enum values ('ROUNDING_MODE_UNSPECIFIED', 'ROUND_HALF_AWAY_FROM_ZERO', 'ROUND_HALF_EVEN')")
        return value

    @field_validator('storage_billing_model')
    def storage_billing_model_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['STORAGE_BILLING_MODEL_UNSPECIFIED', 'LOGICAL', 'PHYSICAL']):
            raise ValueError("must be one of enum values ('STORAGE_BILLING_MODEL_UNSPECIFIED', 'LOGICAL', 'PHYSICAL')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of Dataset from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "creation_time",
            "etag",
            "id",
            "kind",
            "last_modified_time",
            "satisfies_pzi",
            "satisfies_pzs",
            "self_link",
            "tags",
            "type",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in access (list)
        _items = []
        if self.access:
            for _item_access in self.access:
                if _item_access:
                    _items.append(_item_access.to_dict())
            _dict['access'] = _items
        # override the default output from pydantic by calling `to_dict()` of dataset_reference
        if self.dataset_reference:
            _dict['datasetReference'] = self.dataset_reference.to_dict()
        # override the default output from pydantic by calling `to_dict()` of default_encryption_configuration
        if self.default_encryption_configuration:
            _dict['defaultEncryptionConfiguration'] = self.default_encryption_configuration.to_dict()
        # override the default output from pydantic by calling `to_dict()` of external_dataset_reference
        if self.external_dataset_reference:
            _dict['externalDatasetReference'] = self.external_dataset_reference.to_dict()
        # override the default output from pydantic by calling `to_dict()` of linked_dataset_source
        if self.linked_dataset_source:
            _dict['linkedDatasetSource'] = self.linked_dataset_source.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in tags (list)
        _items = []
        if self.tags:
            for _item_tags in self.tags:
                if _item_tags:
                    _items.append(_item_tags.to_dict())
            _dict['tags'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of Dataset from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "access": [DatasetAccessInner.from_dict(_item) for _item in obj["access"]] if obj.get("access") is not None else None,
            "creationTime": obj.get("creationTime"),
            "datasetReference": DatasetReference.from_dict(obj["datasetReference"]) if obj.get("datasetReference") is not None else None,
            "defaultCollation": obj.get("defaultCollation"),
            "defaultEncryptionConfiguration": EncryptionConfiguration.from_dict(obj["defaultEncryptionConfiguration"]) if obj.get("defaultEncryptionConfiguration") is not None else None,
            "defaultPartitionExpirationMs": obj.get("defaultPartitionExpirationMs"),
            "defaultRoundingMode": obj.get("defaultRoundingMode"),
            "defaultTableExpirationMs": obj.get("defaultTableExpirationMs"),
            "description": obj.get("description"),
            "etag": obj.get("etag"),
            "externalDatasetReference": ExternalDatasetReference.from_dict(obj["externalDatasetReference"]) if obj.get("externalDatasetReference") is not None else None,
            "friendlyName": obj.get("friendlyName"),
            "id": obj.get("id"),
            "isCaseInsensitive": obj.get("isCaseInsensitive"),
            "kind": obj.get("kind") if obj.get("kind") is not None else 'bigquery#dataset',
            "labels": obj.get("labels"),
            "lastModifiedTime": obj.get("lastModifiedTime"),
            "linkedDatasetSource": LinkedDatasetSource.from_dict(obj["linkedDatasetSource"]) if obj.get("linkedDatasetSource") is not None else None,
            "location": obj.get("location"),
            "maxTimeTravelHours": obj.get("maxTimeTravelHours"),
            "satisfiesPzi": obj.get("satisfiesPzi"),
            "satisfiesPzs": obj.get("satisfiesPzs"),
            "selfLink": obj.get("selfLink"),
            "storageBillingModel": obj.get("storageBillingModel"),
            "tags": [DatasetTagsInner.from_dict(_item) for _item in obj["tags"]] if obj.get("tags") is not None else None,
            "type": obj.get("type")
        })
        return _obj


