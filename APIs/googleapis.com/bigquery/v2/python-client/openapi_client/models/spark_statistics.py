# coding: utf-8

"""
    BigQuery API

    A data platform for customers to create, manage, share and query data.

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.spark_logging_info import SparkLoggingInfo
from typing import Optional, Set
from typing_extensions import Self

class SparkStatistics(BaseModel):
    """
    Statistics for a BigSpark query. Populated as part of JobStatistics2
    """ # noqa: E501
    endpoints: Optional[Dict[str, StrictStr]] = Field(default=None, description="Output only. Endpoints returned from Dataproc. Key list: - history_server_endpoint: A link to Spark job UI.")
    gcs_staging_bucket: Optional[StrictStr] = Field(default=None, description="Output only. The Google Cloud Storage bucket that is used as the default filesystem by the Spark application. This fields is only filled when the Spark procedure uses the INVOKER security mode. It is inferred from the system variable @@spark_proc_properties.staging_bucket if it is provided. Otherwise, BigQuery creates a default staging bucket for the job and returns the bucket name in this field. Example: * `gs://[bucket_name]`", alias="gcsStagingBucket")
    kms_key_name: Optional[StrictStr] = Field(default=None, description="Output only. The Cloud KMS encryption key that is used to protect the resources created by the Spark job. If the Spark procedure uses DEFINER security mode, the Cloud KMS key is inferred from the Spark connection associated with the procedure if it is provided. Otherwise the key is inferred from the default key of the Spark connection's project if the CMEK organization policy is enforced. If the Spark procedure uses INVOKER security mode, the Cloud KMS encryption key is inferred from the system variable @@spark_proc_properties.kms_key_name if it is provided. Otherwise, the key is inferred fromt he default key of the BigQuery job's project if the CMEK organization policy is enforced. Example: * `projects/[kms_project_id]/locations/[region]/keyRings/[key_region]/cryptoKeys/[key]`", alias="kmsKeyName")
    logging_info: Optional[SparkLoggingInfo] = Field(default=None, alias="loggingInfo")
    spark_job_id: Optional[StrictStr] = Field(default=None, description="Output only. Spark job ID if a Spark job is created successfully.", alias="sparkJobId")
    spark_job_location: Optional[StrictStr] = Field(default=None, description="Output only. Location where the Spark job is executed. A location is selected by BigQueury for jobs configured to run in a multi-region.", alias="sparkJobLocation")
    __properties: ClassVar[List[str]] = ["endpoints", "gcsStagingBucket", "kmsKeyName", "loggingInfo", "sparkJobId", "sparkJobLocation"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of SparkStatistics from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "endpoints",
            "gcs_staging_bucket",
            "kms_key_name",
            "spark_job_id",
            "spark_job_location",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of logging_info
        if self.logging_info:
            _dict['loggingInfo'] = self.logging_info.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of SparkStatistics from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "endpoints": obj.get("endpoints"),
            "gcsStagingBucket": obj.get("gcsStagingBucket"),
            "kmsKeyName": obj.get("kmsKeyName"),
            "loggingInfo": SparkLoggingInfo.from_dict(obj["loggingInfo"]) if obj.get("loggingInfo") is not None else None,
            "sparkJobId": obj.get("sparkJobId"),
            "sparkJobLocation": obj.get("sparkJobLocation")
        })
        return _obj


