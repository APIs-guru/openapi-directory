# coding: utf-8

"""
    BigQuery API

    A data platform for customers to create, manage, share and query data.

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.bi_engine_statistics import BiEngineStatistics
from openapi_client.models.big_query_model_training import BigQueryModelTraining
from openapi_client.models.dataset_reference import DatasetReference
from openapi_client.models.dml_statistics import DmlStatistics
from openapi_client.models.explain_query_stage import ExplainQueryStage
from openapi_client.models.export_data_statistics import ExportDataStatistics
from openapi_client.models.external_service_cost import ExternalServiceCost
from openapi_client.models.job_statistics_reservation_usage_inner import JobStatisticsReservationUsageInner
from openapi_client.models.load_query_statistics import LoadQueryStatistics
from openapi_client.models.materialized_view_statistics import MaterializedViewStatistics
from openapi_client.models.metadata_cache_statistics import MetadataCacheStatistics
from openapi_client.models.ml_statistics import MlStatistics
from openapi_client.models.performance_insights import PerformanceInsights
from openapi_client.models.query_info import QueryInfo
from openapi_client.models.query_parameter import QueryParameter
from openapi_client.models.query_timeline_sample import QueryTimelineSample
from openapi_client.models.routine_reference import RoutineReference
from openapi_client.models.row_access_policy_reference import RowAccessPolicyReference
from openapi_client.models.search_statistics import SearchStatistics
from openapi_client.models.spark_statistics import SparkStatistics
from openapi_client.models.table_reference import TableReference
from openapi_client.models.table_schema import TableSchema
from openapi_client.models.vector_search_statistics import VectorSearchStatistics
from typing import Optional, Set
from typing_extensions import Self

class JobStatistics2(BaseModel):
    """
    Statistics for a query job.
    """ # noqa: E501
    bi_engine_statistics: Optional[BiEngineStatistics] = Field(default=None, alias="biEngineStatistics")
    billing_tier: Optional[StrictInt] = Field(default=None, description="Output only. Billing tier for the job. This is a BigQuery-specific concept which is not related to the Google Cloud notion of \"free tier\". The value here is a measure of the query's resource consumption relative to the amount of data scanned. For on-demand queries, the limit is 100, and all queries within this limit are billed at the standard on-demand rates. On-demand queries that exceed this limit will fail with a billingTierLimitExceeded error.", alias="billingTier")
    cache_hit: Optional[StrictBool] = Field(default=None, description="Output only. Whether the query result was fetched from the query cache.", alias="cacheHit")
    dcl_target_dataset: Optional[DatasetReference] = Field(default=None, alias="dclTargetDataset")
    dcl_target_table: Optional[TableReference] = Field(default=None, alias="dclTargetTable")
    dcl_target_view: Optional[TableReference] = Field(default=None, alias="dclTargetView")
    ddl_affected_row_access_policy_count: Optional[StrictStr] = Field(default=None, description="Output only. The number of row access policies affected by a DDL statement. Present only for DROP ALL ROW ACCESS POLICIES queries.", alias="ddlAffectedRowAccessPolicyCount")
    ddl_destination_table: Optional[TableReference] = Field(default=None, alias="ddlDestinationTable")
    ddl_operation_performed: Optional[StrictStr] = Field(default=None, description="Output only. The DDL operation performed, possibly dependent on the pre-existence of the DDL target.", alias="ddlOperationPerformed")
    ddl_target_dataset: Optional[DatasetReference] = Field(default=None, alias="ddlTargetDataset")
    ddl_target_routine: Optional[RoutineReference] = Field(default=None, alias="ddlTargetRoutine")
    ddl_target_row_access_policy: Optional[RowAccessPolicyReference] = Field(default=None, alias="ddlTargetRowAccessPolicy")
    ddl_target_table: Optional[TableReference] = Field(default=None, alias="ddlTargetTable")
    dml_stats: Optional[DmlStatistics] = Field(default=None, alias="dmlStats")
    estimated_bytes_processed: Optional[StrictStr] = Field(default=None, description="Output only. The original estimate of bytes processed for the job.", alias="estimatedBytesProcessed")
    export_data_statistics: Optional[ExportDataStatistics] = Field(default=None, alias="exportDataStatistics")
    external_service_costs: Optional[List[ExternalServiceCost]] = Field(default=None, description="Output only. Job cost breakdown as bigquery internal cost and external service costs.", alias="externalServiceCosts")
    load_query_statistics: Optional[LoadQueryStatistics] = Field(default=None, alias="loadQueryStatistics")
    materialized_view_statistics: Optional[MaterializedViewStatistics] = Field(default=None, alias="materializedViewStatistics")
    metadata_cache_statistics: Optional[MetadataCacheStatistics] = Field(default=None, alias="metadataCacheStatistics")
    ml_statistics: Optional[MlStatistics] = Field(default=None, alias="mlStatistics")
    model_training: Optional[BigQueryModelTraining] = Field(default=None, alias="modelTraining")
    model_training_current_iteration: Optional[StrictInt] = Field(default=None, description="Deprecated.", alias="modelTrainingCurrentIteration")
    model_training_expected_total_iteration: Optional[StrictStr] = Field(default=None, description="Deprecated.", alias="modelTrainingExpectedTotalIteration")
    num_dml_affected_rows: Optional[StrictStr] = Field(default=None, description="Output only. The number of rows affected by a DML statement. Present only for DML statements INSERT, UPDATE or DELETE.", alias="numDmlAffectedRows")
    performance_insights: Optional[PerformanceInsights] = Field(default=None, alias="performanceInsights")
    query_info: Optional[QueryInfo] = Field(default=None, alias="queryInfo")
    query_plan: Optional[List[ExplainQueryStage]] = Field(default=None, description="Output only. Describes execution plan for the query.", alias="queryPlan")
    referenced_routines: Optional[List[RoutineReference]] = Field(default=None, description="Output only. Referenced routines for the job.", alias="referencedRoutines")
    referenced_tables: Optional[List[TableReference]] = Field(default=None, description="Output only. Referenced tables for the job. Queries that reference more than 50 tables will not have a complete list.", alias="referencedTables")
    reservation_usage: Optional[List[JobStatisticsReservationUsageInner]] = Field(default=None, description="Output only. Job resource usage breakdown by reservation. This field reported misleading information and will no longer be populated.", alias="reservationUsage")
    var_schema: Optional[TableSchema] = Field(default=None, alias="schema")
    search_statistics: Optional[SearchStatistics] = Field(default=None, alias="searchStatistics")
    spark_statistics: Optional[SparkStatistics] = Field(default=None, alias="sparkStatistics")
    statement_type: Optional[StrictStr] = Field(default=None, description="Output only. The type of query statement, if valid. Possible values: * `SELECT`: [`SELECT`](/bigquery/docs/reference/standard-sql/query-syntax#select_list) statement. * `ASSERT`: [`ASSERT`](/bigquery/docs/reference/standard-sql/debugging-statements#assert) statement. * `INSERT`: [`INSERT`](/bigquery/docs/reference/standard-sql/dml-syntax#insert_statement) statement. * `UPDATE`: [`UPDATE`](/bigquery/docs/reference/standard-sql/query-syntax#update_statement) statement. * `DELETE`: [`DELETE`](/bigquery/docs/reference/standard-sql/data-manipulation-language) statement. * `MERGE`: [`MERGE`](/bigquery/docs/reference/standard-sql/data-manipulation-language) statement. * `CREATE_TABLE`: [`CREATE TABLE`](/bigquery/docs/reference/standard-sql/data-definition-language#create_table_statement) statement, without `AS SELECT`. * `CREATE_TABLE_AS_SELECT`: [`CREATE TABLE AS SELECT`](/bigquery/docs/reference/standard-sql/data-definition-language#query_statement) statement. * `CREATE_VIEW`: [`CREATE VIEW`](/bigquery/docs/reference/standard-sql/data-definition-language#create_view_statement) statement. * `CREATE_MODEL`: [`CREATE MODEL`](/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create#create_model_statement) statement. * `CREATE_MATERIALIZED_VIEW`: [`CREATE MATERIALIZED VIEW`](/bigquery/docs/reference/standard-sql/data-definition-language#create_materialized_view_statement) statement. * `CREATE_FUNCTION`: [`CREATE FUNCTION`](/bigquery/docs/reference/standard-sql/data-definition-language#create_function_statement) statement. * `CREATE_TABLE_FUNCTION`: [`CREATE TABLE FUNCTION`](/bigquery/docs/reference/standard-sql/data-definition-language#create_table_function_statement) statement. * `CREATE_PROCEDURE`: [`CREATE PROCEDURE`](/bigquery/docs/reference/standard-sql/data-definition-language#create_procedure) statement. * `CREATE_ROW_ACCESS_POLICY`: [`CREATE ROW ACCESS POLICY`](/bigquery/docs/reference/standard-sql/data-definition-language#create_row_access_policy_statement) statement. * `CREATE_SCHEMA`: [`CREATE SCHEMA`](/bigquery/docs/reference/standard-sql/data-definition-language#create_schema_statement) statement. * `CREATE_SNAPSHOT_TABLE`: [`CREATE SNAPSHOT TABLE`](/bigquery/docs/reference/standard-sql/data-definition-language#create_snapshot_table_statement) statement. * `CREATE_SEARCH_INDEX`: [`CREATE SEARCH INDEX`](/bigquery/docs/reference/standard-sql/data-definition-language#create_search_index_statement) statement. * `DROP_TABLE`: [`DROP TABLE`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_table_statement) statement. * `DROP_EXTERNAL_TABLE`: [`DROP EXTERNAL TABLE`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_external_table_statement) statement. * `DROP_VIEW`: [`DROP VIEW`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_view_statement) statement. * `DROP_MODEL`: [`DROP MODEL`](/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-drop-model) statement. * `DROP_MATERIALIZED_VIEW`: [`DROP MATERIALIZED VIEW`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_materialized_view_statement) statement. * `DROP_FUNCTION` : [`DROP FUNCTION`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_function_statement) statement. * `DROP_TABLE_FUNCTION` : [`DROP TABLE FUNCTION`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_table_function) statement. * `DROP_PROCEDURE`: [`DROP PROCEDURE`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_procedure_statement) statement. * `DROP_SEARCH_INDEX`: [`DROP SEARCH INDEX`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_search_index) statement. * `DROP_SCHEMA`: [`DROP SCHEMA`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_schema_statement) statement. * `DROP_SNAPSHOT_TABLE`: [`DROP SNAPSHOT TABLE`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_snapshot_table_statement) statement. * `DROP_ROW_ACCESS_POLICY`: [`DROP [ALL] ROW ACCESS POLICY|POLICIES`](/bigquery/docs/reference/standard-sql/data-definition-language#drop_row_access_policy_statement) statement. * `ALTER_TABLE`: [`ALTER TABLE`](/bigquery/docs/reference/standard-sql/data-definition-language#alter_table_set_options_statement) statement. * `ALTER_VIEW`: [`ALTER VIEW`](/bigquery/docs/reference/standard-sql/data-definition-language#alter_view_set_options_statement) statement. * `ALTER_MATERIALIZED_VIEW`: [`ALTER MATERIALIZED VIEW`](/bigquery/docs/reference/standard-sql/data-definition-language#alter_materialized_view_set_options_statement) statement. * `ALTER_SCHEMA`: [`ALTER SCHEMA`](/bigquery/docs/reference/standard-sql/data-definition-language#aalter_schema_set_options_statement) statement. * `SCRIPT`: [`SCRIPT`](/bigquery/docs/reference/standard-sql/procedural-language). * `TRUNCATE_TABLE`: [`TRUNCATE TABLE`](/bigquery/docs/reference/standard-sql/dml-syntax#truncate_table_statement) statement. * `CREATE_EXTERNAL_TABLE`: [`CREATE EXTERNAL TABLE`](/bigquery/docs/reference/standard-sql/data-definition-language#create_external_table_statement) statement. * `EXPORT_DATA`: [`EXPORT DATA`](/bigquery/docs/reference/standard-sql/other-statements#export_data_statement) statement. * `EXPORT_MODEL`: [`EXPORT MODEL`](/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-export-model) statement. * `LOAD_DATA`: [`LOAD DATA`](/bigquery/docs/reference/standard-sql/other-statements#load_data_statement) statement. * `CALL`: [`CALL`](/bigquery/docs/reference/standard-sql/procedural-language#call) statement.", alias="statementType")
    timeline: Optional[List[QueryTimelineSample]] = Field(default=None, description="Output only. Describes a timeline of job execution.")
    total_bytes_billed: Optional[StrictStr] = Field(default=None, description="Output only. If the project is configured to use on-demand pricing, then this field contains the total bytes billed for the job. If the project is configured to use flat-rate pricing, then you are not billed for bytes and this field is informational only.", alias="totalBytesBilled")
    total_bytes_processed: Optional[StrictStr] = Field(default=None, description="Output only. Total bytes processed for the job.", alias="totalBytesProcessed")
    total_bytes_processed_accuracy: Optional[StrictStr] = Field(default=None, description="Output only. For dry-run jobs, totalBytesProcessed is an estimate and this field specifies the accuracy of the estimate. Possible values can be: UNKNOWN: accuracy of the estimate is unknown. PRECISE: estimate is precise. LOWER_BOUND: estimate is lower bound of what the query would cost. UPPER_BOUND: estimate is upper bound of what the query would cost.", alias="totalBytesProcessedAccuracy")
    total_partitions_processed: Optional[StrictStr] = Field(default=None, description="Output only. Total number of partitions processed from all partitioned tables referenced in the job.", alias="totalPartitionsProcessed")
    total_slot_ms: Optional[StrictStr] = Field(default=None, description="Output only. Slot-milliseconds for the job.", alias="totalSlotMs")
    transferred_bytes: Optional[StrictStr] = Field(default=None, description="Output only. Total bytes transferred for cross-cloud queries such as Cross Cloud Transfer and CREATE TABLE AS SELECT (CTAS).", alias="transferredBytes")
    undeclared_query_parameters: Optional[List[QueryParameter]] = Field(default=None, description="Output only. GoogleSQL only: list of undeclared query parameters detected during a dry run validation.", alias="undeclaredQueryParameters")
    vector_search_statistics: Optional[VectorSearchStatistics] = Field(default=None, alias="vectorSearchStatistics")
    __properties: ClassVar[List[str]] = ["biEngineStatistics", "billingTier", "cacheHit", "dclTargetDataset", "dclTargetTable", "dclTargetView", "ddlAffectedRowAccessPolicyCount", "ddlDestinationTable", "ddlOperationPerformed", "ddlTargetDataset", "ddlTargetRoutine", "ddlTargetRowAccessPolicy", "ddlTargetTable", "dmlStats", "estimatedBytesProcessed", "exportDataStatistics", "externalServiceCosts", "loadQueryStatistics", "materializedViewStatistics", "metadataCacheStatistics", "mlStatistics", "modelTraining", "modelTrainingCurrentIteration", "modelTrainingExpectedTotalIteration", "numDmlAffectedRows", "performanceInsights", "queryInfo", "queryPlan", "referencedRoutines", "referencedTables", "reservationUsage", "schema", "searchStatistics", "sparkStatistics", "statementType", "timeline", "totalBytesBilled", "totalBytesProcessed", "totalBytesProcessedAccuracy", "totalPartitionsProcessed", "totalSlotMs", "transferredBytes", "undeclaredQueryParameters", "vectorSearchStatistics"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of JobStatistics2 from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "billing_tier",
            "cache_hit",
            "ddl_affected_row_access_policy_count",
            "ddl_operation_performed",
            "estimated_bytes_processed",
            "external_service_costs",
            "num_dml_affected_rows",
            "query_plan",
            "referenced_routines",
            "referenced_tables",
            "reservation_usage",
            "statement_type",
            "timeline",
            "total_bytes_billed",
            "total_bytes_processed",
            "total_bytes_processed_accuracy",
            "total_partitions_processed",
            "total_slot_ms",
            "transferred_bytes",
            "undeclared_query_parameters",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of bi_engine_statistics
        if self.bi_engine_statistics:
            _dict['biEngineStatistics'] = self.bi_engine_statistics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of dcl_target_dataset
        if self.dcl_target_dataset:
            _dict['dclTargetDataset'] = self.dcl_target_dataset.to_dict()
        # override the default output from pydantic by calling `to_dict()` of dcl_target_table
        if self.dcl_target_table:
            _dict['dclTargetTable'] = self.dcl_target_table.to_dict()
        # override the default output from pydantic by calling `to_dict()` of dcl_target_view
        if self.dcl_target_view:
            _dict['dclTargetView'] = self.dcl_target_view.to_dict()
        # override the default output from pydantic by calling `to_dict()` of ddl_destination_table
        if self.ddl_destination_table:
            _dict['ddlDestinationTable'] = self.ddl_destination_table.to_dict()
        # override the default output from pydantic by calling `to_dict()` of ddl_target_dataset
        if self.ddl_target_dataset:
            _dict['ddlTargetDataset'] = self.ddl_target_dataset.to_dict()
        # override the default output from pydantic by calling `to_dict()` of ddl_target_routine
        if self.ddl_target_routine:
            _dict['ddlTargetRoutine'] = self.ddl_target_routine.to_dict()
        # override the default output from pydantic by calling `to_dict()` of ddl_target_row_access_policy
        if self.ddl_target_row_access_policy:
            _dict['ddlTargetRowAccessPolicy'] = self.ddl_target_row_access_policy.to_dict()
        # override the default output from pydantic by calling `to_dict()` of ddl_target_table
        if self.ddl_target_table:
            _dict['ddlTargetTable'] = self.ddl_target_table.to_dict()
        # override the default output from pydantic by calling `to_dict()` of dml_stats
        if self.dml_stats:
            _dict['dmlStats'] = self.dml_stats.to_dict()
        # override the default output from pydantic by calling `to_dict()` of export_data_statistics
        if self.export_data_statistics:
            _dict['exportDataStatistics'] = self.export_data_statistics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in external_service_costs (list)
        _items = []
        if self.external_service_costs:
            for _item_external_service_costs in self.external_service_costs:
                if _item_external_service_costs:
                    _items.append(_item_external_service_costs.to_dict())
            _dict['externalServiceCosts'] = _items
        # override the default output from pydantic by calling `to_dict()` of load_query_statistics
        if self.load_query_statistics:
            _dict['loadQueryStatistics'] = self.load_query_statistics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of materialized_view_statistics
        if self.materialized_view_statistics:
            _dict['materializedViewStatistics'] = self.materialized_view_statistics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of metadata_cache_statistics
        if self.metadata_cache_statistics:
            _dict['metadataCacheStatistics'] = self.metadata_cache_statistics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of ml_statistics
        if self.ml_statistics:
            _dict['mlStatistics'] = self.ml_statistics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of model_training
        if self.model_training:
            _dict['modelTraining'] = self.model_training.to_dict()
        # override the default output from pydantic by calling `to_dict()` of performance_insights
        if self.performance_insights:
            _dict['performanceInsights'] = self.performance_insights.to_dict()
        # override the default output from pydantic by calling `to_dict()` of query_info
        if self.query_info:
            _dict['queryInfo'] = self.query_info.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in query_plan (list)
        _items = []
        if self.query_plan:
            for _item_query_plan in self.query_plan:
                if _item_query_plan:
                    _items.append(_item_query_plan.to_dict())
            _dict['queryPlan'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in referenced_routines (list)
        _items = []
        if self.referenced_routines:
            for _item_referenced_routines in self.referenced_routines:
                if _item_referenced_routines:
                    _items.append(_item_referenced_routines.to_dict())
            _dict['referencedRoutines'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in referenced_tables (list)
        _items = []
        if self.referenced_tables:
            for _item_referenced_tables in self.referenced_tables:
                if _item_referenced_tables:
                    _items.append(_item_referenced_tables.to_dict())
            _dict['referencedTables'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in reservation_usage (list)
        _items = []
        if self.reservation_usage:
            for _item_reservation_usage in self.reservation_usage:
                if _item_reservation_usage:
                    _items.append(_item_reservation_usage.to_dict())
            _dict['reservationUsage'] = _items
        # override the default output from pydantic by calling `to_dict()` of var_schema
        if self.var_schema:
            _dict['schema'] = self.var_schema.to_dict()
        # override the default output from pydantic by calling `to_dict()` of search_statistics
        if self.search_statistics:
            _dict['searchStatistics'] = self.search_statistics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark_statistics
        if self.spark_statistics:
            _dict['sparkStatistics'] = self.spark_statistics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in timeline (list)
        _items = []
        if self.timeline:
            for _item_timeline in self.timeline:
                if _item_timeline:
                    _items.append(_item_timeline.to_dict())
            _dict['timeline'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in undeclared_query_parameters (list)
        _items = []
        if self.undeclared_query_parameters:
            for _item_undeclared_query_parameters in self.undeclared_query_parameters:
                if _item_undeclared_query_parameters:
                    _items.append(_item_undeclared_query_parameters.to_dict())
            _dict['undeclaredQueryParameters'] = _items
        # override the default output from pydantic by calling `to_dict()` of vector_search_statistics
        if self.vector_search_statistics:
            _dict['vectorSearchStatistics'] = self.vector_search_statistics.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of JobStatistics2 from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "biEngineStatistics": BiEngineStatistics.from_dict(obj["biEngineStatistics"]) if obj.get("biEngineStatistics") is not None else None,
            "billingTier": obj.get("billingTier"),
            "cacheHit": obj.get("cacheHit"),
            "dclTargetDataset": DatasetReference.from_dict(obj["dclTargetDataset"]) if obj.get("dclTargetDataset") is not None else None,
            "dclTargetTable": TableReference.from_dict(obj["dclTargetTable"]) if obj.get("dclTargetTable") is not None else None,
            "dclTargetView": TableReference.from_dict(obj["dclTargetView"]) if obj.get("dclTargetView") is not None else None,
            "ddlAffectedRowAccessPolicyCount": obj.get("ddlAffectedRowAccessPolicyCount"),
            "ddlDestinationTable": TableReference.from_dict(obj["ddlDestinationTable"]) if obj.get("ddlDestinationTable") is not None else None,
            "ddlOperationPerformed": obj.get("ddlOperationPerformed"),
            "ddlTargetDataset": DatasetReference.from_dict(obj["ddlTargetDataset"]) if obj.get("ddlTargetDataset") is not None else None,
            "ddlTargetRoutine": RoutineReference.from_dict(obj["ddlTargetRoutine"]) if obj.get("ddlTargetRoutine") is not None else None,
            "ddlTargetRowAccessPolicy": RowAccessPolicyReference.from_dict(obj["ddlTargetRowAccessPolicy"]) if obj.get("ddlTargetRowAccessPolicy") is not None else None,
            "ddlTargetTable": TableReference.from_dict(obj["ddlTargetTable"]) if obj.get("ddlTargetTable") is not None else None,
            "dmlStats": DmlStatistics.from_dict(obj["dmlStats"]) if obj.get("dmlStats") is not None else None,
            "estimatedBytesProcessed": obj.get("estimatedBytesProcessed"),
            "exportDataStatistics": ExportDataStatistics.from_dict(obj["exportDataStatistics"]) if obj.get("exportDataStatistics") is not None else None,
            "externalServiceCosts": [ExternalServiceCost.from_dict(_item) for _item in obj["externalServiceCosts"]] if obj.get("externalServiceCosts") is not None else None,
            "loadQueryStatistics": LoadQueryStatistics.from_dict(obj["loadQueryStatistics"]) if obj.get("loadQueryStatistics") is not None else None,
            "materializedViewStatistics": MaterializedViewStatistics.from_dict(obj["materializedViewStatistics"]) if obj.get("materializedViewStatistics") is not None else None,
            "metadataCacheStatistics": MetadataCacheStatistics.from_dict(obj["metadataCacheStatistics"]) if obj.get("metadataCacheStatistics") is not None else None,
            "mlStatistics": MlStatistics.from_dict(obj["mlStatistics"]) if obj.get("mlStatistics") is not None else None,
            "modelTraining": BigQueryModelTraining.from_dict(obj["modelTraining"]) if obj.get("modelTraining") is not None else None,
            "modelTrainingCurrentIteration": obj.get("modelTrainingCurrentIteration"),
            "modelTrainingExpectedTotalIteration": obj.get("modelTrainingExpectedTotalIteration"),
            "numDmlAffectedRows": obj.get("numDmlAffectedRows"),
            "performanceInsights": PerformanceInsights.from_dict(obj["performanceInsights"]) if obj.get("performanceInsights") is not None else None,
            "queryInfo": QueryInfo.from_dict(obj["queryInfo"]) if obj.get("queryInfo") is not None else None,
            "queryPlan": [ExplainQueryStage.from_dict(_item) for _item in obj["queryPlan"]] if obj.get("queryPlan") is not None else None,
            "referencedRoutines": [RoutineReference.from_dict(_item) for _item in obj["referencedRoutines"]] if obj.get("referencedRoutines") is not None else None,
            "referencedTables": [TableReference.from_dict(_item) for _item in obj["referencedTables"]] if obj.get("referencedTables") is not None else None,
            "reservationUsage": [JobStatisticsReservationUsageInner.from_dict(_item) for _item in obj["reservationUsage"]] if obj.get("reservationUsage") is not None else None,
            "schema": TableSchema.from_dict(obj["schema"]) if obj.get("schema") is not None else None,
            "searchStatistics": SearchStatistics.from_dict(obj["searchStatistics"]) if obj.get("searchStatistics") is not None else None,
            "sparkStatistics": SparkStatistics.from_dict(obj["sparkStatistics"]) if obj.get("sparkStatistics") is not None else None,
            "statementType": obj.get("statementType"),
            "timeline": [QueryTimelineSample.from_dict(_item) for _item in obj["timeline"]] if obj.get("timeline") is not None else None,
            "totalBytesBilled": obj.get("totalBytesBilled"),
            "totalBytesProcessed": obj.get("totalBytesProcessed"),
            "totalBytesProcessedAccuracy": obj.get("totalBytesProcessedAccuracy"),
            "totalPartitionsProcessed": obj.get("totalPartitionsProcessed"),
            "totalSlotMs": obj.get("totalSlotMs"),
            "transferredBytes": obj.get("transferredBytes"),
            "undeclaredQueryParameters": [QueryParameter.from_dict(_item) for _item in obj["undeclaredQueryParameters"]] if obj.get("undeclaredQueryParameters") is not None else None,
            "vectorSearchStatistics": VectorSearchStatistics.from_dict(obj["vectorSearchStatistics"]) if obj.get("vectorSearchStatistics") is not None else None
        })
        return _obj


