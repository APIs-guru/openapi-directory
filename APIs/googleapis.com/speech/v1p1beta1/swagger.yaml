swagger: '2.0'
schemes:
  - https
host: speech.googleapis.com
basePath: /
info:
  contact:
    name: Google
    url: 'https://google.com'
  description: Converts audio to text by applying powerful neural network models.
  title: Cloud Speech-to-Text
  version: v1p1beta1
  x-apiClientRegistration:
    url: 'https://console.developers.google.com'
  x-apisguru-categories:
    - machine_learning
  x-logo:
    url: 'https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png'
  x-origin:
    - converter:
        url: 'https://github.com/lucybot/api-spec-converter'
        version: 2.7.31
      format: google
      url: 'https://speech.googleapis.com/$discovery/rest?version=v1p1beta1'
      version: v1
  x-preferred: false
  x-providerName: googleapis.com
  x-serviceName: speech
externalDocs:
  url: 'https://cloud.google.com/speech-to-text/docs/quickstart-protocol'
securityDefinitions:
  Oauth2:
    authorizationUrl: 'https://accounts.google.com/o/oauth2/auth'
    description: Oauth 2.0 authentication
    flow: implicit
    scopes:
      'https://www.googleapis.com/auth/cloud-platform': View and manage your data across Google Cloud Platform services
    type: oauth2
parameters:
  $.xgafv:
    description: V1 error format.
    enum:
      - '1'
      - '2'
    in: query
    name: $.xgafv
    type: string
  access_token:
    description: OAuth access token.
    in: query
    name: access_token
    type: string
  alt:
    default: json
    description: Data format for response.
    enum:
      - json
      - media
      - proto
    in: query
    name: alt
    type: string
  callback:
    description: JSONP
    in: query
    name: callback
    type: string
  fields:
    description: Selector specifying which fields to include in a partial response.
    in: query
    name: fields
    type: string
  key:
    description: 'API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.'
    in: query
    name: key
    type: string
  oauth_token:
    description: OAuth 2.0 token for the current user.
    in: query
    name: oauth_token
    type: string
  prettyPrint:
    default: true
    description: Returns response with indentations and line breaks.
    in: query
    name: prettyPrint
    type: boolean
  quotaUser:
    description: 'Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.'
    in: query
    name: quotaUser
    type: string
  uploadType:
    description: 'Legacy upload protocol for media (e.g. "media", "multipart").'
    in: query
    name: uploadType
    type: string
  upload_protocol:
    description: 'Upload protocol for media (e.g. "raw", "multipart").'
    in: query
    name: upload_protocol
    type: string
tags:
  - name: operations
  - name: projects
  - name: speech
paths:
  /v1p1beta1/operations:
    get:
      description: |-
        Lists operations that match the specified filter in the request. If the
        server doesn't support this method, it returns `UNIMPLEMENTED`.

        NOTE: the `name` binding allows API services to override the binding
        to use different resource name schemes, such as `users/*/operations`. To
        override the binding, API services can add a binding such as
        `"/v1/{name=users/*}/operations"` to their service configuration.
        For backwards compatibility, the default name includes the operations
        collection id, however overriding users must ensure the name binding
        is the parent resource, without the operations collection id.
      operationId: speech.operations.list
      parameters:
        - description: The standard list filter.
          in: query
          name: filter
          type: string
        - description: The name of the operation's parent resource.
          in: query
          name: name
          type: string
        - description: The standard list page size.
          in: query
          name: pageSize
          type: integer
        - description: The standard list page token.
          in: query
          name: pageToken
          type: string
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/ListOperationsResponse'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
      tags:
        - operations
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
  '/v1p1beta1/operations/{name}':
    get:
      description: |-
        Gets the latest state of a long-running operation.  Clients can use this
        method to poll the operation result at intervals as recommended by the API
        service.
      operationId: speech.operations.get
      parameters:
        - description: The name of the operation resource.
          in: path
          name: name
          required: true
          type: string
          x-reservedExpansion: true
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/Operation'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
      tags:
        - operations
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
  '/v1p1beta1/speech:longrunningrecognize':
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
    post:
      description: |-
        Performs asynchronous speech recognition: receive results via the
        google.longrunning.Operations interface. Returns either an
        `Operation.error` or an `Operation.response` which contains
        a `LongRunningRecognizeResponse` message.
      operationId: speech.speech.longrunningrecognize
      parameters:
        - in: body
          name: body
          schema:
            $ref: '#/definitions/LongRunningRecognizeRequest'
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/Operation'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
      tags:
        - speech
  '/v1p1beta1/speech:recognize':
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
    post:
      description: |-
        Performs synchronous speech recognition: receive results after all audio
        has been sent and processed.
      operationId: speech.speech.recognize
      parameters:
        - in: body
          name: body
          schema:
            $ref: '#/definitions/RecognizeRequest'
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/RecognizeResponse'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
      tags:
        - speech
  '/v1p1beta1/{name}':
    get:
      description: |-
        Gets the latest state of a long-running operation.  Clients can use this
        method to poll the operation result at intervals as recommended by the API
        service.
      operationId: speech.projects.locations.operations.get
      parameters:
        - description: The name of the operation resource.
          in: path
          name: name
          required: true
          type: string
          x-reservedExpansion: true
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/Operation'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
      tags:
        - projects
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
  '/v1p1beta1/{name}/operations':
    get:
      description: |-
        Lists operations that match the specified filter in the request. If the
        server doesn't support this method, it returns `UNIMPLEMENTED`.

        NOTE: the `name` binding allows API services to override the binding
        to use different resource name schemes, such as `users/*/operations`. To
        override the binding, API services can add a binding such as
        `"/v1/{name=users/*}/operations"` to their service configuration.
        For backwards compatibility, the default name includes the operations
        collection id, however overriding users must ensure the name binding
        is the parent resource, without the operations collection id.
      operationId: speech.projects.locations.operations.list
      parameters:
        - description: The standard list filter.
          in: query
          name: filter
          type: string
        - description: The name of the operation's parent resource.
          in: path
          name: name
          required: true
          type: string
          x-reservedExpansion: true
        - description: The standard list page size.
          in: query
          name: pageSize
          type: integer
        - description: The standard list page token.
          in: query
          name: pageToken
          type: string
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/ListOperationsResponse'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
      tags:
        - projects
    parameters:
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/oauth_token'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/upload_protocol'
definitions:
  ListOperationsResponse:
    description: The response message for Operations.ListOperations.
    properties:
      nextPageToken:
        description: The standard List next-page token.
        type: string
      operations:
        description: A list of operations that matches the specified filter in the request.
        items:
          $ref: '#/definitions/Operation'
        type: array
    type: object
  LongRunningRecognizeMetadata:
    description: |-
      Describes the progress of a long-running `LongRunningRecognize` call. It is
      included in the `metadata` field of the `Operation` returned by the
      `GetOperation` call of the `google::longrunning::Operations` service.
    properties:
      lastUpdateTime:
        description: Time of the most recent processing update.
        format: google-datetime
        type: string
      progressPercent:
        description: |-
          Approximate percentage of audio processed thus far. Guaranteed to be 100
          when the audio is fully processed and the results are available.
        format: int32
        type: integer
      startTime:
        description: Time when the request was received.
        format: google-datetime
        type: string
    type: object
  LongRunningRecognizeRequest:
    description: |-
      The top-level message sent by the client for the `LongRunningRecognize`
      method.
    properties:
      audio:
        $ref: '#/definitions/RecognitionAudio'
        description: '*Required* The audio data to be recognized.'
      config:
        $ref: '#/definitions/RecognitionConfig'
        description: |-
          *Required* Provides information to the recognizer that specifies how to
          process the request.
    type: object
  LongRunningRecognizeResponse:
    description: |-
      The only message returned to the client by the `LongRunningRecognize` method.
      It contains the result as zero or more sequential `SpeechRecognitionResult`
      messages. It is included in the `result.response` field of the `Operation`
      returned by the `GetOperation` call of the `google::longrunning::Operations`
      service.
    properties:
      results:
        description: |-
          Output only. Sequential list of transcription results corresponding to
          sequential portions of audio.
        items:
          $ref: '#/definitions/SpeechRecognitionResult'
        type: array
    type: object
  Operation:
    description: |-
      This resource represents a long-running operation that is the result of a
      network API call.
    properties:
      done:
        description: |-
          If the value is `false`, it means the operation is still in progress.
          If `true`, the operation is completed, and either `error` or `response` is
          available.
        type: boolean
      error:
        $ref: '#/definitions/Status'
        description: The error result of the operation in case of failure or cancellation.
      metadata:
        additionalProperties:
          description: Properties of the object. Contains field @type with type URL.
        description: |-
          Service-specific metadata associated with the operation.  It typically
          contains progress information and common metadata such as create time.
          Some services might not provide such metadata.  Any method that returns a
          long-running operation should document the metadata type, if any.
        type: object
      name:
        description: |-
          The server-assigned name, which is only unique within the same service that
          originally returns it. If you use the default HTTP mapping, the
          `name` should have the format of `operations/some/unique/name`.
        type: string
      response:
        additionalProperties:
          description: Properties of the object. Contains field @type with type URL.
        description: |-
          The normal response of the operation in case of success.  If the original
          method returns no data on success, such as `Delete`, the response is
          `google.protobuf.Empty`.  If the original method is standard
          `Get`/`Create`/`Update`, the response should be the resource.  For other
          methods, the response should have the type `XxxResponse`, where `Xxx`
          is the original method name.  For example, if the original method name
          is `TakeSnapshot()`, the inferred response type is
          `TakeSnapshotResponse`.
        type: object
    type: object
  RecognitionAudio:
    description: |-
      Contains audio data in the encoding specified in the `RecognitionConfig`.
      Either `content` or `uri` must be supplied. Supplying both or neither
      returns google.rpc.Code.INVALID_ARGUMENT. See
      [content limits](/speech-to-text/quotas#content).
    properties:
      content:
        description: |-
          The audio data bytes encoded as specified in
          `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
          pure binary representation, whereas JSON representations use base64.
        format: byte
        type: string
      uri:
        description: |-
          URI that points to a file that contains audio data bytes as specified in
          `RecognitionConfig`. The file must not be compressed (for example, gzip).
          Currently, only Google Cloud Storage URIs are
          supported, which must be specified in the following format:
          `gs://bucket_name/object_name` (other URI formats return
          google.rpc.Code.INVALID_ARGUMENT). For more information, see
          [Request URIs](https://cloud.google.com/storage/docs/reference-uris).
        type: string
    type: object
  RecognitionConfig:
    description: |-
      Provides information to the recognizer that specifies how to process the
      request.
    properties:
      alternativeLanguageCodes:
        description: |-
          *Optional* A list of up to 3 additional
          [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tags,
          listing possible alternative languages of the supplied audio.
          See [Language Support](/speech-to-text/docs/languages)
          for a list of the currently supported language codes.
          If alternative languages are listed, recognition result will contain
          recognition in the most likely language detected including the main
          language_code. The recognition result will include the language tag
          of the language detected in the audio.
          Note: This feature is only supported for Voice Command and Voice Search
          use cases and performance may vary for other use cases (e.g., phone call
          transcription).
        items:
          type: string
        type: array
      audioChannelCount:
        description: |-
          *Optional* The number of channels in the input audio data.
          ONLY set this for MULTI-CHANNEL recognition.
          Valid values for LINEAR16 and FLAC are `1`-`8`.
          Valid values for OGG_OPUS are '1'-'254'.
          Valid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is only `1`.
          If `0` or omitted, defaults to one channel (mono).
          Note: We only recognize the first channel by default.
          To perform independent recognition on each channel set
          `enable_separate_recognition_per_channel` to 'true'.
        format: int32
        type: integer
      diarizationConfig:
        $ref: '#/definitions/SpeakerDiarizationConfig'
        description: |-
          *Optional* Config to enable speaker diarization and set additional
          parameters to make diarization better suited for your application.
          Note: When this is enabled, we send all the words from the beginning of the
          audio for the top alternative in every consecutive STREAMING responses.
          This is done in order to improve our speaker tags as our models learn to
          identify the speakers in the conversation over time.
          For non-streaming requests, the diarization results will be provided only
          in the top alternative of the FINAL SpeechRecognitionResult.
      diarizationSpeakerCount:
        description: |-
          *Optional*
          If set, specifies the estimated number of speakers in the conversation.
          If not set, defaults to '2'.
          Ignored unless enable_speaker_diarization is set to true."
          Note: Use diarization_config instead. This field will be DEPRECATED soon.
        format: int32
        type: integer
      enableAutomaticPunctuation:
        description: |-
          *Optional* If 'true', adds punctuation to recognition result hypotheses.
          This feature is only available in select languages. Setting this for
          requests in other languages has no effect at all.
          The default 'false' value does not add punctuation to result hypotheses.
          Note: This is currently offered as an experimental service, complimentary
          to all users. In the future this may be exclusively available as a
          premium feature.
        type: boolean
      enableSeparateRecognitionPerChannel:
        description: |-
          This needs to be set to `true` explicitly and `audio_channel_count` > 1
          to get each channel recognized separately. The recognition result will
          contain a `channel_tag` field to state which channel that result belongs
          to. If this is not true, we will only recognize the first channel. The
          request is billed cumulatively for all channels recognized:
          `audio_channel_count` multiplied by the length of the audio.
        type: boolean
      enableSpeakerDiarization:
        description: |-
          *Optional* If 'true', enables speaker detection for each recognized word in
          the top alternative of the recognition result using a speaker_tag provided
          in the WordInfo.
          Note: Use diarization_config instead. This field will be DEPRECATED soon.
        type: boolean
      enableWordConfidence:
        description: |-
          *Optional* If `true`, the top result includes a list of words and the
          confidence for those words. If `false`, no word-level confidence
          information is returned. The default is `false`.
        type: boolean
      enableWordTimeOffsets:
        description: |-
          *Optional* If `true`, the top result includes a list of words and
          the start and end time offsets (timestamps) for those words. If
          `false`, no word-level time offset information is returned. The default is
          `false`.
        type: boolean
      encoding:
        description: |-
          Encoding of audio data sent in all `RecognitionAudio` messages.
          This field is optional for `FLAC` and `WAV` audio files and required
          for all other audio formats. For details, see AudioEncoding.
        enum:
          - ENCODING_UNSPECIFIED
          - LINEAR16
          - FLAC
          - MULAW
          - AMR
          - AMR_WB
          - OGG_OPUS
          - SPEEX_WITH_HEADER_BYTE
        type: string
      languageCode:
        description: |-
          *Required* The language of the supplied audio as a
          [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
          Example: "en-US".
          See [Language Support](/speech-to-text/docs/languages)
          for a list of the currently supported language codes.
        type: string
      maxAlternatives:
        description: |-
          *Optional* Maximum number of recognition hypotheses to be returned.
          Specifically, the maximum number of `SpeechRecognitionAlternative` messages
          within each `SpeechRecognitionResult`.
          The server may return fewer than `max_alternatives`.
          Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
          one. If omitted, will return a maximum of one.
        format: int32
        type: integer
      metadata:
        $ref: '#/definitions/RecognitionMetadata'
        description: '*Optional* Metadata regarding this request.'
      model:
        description: |-
          *Optional* Which model to select for the given request. Select the model
          best suited to your domain to get best results. If a model is not
          explicitly specified, then we auto-select a model based on the parameters
          in the RecognitionConfig.
          <table>
            <tr>
              <td><b>Model</b></td>
              <td><b>Description</b></td>
            </tr>
            <tr>
              <td><code>command_and_search</code></td>
              <td>Best for short queries such as voice commands or voice search.</td>
            </tr>
            <tr>
              <td><code>phone_call</code></td>
              <td>Best for audio that originated from a phone call (typically
              recorded at an 8khz sampling rate).</td>
            </tr>
            <tr>
              <td><code>video</code></td>
              <td>Best for audio that originated from from video or includes multiple
                  speakers. Ideally the audio is recorded at a 16khz or greater
                  sampling rate. This is a premium model that costs more than the
                  standard rate.</td>
            </tr>
            <tr>
              <td><code>default</code></td>
              <td>Best for audio that is not one of the specific audio models.
                  For example, long-form audio. Ideally the audio is high-fidelity,
                  recorded at a 16khz or greater sampling rate.</td>
            </tr>
          </table>
        type: string
      profanityFilter:
        description: |-
          *Optional* If set to `true`, the server will attempt to filter out
          profanities, replacing all but the initial character in each filtered word
          with asterisks, e.g. "f***". If set to `false` or omitted, profanities
          won't be filtered out.
        type: boolean
      sampleRateHertz:
        description: |-
          Sample rate in Hertz of the audio data sent in all
          `RecognitionAudio` messages. Valid values are: 8000-48000.
          16000 is optimal. For best results, set the sampling rate of the audio
          source to 16000 Hz. If that's not possible, use the native sample rate of
          the audio source (instead of re-sampling).
          This field is optional for `FLAC`,  `WAV`. and 'MP3' audio files, and is
          required for all other audio formats. For details, see AudioEncoding.
        format: int32
        type: integer
      speechContexts:
        description: |-
          *Optional* array of SpeechContext.
          A means to provide context to assist the speech recognition. For more
          information, see [Phrase Hints](/speech-to-text/docs/basics#phrase-hints).
        items:
          $ref: '#/definitions/SpeechContext'
        type: array
      useEnhanced:
        description: |-
          *Optional* Set to true to use an enhanced model for speech recognition.
          If `use_enhanced` is set to true and the `model` field is not set, then
          an appropriate enhanced model is chosen if:
          1. project is eligible for requesting enhanced models
          2. an enhanced model exists for the audio

          If `use_enhanced` is true and an enhanced version of the specified model
          does not exist, then the speech is recognized using the standard version
          of the specified model.

          Enhanced speech models require that you opt-in to data logging using
          instructions in the
          [documentation](/speech-to-text/docs/enable-data-logging). If you set
          `use_enhanced` to true and you have not enabled audio logging, then you
          will receive an error.
        type: boolean
    type: object
  RecognitionMetadata:
    description: Description of audio data to be recognized.
    properties:
      audioTopic:
        description: |-
          Description of the content. Eg. "Recordings of federal supreme court
          hearings from 2012".
        type: string
      industryNaicsCodeOfAudio:
        description: |-
          The industry vertical to which this speech recognition request most
          closely applies. This is most indicative of the topics contained
          in the audio.  Use the 6-digit NAICS code to identify the industry
          vertical - see https://www.naics.com/search/.
        format: uint32
        type: integer
      interactionType:
        description: The use case most closely describing the audio content to be recognized.
        enum:
          - INTERACTION_TYPE_UNSPECIFIED
          - DISCUSSION
          - PRESENTATION
          - PHONE_CALL
          - VOICEMAIL
          - PROFESSIONALLY_PRODUCED
          - VOICE_SEARCH
          - VOICE_COMMAND
          - DICTATION
        type: string
      microphoneDistance:
        description: The audio type that most closely describes the audio being recognized.
        enum:
          - MICROPHONE_DISTANCE_UNSPECIFIED
          - NEARFIELD
          - MIDFIELD
          - FARFIELD
        type: string
      obfuscatedId:
        description: |-
          Obfuscated (privacy-protected) ID of the user, to identify number of
          unique users using the service.
        format: int64
        type: string
      originalMediaType:
        description: The original media the speech was recorded on.
        enum:
          - ORIGINAL_MEDIA_TYPE_UNSPECIFIED
          - AUDIO
          - VIDEO
        type: string
      originalMimeType:
        description: |-
          Mime type of the original audio file.  For example `audio/m4a`,
          `audio/x-alaw-basic`, `audio/mp3`, `audio/3gpp`.
          A list of possible audio mime types is maintained at
          http://www.iana.org/assignments/media-types/media-types.xhtml#audio
        type: string
      recordingDeviceName:
        description: |-
          The device used to make the recording.  Examples 'Nexus 5X' or
          'Polycom SoundStation IP 6000' or 'POTS' or 'VoIP' or
          'Cardioid Microphone'.
        type: string
      recordingDeviceType:
        description: The type of device the speech was recorded with.
        enum:
          - RECORDING_DEVICE_TYPE_UNSPECIFIED
          - SMARTPHONE
          - PC
          - PHONE_LINE
          - VEHICLE
          - OTHER_OUTDOOR_DEVICE
          - OTHER_INDOOR_DEVICE
        type: string
    type: object
  RecognizeRequest:
    description: The top-level message sent by the client for the `Recognize` method.
    properties:
      audio:
        $ref: '#/definitions/RecognitionAudio'
        description: '*Required* The audio data to be recognized.'
      config:
        $ref: '#/definitions/RecognitionConfig'
        description: |-
          *Required* Provides information to the recognizer that specifies how to
          process the request.
      name:
        description: '*Optional* The name of the model to use for recognition.'
        type: string
    type: object
  RecognizeResponse:
    description: |-
      The only message returned to the client by the `Recognize` method. It
      contains the result as zero or more sequential `SpeechRecognitionResult`
      messages.
    properties:
      results:
        description: |-
          Output only. Sequential list of transcription results corresponding to
          sequential portions of audio.
        items:
          $ref: '#/definitions/SpeechRecognitionResult'
        type: array
    type: object
  SpeakerDiarizationConfig:
    properties:
      enableSpeakerDiarization:
        description: |-
          *Optional* If 'true', enables speaker detection for each recognized word in
          the top alternative of the recognition result using a speaker_tag provided
          in the WordInfo.
        type: boolean
      maxSpeakerCount:
        description: |-
          *Optional* Only used if diarization_speaker_count is not set.
          Maximum number of speakers in the conversation. This range gives you more
          flexibility by allowing the system to automatically determine the correct
          number of speakers. If not set, the default value is 6.
        format: int32
        type: integer
      minSpeakerCount:
        description: |-
          *Optional* Only used if diarization_speaker_count is not set.
          Minimum number of speakers in the conversation. This range gives you more
          flexibility by allowing the system to automatically determine the correct
          number of speakers. If not set, the default value is 2.
        format: int32
        type: integer
    type: object
  SpeechContext:
    description: |-
      Provides "hints" to the speech recognizer to favor specific words and phrases
      in the results.
    properties:
      phrases:
        description: |-
          *Optional* A list of strings containing words and phrases "hints" so that
          the speech recognition is more likely to recognize them. This can be used
          to improve the accuracy for specific words and phrases, for example, if
          specific commands are typically spoken by the user. This can also be used
          to add additional words to the vocabulary of the recognizer. See
          [usage limits](/speech-to-text/quotas#content).
        items:
          type: string
        type: array
    type: object
  SpeechRecognitionAlternative:
    description: Alternative hypotheses (a.k.a. n-best list).
    properties:
      confidence:
        description: |-
          Output only. The confidence estimate between 0.0 and 1.0. A higher number
          indicates an estimated greater likelihood that the recognized words are
          correct. This field is set only for the top alternative of a non-streaming
          result or, of a streaming result where `is_final=true`.
          This field is not guaranteed to be accurate and users should not rely on it
          to be always provided.
          The default of 0.0 is a sentinel value indicating `confidence` was not set.
        format: float
        type: number
      transcript:
        description: Output only. Transcript text representing the words that the user spoke.
        type: string
      words:
        description: |-
          Output only. A list of word-specific information for each recognized word.
          Note: When `enable_speaker_diarization` is true, you will see all the words
          from the beginning of the audio.
        items:
          $ref: '#/definitions/WordInfo'
        type: array
    type: object
  SpeechRecognitionResult:
    description: A speech recognition result corresponding to a portion of the audio.
    properties:
      alternatives:
        description: |-
          Output only. May contain one or more recognition hypotheses (up to the
          maximum specified in `max_alternatives`).
          These alternatives are ordered in terms of accuracy, with the top (first)
          alternative being the most probable, as ranked by the recognizer.
        items:
          $ref: '#/definitions/SpeechRecognitionAlternative'
        type: array
      channelTag:
        description: |-
          For multi-channel audio, this is the channel number corresponding to the
          recognized result for the audio from that channel.
          For audio_channel_count = N, its output values can range from '1' to 'N'.
        format: int32
        type: integer
      languageCode:
        description: |-
          Output only. The
          [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the
          language in this result. This language code was detected to have the most
          likelihood of being spoken in the audio.
        type: string
    type: object
  Status:
    description: |-
      The `Status` type defines a logical error model that is suitable for
      different programming environments, including REST APIs and RPC APIs. It is
      used by [gRPC](https://github.com/grpc). The error model is designed to be:

      - Simple to use and understand for most users
      - Flexible enough to meet unexpected needs

      # Overview

      The `Status` message contains three pieces of data: error code, error
      message, and error details. The error code should be an enum value of
      google.rpc.Code, but it may accept additional error codes if needed.  The
      error message should be a developer-facing English message that helps
      developers *understand* and *resolve* the error. If a localized user-facing
      error message is needed, put the localized message in the error details or
      localize it in the client. The optional error details may contain arbitrary
      information about the error. There is a predefined set of error detail types
      in the package `google.rpc` that can be used for common error conditions.

      # Language mapping

      The `Status` message is the logical representation of the error model, but it
      is not necessarily the actual wire format. When the `Status` message is
      exposed in different client libraries and different wire protocols, it can be
      mapped differently. For example, it will likely be mapped to some exceptions
      in Java, but more likely mapped to some error codes in C.

      # Other uses

      The error model and the `Status` message can be used in a variety of
      environments, either with or without APIs, to provide a
      consistent developer experience across different environments.

      Example uses of this error model include:

      - Partial errors. If a service needs to return partial errors to the client,
          it may embed the `Status` in the normal response to indicate the partial
          errors.

      - Workflow errors. A typical workflow has multiple steps. Each step may
          have a `Status` message for error reporting.

      - Batch operations. If a client uses batch request and batch response, the
          `Status` message should be used directly inside batch response, one for
          each error sub-response.

      - Asynchronous operations. If an API call embeds asynchronous operation
          results in its response, the status of those operations should be
          represented directly using the `Status` message.

      - Logging. If some API errors are stored in logs, the message `Status` could
          be used directly after any stripping needed for security/privacy reasons.
    properties:
      code:
        description: 'The status code, which should be an enum value of google.rpc.Code.'
        format: int32
        type: integer
      details:
        description: |-
          A list of messages that carry the error details.  There is a common set of
          message types for APIs to use.
        items:
          additionalProperties:
            description: Properties of the object. Contains field @type with type URL.
          type: object
        type: array
      message:
        description: |-
          A developer-facing error message, which should be in English. Any
          user-facing error message should be localized and sent in the
          google.rpc.Status.details field, or localized by the client.
        type: string
    type: object
  WordInfo:
    description: Word-specific information for recognized words.
    properties:
      confidence:
        description: |-
          Output only. The confidence estimate between 0.0 and 1.0. A higher number
          indicates an estimated greater likelihood that the recognized words are
          correct. This field is set only for the top alternative of a non-streaming
          result or, of a streaming result where `is_final=true`.
          This field is not guaranteed to be accurate and users should not rely on it
          to be always provided.
          The default of 0.0 is a sentinel value indicating `confidence` was not set.
        format: float
        type: number
      endTime:
        description: |-
          Output only. Time offset relative to the beginning of the audio,
          and corresponding to the end of the spoken word.
          This field is only set if `enable_word_time_offsets=true` and only
          in the top hypothesis.
          This is an experimental feature and the accuracy of the time offset can
          vary.
        format: google-duration
        type: string
      speakerTag:
        description: |-
          Output only. A distinct integer value is assigned for every speaker within
          the audio. This field specifies which one of those speakers was detected to
          have spoken this word. Value ranges from '1' to diarization_speaker_count.
          speaker_tag is set if enable_speaker_diarization = 'true' and only in the
          top alternative.
        format: int32
        type: integer
      startTime:
        description: |-
          Output only. Time offset relative to the beginning of the audio,
          and corresponding to the start of the spoken word.
          This field is only set if `enable_word_time_offsets=true` and only
          in the top hypothesis.
          This is an experimental feature and the accuracy of the time offset can
          vary.
        format: google-duration
        type: string
      word:
        description: Output only. The word corresponding to this set of information.
        type: string
    type: object
