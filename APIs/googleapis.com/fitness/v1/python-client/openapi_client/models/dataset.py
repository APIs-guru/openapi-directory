# coding: utf-8

"""
    Fitness API

    The Fitness API for managing users' fitness tracking data.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.data_point import DataPoint
from typing import Optional, Set
from typing_extensions import Self

class Dataset(BaseModel):
    """
    A dataset represents a projection container for data points. They do not carry any info of their own. Datasets represent a set of data points from a particular data source. A data point can be found in more than one dataset.
    """ # noqa: E501
    data_source_id: Optional[StrictStr] = Field(default=None, description="The data stream ID of the data source that created the points in this dataset.", alias="dataSourceId")
    max_end_time_ns: Optional[StrictStr] = Field(default=None, description="The largest end time of all data points in this possibly partial representation of the dataset. Time is in nanoseconds from epoch. This should also match the second part of the dataset identifier.", alias="maxEndTimeNs")
    min_start_time_ns: Optional[StrictStr] = Field(default=None, description="The smallest start time of all data points in this possibly partial representation of the dataset. Time is in nanoseconds from epoch. This should also match the first part of the dataset identifier.", alias="minStartTimeNs")
    next_page_token: Optional[StrictStr] = Field(default=None, description="This token will be set when a dataset is received in response to a GET request and the dataset is too large to be included in a single response. Provide this value in a subsequent GET request to return the next page of data points within this dataset.", alias="nextPageToken")
    point: Optional[List[DataPoint]] = Field(default=None, description="A partial list of data points contained in the dataset, ordered by endTimeNanos. This list is considered complete when retrieving a small dataset and partial when patching a dataset or retrieving a dataset that is too large to include in a single response.")
    __properties: ClassVar[List[str]] = ["dataSourceId", "maxEndTimeNs", "minStartTimeNs", "nextPageToken", "point"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of Dataset from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in point (list)
        _items = []
        if self.point:
            for _item_point in self.point:
                if _item_point:
                    _items.append(_item_point.to_dict())
            _dict['point'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of Dataset from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "dataSourceId": obj.get("dataSourceId"),
            "maxEndTimeNs": obj.get("maxEndTimeNs"),
            "minStartTimeNs": obj.get("minStartTimeNs"),
            "nextPageToken": obj.get("nextPageToken"),
            "point": [DataPoint.from_dict(_item) for _item in obj["point"]] if obj.get("point") is not None else None
        })
        return _obj


