# coding: utf-8

"""
    Cloud Video Intelligence API

    Detects objects, explicit content, and scene changes in videos. It also specifies the region for annotation and transcribes speech to text. Supports both asynchronous API and streaming API.

    The version of the OpenAPI document: v1beta2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_cloud_videointelligence_v1p1beta1_explicit_content_annotation import GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation
from openapi_client.models.google_cloud_videointelligence_v1p1beta1_face_annotation import GoogleCloudVideointelligenceV1p1beta1FaceAnnotation
from openapi_client.models.google_cloud_videointelligence_v1p1beta1_face_detection_annotation import GoogleCloudVideointelligenceV1p1beta1FaceDetectionAnnotation
from openapi_client.models.google_cloud_videointelligence_v1p1beta1_label_annotation import GoogleCloudVideointelligenceV1p1beta1LabelAnnotation
from openapi_client.models.google_cloud_videointelligence_v1p1beta1_logo_recognition_annotation import GoogleCloudVideointelligenceV1p1beta1LogoRecognitionAnnotation
from openapi_client.models.google_cloud_videointelligence_v1p1beta1_object_tracking_annotation import GoogleCloudVideointelligenceV1p1beta1ObjectTrackingAnnotation
from openapi_client.models.google_cloud_videointelligence_v1p1beta1_person_detection_annotation import GoogleCloudVideointelligenceV1p1beta1PersonDetectionAnnotation
from openapi_client.models.google_cloud_videointelligence_v1p1beta1_speech_transcription import GoogleCloudVideointelligenceV1p1beta1SpeechTranscription
from openapi_client.models.google_cloud_videointelligence_v1p1beta1_text_annotation import GoogleCloudVideointelligenceV1p1beta1TextAnnotation
from openapi_client.models.google_cloud_videointelligence_v1p1beta1_video_segment import GoogleCloudVideointelligenceV1p1beta1VideoSegment
from openapi_client.models.google_rpc_status import GoogleRpcStatus
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults(BaseModel):
    """
    Annotation results for a single video.
    """ # noqa: E501
    error: Optional[GoogleRpcStatus] = None
    explicit_annotation: Optional[GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation] = Field(default=None, alias="explicitAnnotation")
    face_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1FaceAnnotation]] = Field(default=None, description="Deprecated. Please use `face_detection_annotations` instead.", alias="faceAnnotations")
    face_detection_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1FaceDetectionAnnotation]] = Field(default=None, description="Face detection annotations.", alias="faceDetectionAnnotations")
    frame_label_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1LabelAnnotation]] = Field(default=None, description="Label annotations on frame level. There is exactly one element for each unique label.", alias="frameLabelAnnotations")
    input_uri: Optional[StrictStr] = Field(default=None, description="Video file location in [Cloud Storage](https://cloud.google.com/storage/).", alias="inputUri")
    logo_recognition_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1LogoRecognitionAnnotation]] = Field(default=None, description="Annotations for list of logos detected, tracked and recognized in video.", alias="logoRecognitionAnnotations")
    object_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1ObjectTrackingAnnotation]] = Field(default=None, description="Annotations for list of objects detected and tracked in video.", alias="objectAnnotations")
    person_detection_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1PersonDetectionAnnotation]] = Field(default=None, description="Person detection annotations.", alias="personDetectionAnnotations")
    segment: Optional[GoogleCloudVideointelligenceV1p1beta1VideoSegment] = None
    segment_label_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1LabelAnnotation]] = Field(default=None, description="Topical label annotations on video level or user-specified segment level. There is exactly one element for each unique label.", alias="segmentLabelAnnotations")
    segment_presence_label_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1LabelAnnotation]] = Field(default=None, description="Presence label annotations on video level or user-specified segment level. There is exactly one element for each unique label. Compared to the existing topical `segment_label_annotations`, this field presents more fine-grained, segment-level labels detected in video content and is made available only when the client sets `LabelDetectionConfig.model` to \"builtin/latest\" in the request.", alias="segmentPresenceLabelAnnotations")
    shot_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1VideoSegment]] = Field(default=None, description="Shot annotations. Each shot is represented as a video segment.", alias="shotAnnotations")
    shot_label_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1LabelAnnotation]] = Field(default=None, description="Topical label annotations on shot level. There is exactly one element for each unique label.", alias="shotLabelAnnotations")
    shot_presence_label_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1LabelAnnotation]] = Field(default=None, description="Presence label annotations on shot level. There is exactly one element for each unique label. Compared to the existing topical `shot_label_annotations`, this field presents more fine-grained, shot-level labels detected in video content and is made available only when the client sets `LabelDetectionConfig.model` to \"builtin/latest\" in the request.", alias="shotPresenceLabelAnnotations")
    speech_transcriptions: Optional[List[GoogleCloudVideointelligenceV1p1beta1SpeechTranscription]] = Field(default=None, description="Speech transcription.", alias="speechTranscriptions")
    text_annotations: Optional[List[GoogleCloudVideointelligenceV1p1beta1TextAnnotation]] = Field(default=None, description="OCR text detection and tracking. Annotations for list of detected text snippets. Each will have list of frame information associated with it.", alias="textAnnotations")
    __properties: ClassVar[List[str]] = ["error", "explicitAnnotation", "faceAnnotations", "faceDetectionAnnotations", "frameLabelAnnotations", "inputUri", "logoRecognitionAnnotations", "objectAnnotations", "personDetectionAnnotations", "segment", "segmentLabelAnnotations", "segmentPresenceLabelAnnotations", "shotAnnotations", "shotLabelAnnotations", "shotPresenceLabelAnnotations", "speechTranscriptions", "textAnnotations"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of error
        if self.error:
            _dict['error'] = self.error.to_dict()
        # override the default output from pydantic by calling `to_dict()` of explicit_annotation
        if self.explicit_annotation:
            _dict['explicitAnnotation'] = self.explicit_annotation.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in face_annotations (list)
        _items = []
        if self.face_annotations:
            for _item_face_annotations in self.face_annotations:
                if _item_face_annotations:
                    _items.append(_item_face_annotations.to_dict())
            _dict['faceAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in face_detection_annotations (list)
        _items = []
        if self.face_detection_annotations:
            for _item_face_detection_annotations in self.face_detection_annotations:
                if _item_face_detection_annotations:
                    _items.append(_item_face_detection_annotations.to_dict())
            _dict['faceDetectionAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in frame_label_annotations (list)
        _items = []
        if self.frame_label_annotations:
            for _item_frame_label_annotations in self.frame_label_annotations:
                if _item_frame_label_annotations:
                    _items.append(_item_frame_label_annotations.to_dict())
            _dict['frameLabelAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in logo_recognition_annotations (list)
        _items = []
        if self.logo_recognition_annotations:
            for _item_logo_recognition_annotations in self.logo_recognition_annotations:
                if _item_logo_recognition_annotations:
                    _items.append(_item_logo_recognition_annotations.to_dict())
            _dict['logoRecognitionAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in object_annotations (list)
        _items = []
        if self.object_annotations:
            for _item_object_annotations in self.object_annotations:
                if _item_object_annotations:
                    _items.append(_item_object_annotations.to_dict())
            _dict['objectAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in person_detection_annotations (list)
        _items = []
        if self.person_detection_annotations:
            for _item_person_detection_annotations in self.person_detection_annotations:
                if _item_person_detection_annotations:
                    _items.append(_item_person_detection_annotations.to_dict())
            _dict['personDetectionAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of segment
        if self.segment:
            _dict['segment'] = self.segment.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in segment_label_annotations (list)
        _items = []
        if self.segment_label_annotations:
            for _item_segment_label_annotations in self.segment_label_annotations:
                if _item_segment_label_annotations:
                    _items.append(_item_segment_label_annotations.to_dict())
            _dict['segmentLabelAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in segment_presence_label_annotations (list)
        _items = []
        if self.segment_presence_label_annotations:
            for _item_segment_presence_label_annotations in self.segment_presence_label_annotations:
                if _item_segment_presence_label_annotations:
                    _items.append(_item_segment_presence_label_annotations.to_dict())
            _dict['segmentPresenceLabelAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in shot_annotations (list)
        _items = []
        if self.shot_annotations:
            for _item_shot_annotations in self.shot_annotations:
                if _item_shot_annotations:
                    _items.append(_item_shot_annotations.to_dict())
            _dict['shotAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in shot_label_annotations (list)
        _items = []
        if self.shot_label_annotations:
            for _item_shot_label_annotations in self.shot_label_annotations:
                if _item_shot_label_annotations:
                    _items.append(_item_shot_label_annotations.to_dict())
            _dict['shotLabelAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in shot_presence_label_annotations (list)
        _items = []
        if self.shot_presence_label_annotations:
            for _item_shot_presence_label_annotations in self.shot_presence_label_annotations:
                if _item_shot_presence_label_annotations:
                    _items.append(_item_shot_presence_label_annotations.to_dict())
            _dict['shotPresenceLabelAnnotations'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in speech_transcriptions (list)
        _items = []
        if self.speech_transcriptions:
            for _item_speech_transcriptions in self.speech_transcriptions:
                if _item_speech_transcriptions:
                    _items.append(_item_speech_transcriptions.to_dict())
            _dict['speechTranscriptions'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in text_annotations (list)
        _items = []
        if self.text_annotations:
            for _item_text_annotations in self.text_annotations:
                if _item_text_annotations:
                    _items.append(_item_text_annotations.to_dict())
            _dict['textAnnotations'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudVideointelligenceV1p1beta1VideoAnnotationResults from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "error": GoogleRpcStatus.from_dict(obj["error"]) if obj.get("error") is not None else None,
            "explicitAnnotation": GoogleCloudVideointelligenceV1p1beta1ExplicitContentAnnotation.from_dict(obj["explicitAnnotation"]) if obj.get("explicitAnnotation") is not None else None,
            "faceAnnotations": [GoogleCloudVideointelligenceV1p1beta1FaceAnnotation.from_dict(_item) for _item in obj["faceAnnotations"]] if obj.get("faceAnnotations") is not None else None,
            "faceDetectionAnnotations": [GoogleCloudVideointelligenceV1p1beta1FaceDetectionAnnotation.from_dict(_item) for _item in obj["faceDetectionAnnotations"]] if obj.get("faceDetectionAnnotations") is not None else None,
            "frameLabelAnnotations": [GoogleCloudVideointelligenceV1p1beta1LabelAnnotation.from_dict(_item) for _item in obj["frameLabelAnnotations"]] if obj.get("frameLabelAnnotations") is not None else None,
            "inputUri": obj.get("inputUri"),
            "logoRecognitionAnnotations": [GoogleCloudVideointelligenceV1p1beta1LogoRecognitionAnnotation.from_dict(_item) for _item in obj["logoRecognitionAnnotations"]] if obj.get("logoRecognitionAnnotations") is not None else None,
            "objectAnnotations": [GoogleCloudVideointelligenceV1p1beta1ObjectTrackingAnnotation.from_dict(_item) for _item in obj["objectAnnotations"]] if obj.get("objectAnnotations") is not None else None,
            "personDetectionAnnotations": [GoogleCloudVideointelligenceV1p1beta1PersonDetectionAnnotation.from_dict(_item) for _item in obj["personDetectionAnnotations"]] if obj.get("personDetectionAnnotations") is not None else None,
            "segment": GoogleCloudVideointelligenceV1p1beta1VideoSegment.from_dict(obj["segment"]) if obj.get("segment") is not None else None,
            "segmentLabelAnnotations": [GoogleCloudVideointelligenceV1p1beta1LabelAnnotation.from_dict(_item) for _item in obj["segmentLabelAnnotations"]] if obj.get("segmentLabelAnnotations") is not None else None,
            "segmentPresenceLabelAnnotations": [GoogleCloudVideointelligenceV1p1beta1LabelAnnotation.from_dict(_item) for _item in obj["segmentPresenceLabelAnnotations"]] if obj.get("segmentPresenceLabelAnnotations") is not None else None,
            "shotAnnotations": [GoogleCloudVideointelligenceV1p1beta1VideoSegment.from_dict(_item) for _item in obj["shotAnnotations"]] if obj.get("shotAnnotations") is not None else None,
            "shotLabelAnnotations": [GoogleCloudVideointelligenceV1p1beta1LabelAnnotation.from_dict(_item) for _item in obj["shotLabelAnnotations"]] if obj.get("shotLabelAnnotations") is not None else None,
            "shotPresenceLabelAnnotations": [GoogleCloudVideointelligenceV1p1beta1LabelAnnotation.from_dict(_item) for _item in obj["shotPresenceLabelAnnotations"]] if obj.get("shotPresenceLabelAnnotations") is not None else None,
            "speechTranscriptions": [GoogleCloudVideointelligenceV1p1beta1SpeechTranscription.from_dict(_item) for _item in obj["speechTranscriptions"]] if obj.get("speechTranscriptions") is not None else None,
            "textAnnotations": [GoogleCloudVideointelligenceV1p1beta1TextAnnotation.from_dict(_item) for _item in obj["textAnnotations"]] if obj.get("textAnnotations") is not None else None
        })
        return _obj


