# coding: utf-8

"""
    Cloud Video Intelligence API

    Detects objects, explicit content, and scene changes in videos. It also specifies the region for annotation and transcribes speech to text. Supports both asynchronous API and streaming API.

    The version of the OpenAPI document: v1p3beta1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_cloud_videointelligence_v1p3beta1_explicit_content_detection_config import GoogleCloudVideointelligenceV1p3beta1ExplicitContentDetectionConfig
from openapi_client.models.google_cloud_videointelligence_v1p3beta1_face_detection_config import GoogleCloudVideointelligenceV1p3beta1FaceDetectionConfig
from openapi_client.models.google_cloud_videointelligence_v1p3beta1_label_detection_config import GoogleCloudVideointelligenceV1p3beta1LabelDetectionConfig
from openapi_client.models.google_cloud_videointelligence_v1p3beta1_object_tracking_config import GoogleCloudVideointelligenceV1p3beta1ObjectTrackingConfig
from openapi_client.models.google_cloud_videointelligence_v1p3beta1_person_detection_config import GoogleCloudVideointelligenceV1p3beta1PersonDetectionConfig
from openapi_client.models.google_cloud_videointelligence_v1p3beta1_shot_change_detection_config import GoogleCloudVideointelligenceV1p3beta1ShotChangeDetectionConfig
from openapi_client.models.google_cloud_videointelligence_v1p3beta1_speech_transcription_config import GoogleCloudVideointelligenceV1p3beta1SpeechTranscriptionConfig
from openapi_client.models.google_cloud_videointelligence_v1p3beta1_text_detection_config import GoogleCloudVideointelligenceV1p3beta1TextDetectionConfig
from openapi_client.models.google_cloud_videointelligence_v1p3beta1_video_segment import GoogleCloudVideointelligenceV1p3beta1VideoSegment
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudVideointelligenceV1p3beta1VideoContext(BaseModel):
    """
    Video context and/or feature-specific parameters.
    """ # noqa: E501
    explicit_content_detection_config: Optional[GoogleCloudVideointelligenceV1p3beta1ExplicitContentDetectionConfig] = Field(default=None, alias="explicitContentDetectionConfig")
    face_detection_config: Optional[GoogleCloudVideointelligenceV1p3beta1FaceDetectionConfig] = Field(default=None, alias="faceDetectionConfig")
    label_detection_config: Optional[GoogleCloudVideointelligenceV1p3beta1LabelDetectionConfig] = Field(default=None, alias="labelDetectionConfig")
    object_tracking_config: Optional[GoogleCloudVideointelligenceV1p3beta1ObjectTrackingConfig] = Field(default=None, alias="objectTrackingConfig")
    person_detection_config: Optional[GoogleCloudVideointelligenceV1p3beta1PersonDetectionConfig] = Field(default=None, alias="personDetectionConfig")
    segments: Optional[List[GoogleCloudVideointelligenceV1p3beta1VideoSegment]] = Field(default=None, description="Video segments to annotate. The segments may overlap and are not required to be contiguous or span the whole video. If unspecified, each video is treated as a single segment.")
    shot_change_detection_config: Optional[GoogleCloudVideointelligenceV1p3beta1ShotChangeDetectionConfig] = Field(default=None, alias="shotChangeDetectionConfig")
    speech_transcription_config: Optional[GoogleCloudVideointelligenceV1p3beta1SpeechTranscriptionConfig] = Field(default=None, alias="speechTranscriptionConfig")
    text_detection_config: Optional[GoogleCloudVideointelligenceV1p3beta1TextDetectionConfig] = Field(default=None, alias="textDetectionConfig")
    __properties: ClassVar[List[str]] = ["explicitContentDetectionConfig", "faceDetectionConfig", "labelDetectionConfig", "objectTrackingConfig", "personDetectionConfig", "segments", "shotChangeDetectionConfig", "speechTranscriptionConfig", "textDetectionConfig"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudVideointelligenceV1p3beta1VideoContext from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of explicit_content_detection_config
        if self.explicit_content_detection_config:
            _dict['explicitContentDetectionConfig'] = self.explicit_content_detection_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of face_detection_config
        if self.face_detection_config:
            _dict['faceDetectionConfig'] = self.face_detection_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of label_detection_config
        if self.label_detection_config:
            _dict['labelDetectionConfig'] = self.label_detection_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of object_tracking_config
        if self.object_tracking_config:
            _dict['objectTrackingConfig'] = self.object_tracking_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of person_detection_config
        if self.person_detection_config:
            _dict['personDetectionConfig'] = self.person_detection_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in segments (list)
        _items = []
        if self.segments:
            for _item_segments in self.segments:
                if _item_segments:
                    _items.append(_item_segments.to_dict())
            _dict['segments'] = _items
        # override the default output from pydantic by calling `to_dict()` of shot_change_detection_config
        if self.shot_change_detection_config:
            _dict['shotChangeDetectionConfig'] = self.shot_change_detection_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of speech_transcription_config
        if self.speech_transcription_config:
            _dict['speechTranscriptionConfig'] = self.speech_transcription_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of text_detection_config
        if self.text_detection_config:
            _dict['textDetectionConfig'] = self.text_detection_config.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudVideointelligenceV1p3beta1VideoContext from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "explicitContentDetectionConfig": GoogleCloudVideointelligenceV1p3beta1ExplicitContentDetectionConfig.from_dict(obj["explicitContentDetectionConfig"]) if obj.get("explicitContentDetectionConfig") is not None else None,
            "faceDetectionConfig": GoogleCloudVideointelligenceV1p3beta1FaceDetectionConfig.from_dict(obj["faceDetectionConfig"]) if obj.get("faceDetectionConfig") is not None else None,
            "labelDetectionConfig": GoogleCloudVideointelligenceV1p3beta1LabelDetectionConfig.from_dict(obj["labelDetectionConfig"]) if obj.get("labelDetectionConfig") is not None else None,
            "objectTrackingConfig": GoogleCloudVideointelligenceV1p3beta1ObjectTrackingConfig.from_dict(obj["objectTrackingConfig"]) if obj.get("objectTrackingConfig") is not None else None,
            "personDetectionConfig": GoogleCloudVideointelligenceV1p3beta1PersonDetectionConfig.from_dict(obj["personDetectionConfig"]) if obj.get("personDetectionConfig") is not None else None,
            "segments": [GoogleCloudVideointelligenceV1p3beta1VideoSegment.from_dict(_item) for _item in obj["segments"]] if obj.get("segments") is not None else None,
            "shotChangeDetectionConfig": GoogleCloudVideointelligenceV1p3beta1ShotChangeDetectionConfig.from_dict(obj["shotChangeDetectionConfig"]) if obj.get("shotChangeDetectionConfig") is not None else None,
            "speechTranscriptionConfig": GoogleCloudVideointelligenceV1p3beta1SpeechTranscriptionConfig.from_dict(obj["speechTranscriptionConfig"]) if obj.get("speechTranscriptionConfig") is not None else None,
            "textDetectionConfig": GoogleCloudVideointelligenceV1p3beta1TextDetectionConfig.from_dict(obj["textDetectionConfig"]) if obj.get("textDetectionConfig") is not None else None
        })
        return _obj


