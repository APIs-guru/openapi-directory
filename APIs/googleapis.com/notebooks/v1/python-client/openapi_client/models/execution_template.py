# coding: utf-8

"""
    Notebooks API

    Notebooks API is used to manage notebook resources in Google Cloud.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.dataproc_parameters import DataprocParameters
from openapi_client.models.scheduler_accelerator_config import SchedulerAcceleratorConfig
from openapi_client.models.vertex_ai_parameters import VertexAIParameters
from typing import Optional, Set
from typing_extensions import Self

class ExecutionTemplate(BaseModel):
    """
    The description a notebook execution workload.
    """ # noqa: E501
    accelerator_config: Optional[SchedulerAcceleratorConfig] = Field(default=None, alias="acceleratorConfig")
    container_image_uri: Optional[StrictStr] = Field(default=None, description="Container Image URI to a DLVM Example: 'gcr.io/deeplearning-platform-release/base-cu100' More examples can be found at: https://cloud.google.com/ai-platform/deep-learning-containers/docs/choosing-container", alias="containerImageUri")
    dataproc_parameters: Optional[DataprocParameters] = Field(default=None, alias="dataprocParameters")
    input_notebook_file: Optional[StrictStr] = Field(default=None, description="Path to the notebook file to execute. Must be in a Google Cloud Storage bucket. Format: `gs://{bucket_name}/{folder}/{notebook_file_name}` Ex: `gs://notebook_user/scheduled_notebooks/sentiment_notebook.ipynb`", alias="inputNotebookFile")
    job_type: Optional[StrictStr] = Field(default=None, description="The type of Job to be used on this execution.", alias="jobType")
    kernel_spec: Optional[StrictStr] = Field(default=None, description="Name of the kernel spec to use. This must be specified if the kernel spec name on the execution target does not match the name in the input notebook file.", alias="kernelSpec")
    labels: Optional[Dict[str, StrictStr]] = Field(default=None, description="Labels for execution. If execution is scheduled, a field included will be 'nbs-scheduled'. Otherwise, it is an immediate execution, and an included field will be 'nbs-immediate'. Use fields to efficiently index between various types of executions.")
    master_type: Optional[StrictStr] = Field(default=None, description="Specifies the type of virtual machine to use for your training job's master worker. You must specify this field when `scaleTier` is set to `CUSTOM`. You can use certain Compute Engine machine types directly in this field. The following types are supported: - `n1-standard-4` - `n1-standard-8` - `n1-standard-16` - `n1-standard-32` - `n1-standard-64` - `n1-standard-96` - `n1-highmem-2` - `n1-highmem-4` - `n1-highmem-8` - `n1-highmem-16` - `n1-highmem-32` - `n1-highmem-64` - `n1-highmem-96` - `n1-highcpu-16` - `n1-highcpu-32` - `n1-highcpu-64` - `n1-highcpu-96` Alternatively, you can use the following legacy machine types: - `standard` - `large_model` - `complex_model_s` - `complex_model_m` - `complex_model_l` - `standard_gpu` - `complex_model_m_gpu` - `complex_model_l_gpu` - `standard_p100` - `complex_model_m_p100` - `standard_v100` - `large_model_v100` - `complex_model_m_v100` - `complex_model_l_v100` Finally, if you want to use a TPU for training, specify `cloud_tpu` in this field. Learn more about the [special configuration options for training with TPU](https://cloud.google.com/ai-platform/training/docs/using-tpus#configuring_a_custom_tpu_machine).", alias="masterType")
    output_notebook_folder: Optional[StrictStr] = Field(default=None, description="Path to the notebook folder to write to. Must be in a Google Cloud Storage bucket path. Format: `gs://{bucket_name}/{folder}` Ex: `gs://notebook_user/scheduled_notebooks`", alias="outputNotebookFolder")
    parameters: Optional[StrictStr] = Field(default=None, description="Parameters used within the 'input_notebook_file' notebook.")
    params_yaml_file: Optional[StrictStr] = Field(default=None, description="Parameters to be overridden in the notebook during execution. Ref https://papermill.readthedocs.io/en/latest/usage-parameterize.html on how to specifying parameters in the input notebook and pass them here in an YAML file. Ex: `gs://notebook_user/scheduled_notebooks/sentiment_notebook_params.yaml`", alias="paramsYamlFile")
    scale_tier: Optional[StrictStr] = Field(default=None, description="Required. Scale tier of the hardware used for notebook execution. DEPRECATED Will be discontinued. As right now only CUSTOM is supported.", alias="scaleTier")
    service_account: Optional[StrictStr] = Field(default=None, description="The email address of a service account to use when running the execution. You must have the `iam.serviceAccounts.actAs` permission for the specified service account.", alias="serviceAccount")
    tensorboard: Optional[StrictStr] = Field(default=None, description="The name of a Vertex AI [Tensorboard] resource to which this execution will upload Tensorboard logs. Format: `projects/{project}/locations/{location}/tensorboards/{tensorboard}`")
    vertex_ai_parameters: Optional[VertexAIParameters] = Field(default=None, alias="vertexAiParameters")
    __properties: ClassVar[List[str]] = ["acceleratorConfig", "containerImageUri", "dataprocParameters", "inputNotebookFile", "jobType", "kernelSpec", "labels", "masterType", "outputNotebookFolder", "parameters", "paramsYamlFile", "scaleTier", "serviceAccount", "tensorboard", "vertexAiParameters"]

    @field_validator('job_type')
    def job_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['JOB_TYPE_UNSPECIFIED', 'VERTEX_AI', 'DATAPROC']):
            raise ValueError("must be one of enum values ('JOB_TYPE_UNSPECIFIED', 'VERTEX_AI', 'DATAPROC')")
        return value

    @field_validator('scale_tier')
    def scale_tier_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['SCALE_TIER_UNSPECIFIED', 'BASIC', 'STANDARD_1', 'PREMIUM_1', 'BASIC_GPU', 'BASIC_TPU', 'CUSTOM']):
            raise ValueError("must be one of enum values ('SCALE_TIER_UNSPECIFIED', 'BASIC', 'STANDARD_1', 'PREMIUM_1', 'BASIC_GPU', 'BASIC_TPU', 'CUSTOM')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of ExecutionTemplate from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of accelerator_config
        if self.accelerator_config:
            _dict['acceleratorConfig'] = self.accelerator_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of dataproc_parameters
        if self.dataproc_parameters:
            _dict['dataprocParameters'] = self.dataproc_parameters.to_dict()
        # override the default output from pydantic by calling `to_dict()` of vertex_ai_parameters
        if self.vertex_ai_parameters:
            _dict['vertexAiParameters'] = self.vertex_ai_parameters.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of ExecutionTemplate from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "acceleratorConfig": SchedulerAcceleratorConfig.from_dict(obj["acceleratorConfig"]) if obj.get("acceleratorConfig") is not None else None,
            "containerImageUri": obj.get("containerImageUri"),
            "dataprocParameters": DataprocParameters.from_dict(obj["dataprocParameters"]) if obj.get("dataprocParameters") is not None else None,
            "inputNotebookFile": obj.get("inputNotebookFile"),
            "jobType": obj.get("jobType"),
            "kernelSpec": obj.get("kernelSpec"),
            "labels": obj.get("labels"),
            "masterType": obj.get("masterType"),
            "outputNotebookFolder": obj.get("outputNotebookFolder"),
            "parameters": obj.get("parameters"),
            "paramsYamlFile": obj.get("paramsYamlFile"),
            "scaleTier": obj.get("scaleTier"),
            "serviceAccount": obj.get("serviceAccount"),
            "tensorboard": obj.get("tensorboard"),
            "vertexAiParameters": VertexAIParameters.from_dict(obj["vertexAiParameters"]) if obj.get("vertexAiParameters") is not None else None
        })
        return _obj


