# coding: utf-8

"""
    Dataflow API

    Manages Google Cloud Dataflow projects on Google Cloud Platform.

    The version of the OpenAPI document: v1b3
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.autoscaling_settings import AutoscalingSettings
from openapi_client.models.disk import Disk
from openapi_client.models.package import Package
from openapi_client.models.sdk_harness_container_image import SdkHarnessContainerImage
from openapi_client.models.task_runner_settings import TaskRunnerSettings
from typing import Optional, Set
from typing_extensions import Self

class WorkerPool(BaseModel):
    """
    Describes one particular pool of Cloud Dataflow workers to be instantiated by the Cloud Dataflow service in order to perform the computations required by a job. Note that a workflow job may use multiple pools, in order to match the various computational requirements of the various stages of the job.
    """ # noqa: E501
    autoscaling_settings: Optional[AutoscalingSettings] = Field(default=None, alias="autoscalingSettings")
    data_disks: Optional[List[Disk]] = Field(default=None, description="Data disks that are used by a VM in this workflow.", alias="dataDisks")
    default_package_set: Optional[StrictStr] = Field(default=None, description="The default package set to install. This allows the service to select a default set of packages which are useful to worker harnesses written in a particular language.", alias="defaultPackageSet")
    disk_size_gb: Optional[StrictInt] = Field(default=None, description="Size of root disk for VMs, in GB. If zero or unspecified, the service will attempt to choose a reasonable default.", alias="diskSizeGb")
    disk_source_image: Optional[StrictStr] = Field(default=None, description="Fully qualified source image for disks.", alias="diskSourceImage")
    disk_type: Optional[StrictStr] = Field(default=None, description="Type of root disk for VMs. If empty or unspecified, the service will attempt to choose a reasonable default.", alias="diskType")
    ip_configuration: Optional[StrictStr] = Field(default=None, description="Configuration for VM IPs.", alias="ipConfiguration")
    kind: Optional[StrictStr] = Field(default=None, description="The kind of the worker pool; currently only `harness` and `shuffle` are supported.")
    machine_type: Optional[StrictStr] = Field(default=None, description="Machine type (e.g. \"n1-standard-1\"). If empty or unspecified, the service will attempt to choose a reasonable default.", alias="machineType")
    metadata: Optional[Dict[str, StrictStr]] = Field(default=None, description="Metadata to set on the Google Compute Engine VMs.")
    network: Optional[StrictStr] = Field(default=None, description="Network to which VMs will be assigned. If empty or unspecified, the service will use the network \"default\".")
    num_threads_per_worker: Optional[StrictInt] = Field(default=None, description="The number of threads per worker harness. If empty or unspecified, the service will choose a number of threads (according to the number of cores on the selected machine type for batch, or 1 by convention for streaming).", alias="numThreadsPerWorker")
    num_workers: Optional[StrictInt] = Field(default=None, description="Number of Google Compute Engine workers in this pool needed to execute the job. If zero or unspecified, the service will attempt to choose a reasonable default.", alias="numWorkers")
    on_host_maintenance: Optional[StrictStr] = Field(default=None, description="The action to take on host maintenance, as defined by the Google Compute Engine API.", alias="onHostMaintenance")
    packages: Optional[List[Package]] = Field(default=None, description="Packages to be installed on workers.")
    pool_args: Optional[Dict[str, Any]] = Field(default=None, description="Extra arguments for this worker pool.", alias="poolArgs")
    sdk_harness_container_images: Optional[List[SdkHarnessContainerImage]] = Field(default=None, description="Set of SDK harness containers needed to execute this pipeline. This will only be set in the Fn API path. For non-cross-language pipelines this should have only one entry. Cross-language pipelines will have two or more entries.", alias="sdkHarnessContainerImages")
    subnetwork: Optional[StrictStr] = Field(default=None, description="Subnetwork to which VMs will be assigned, if desired. Expected to be of the form \"regions/REGION/subnetworks/SUBNETWORK\".")
    taskrunner_settings: Optional[TaskRunnerSettings] = Field(default=None, alias="taskrunnerSettings")
    teardown_policy: Optional[StrictStr] = Field(default=None, description="Sets the policy for determining when to turndown worker pool. Allowed values are: `TEARDOWN_ALWAYS`, `TEARDOWN_ON_SUCCESS`, and `TEARDOWN_NEVER`. `TEARDOWN_ALWAYS` means workers are always torn down regardless of whether the job succeeds. `TEARDOWN_ON_SUCCESS` means workers are torn down if the job succeeds. `TEARDOWN_NEVER` means the workers are never torn down. If the workers are not torn down by the service, they will continue to run and use Google Compute Engine VM resources in the user's project until they are explicitly terminated by the user. Because of this, Google recommends using the `TEARDOWN_ALWAYS` policy except for small, manually supervised test jobs. If unknown or unspecified, the service will attempt to choose a reasonable default.", alias="teardownPolicy")
    worker_harness_container_image: Optional[StrictStr] = Field(default=None, description="Required. Docker container image that executes the Cloud Dataflow worker harness, residing in Google Container Registry. Deprecated for the Fn API path. Use sdk_harness_container_images instead.", alias="workerHarnessContainerImage")
    zone: Optional[StrictStr] = Field(default=None, description="Zone to run the worker pools in. If empty or unspecified, the service will attempt to choose a reasonable default.")
    __properties: ClassVar[List[str]] = ["autoscalingSettings", "dataDisks", "defaultPackageSet", "diskSizeGb", "diskSourceImage", "diskType", "ipConfiguration", "kind", "machineType", "metadata", "network", "numThreadsPerWorker", "numWorkers", "onHostMaintenance", "packages", "poolArgs", "sdkHarnessContainerImages", "subnetwork", "taskrunnerSettings", "teardownPolicy", "workerHarnessContainerImage", "zone"]

    @field_validator('default_package_set')
    def default_package_set_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['DEFAULT_PACKAGE_SET_UNKNOWN', 'DEFAULT_PACKAGE_SET_NONE', 'DEFAULT_PACKAGE_SET_JAVA', 'DEFAULT_PACKAGE_SET_PYTHON']):
            raise ValueError("must be one of enum values ('DEFAULT_PACKAGE_SET_UNKNOWN', 'DEFAULT_PACKAGE_SET_NONE', 'DEFAULT_PACKAGE_SET_JAVA', 'DEFAULT_PACKAGE_SET_PYTHON')")
        return value

    @field_validator('ip_configuration')
    def ip_configuration_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['WORKER_IP_UNSPECIFIED', 'WORKER_IP_PUBLIC', 'WORKER_IP_PRIVATE']):
            raise ValueError("must be one of enum values ('WORKER_IP_UNSPECIFIED', 'WORKER_IP_PUBLIC', 'WORKER_IP_PRIVATE')")
        return value

    @field_validator('teardown_policy')
    def teardown_policy_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['TEARDOWN_POLICY_UNKNOWN', 'TEARDOWN_ALWAYS', 'TEARDOWN_ON_SUCCESS', 'TEARDOWN_NEVER']):
            raise ValueError("must be one of enum values ('TEARDOWN_POLICY_UNKNOWN', 'TEARDOWN_ALWAYS', 'TEARDOWN_ON_SUCCESS', 'TEARDOWN_NEVER')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of WorkerPool from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of autoscaling_settings
        if self.autoscaling_settings:
            _dict['autoscalingSettings'] = self.autoscaling_settings.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in data_disks (list)
        _items = []
        if self.data_disks:
            for _item_data_disks in self.data_disks:
                if _item_data_disks:
                    _items.append(_item_data_disks.to_dict())
            _dict['dataDisks'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in packages (list)
        _items = []
        if self.packages:
            for _item_packages in self.packages:
                if _item_packages:
                    _items.append(_item_packages.to_dict())
            _dict['packages'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in sdk_harness_container_images (list)
        _items = []
        if self.sdk_harness_container_images:
            for _item_sdk_harness_container_images in self.sdk_harness_container_images:
                if _item_sdk_harness_container_images:
                    _items.append(_item_sdk_harness_container_images.to_dict())
            _dict['sdkHarnessContainerImages'] = _items
        # override the default output from pydantic by calling `to_dict()` of taskrunner_settings
        if self.taskrunner_settings:
            _dict['taskrunnerSettings'] = self.taskrunner_settings.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of WorkerPool from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "autoscalingSettings": AutoscalingSettings.from_dict(obj["autoscalingSettings"]) if obj.get("autoscalingSettings") is not None else None,
            "dataDisks": [Disk.from_dict(_item) for _item in obj["dataDisks"]] if obj.get("dataDisks") is not None else None,
            "defaultPackageSet": obj.get("defaultPackageSet"),
            "diskSizeGb": obj.get("diskSizeGb"),
            "diskSourceImage": obj.get("diskSourceImage"),
            "diskType": obj.get("diskType"),
            "ipConfiguration": obj.get("ipConfiguration"),
            "kind": obj.get("kind"),
            "machineType": obj.get("machineType"),
            "metadata": obj.get("metadata"),
            "network": obj.get("network"),
            "numThreadsPerWorker": obj.get("numThreadsPerWorker"),
            "numWorkers": obj.get("numWorkers"),
            "onHostMaintenance": obj.get("onHostMaintenance"),
            "packages": [Package.from_dict(_item) for _item in obj["packages"]] if obj.get("packages") is not None else None,
            "poolArgs": obj.get("poolArgs"),
            "sdkHarnessContainerImages": [SdkHarnessContainerImage.from_dict(_item) for _item in obj["sdkHarnessContainerImages"]] if obj.get("sdkHarnessContainerImages") is not None else None,
            "subnetwork": obj.get("subnetwork"),
            "taskrunnerSettings": TaskRunnerSettings.from_dict(obj["taskrunnerSettings"]) if obj.get("taskrunnerSettings") is not None else None,
            "teardownPolicy": obj.get("teardownPolicy"),
            "workerHarnessContainerImage": obj.get("workerHarnessContainerImage"),
            "zone": obj.get("zone")
        })
        return _obj


