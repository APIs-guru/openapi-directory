# coding: utf-8

"""
    Dataflow API

    Manages Google Cloud Dataflow projects on Google Cloud Platform.

    The version of the OpenAPI document: v1b3
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.worker_settings import WorkerSettings
from typing import Optional, Set
from typing_extensions import Self

class TaskRunnerSettings(BaseModel):
    """
    Taskrunner configuration settings.
    """ # noqa: E501
    alsologtostderr: Optional[StrictBool] = Field(default=None, description="Whether to also send taskrunner log info to stderr.")
    base_task_dir: Optional[StrictStr] = Field(default=None, description="The location on the worker for task-specific subdirectories.", alias="baseTaskDir")
    base_url: Optional[StrictStr] = Field(default=None, description="The base URL for the taskrunner to use when accessing Google Cloud APIs. When workers access Google Cloud APIs, they logically do so via relative URLs. If this field is specified, it supplies the base URL to use for resolving these relative URLs. The normative algorithm used is defined by RFC 1808, \"Relative Uniform Resource Locators\". If not specified, the default value is \"http://www.googleapis.com/\"", alias="baseUrl")
    commandlines_file_name: Optional[StrictStr] = Field(default=None, description="The file to store preprocessing commands in.", alias="commandlinesFileName")
    continue_on_exception: Optional[StrictBool] = Field(default=None, description="Whether to continue taskrunner if an exception is hit.", alias="continueOnException")
    dataflow_api_version: Optional[StrictStr] = Field(default=None, description="The API version of endpoint, e.g. \"v1b3\"", alias="dataflowApiVersion")
    harness_command: Optional[StrictStr] = Field(default=None, description="The command to launch the worker harness.", alias="harnessCommand")
    language_hint: Optional[StrictStr] = Field(default=None, description="The suggested backend language.", alias="languageHint")
    log_dir: Optional[StrictStr] = Field(default=None, description="The directory on the VM to store logs.", alias="logDir")
    log_to_serialconsole: Optional[StrictBool] = Field(default=None, description="Whether to send taskrunner log info to Google Compute Engine VM serial console.", alias="logToSerialconsole")
    log_upload_location: Optional[StrictStr] = Field(default=None, description="Indicates where to put logs. If this is not specified, the logs will not be uploaded. The supported resource type is: Google Cloud Storage: storage.googleapis.com/{bucket}/{object} bucket.storage.googleapis.com/{object}", alias="logUploadLocation")
    oauth_scopes: Optional[List[StrictStr]] = Field(default=None, description="The OAuth2 scopes to be requested by the taskrunner in order to access the Cloud Dataflow API.", alias="oauthScopes")
    parallel_worker_settings: Optional[WorkerSettings] = Field(default=None, alias="parallelWorkerSettings")
    streaming_worker_main_class: Optional[StrictStr] = Field(default=None, description="The streaming worker main class name.", alias="streamingWorkerMainClass")
    task_group: Optional[StrictStr] = Field(default=None, description="The UNIX group ID on the worker VM to use for tasks launched by taskrunner; e.g. \"wheel\".", alias="taskGroup")
    task_user: Optional[StrictStr] = Field(default=None, description="The UNIX user ID on the worker VM to use for tasks launched by taskrunner; e.g. \"root\".", alias="taskUser")
    temp_storage_prefix: Optional[StrictStr] = Field(default=None, description="The prefix of the resources the taskrunner should use for temporary storage. The supported resource type is: Google Cloud Storage: storage.googleapis.com/{bucket}/{object} bucket.storage.googleapis.com/{object}", alias="tempStoragePrefix")
    vm_id: Optional[StrictStr] = Field(default=None, description="The ID string of the VM.", alias="vmId")
    workflow_file_name: Optional[StrictStr] = Field(default=None, description="The file to store the workflow in.", alias="workflowFileName")
    __properties: ClassVar[List[str]] = ["alsologtostderr", "baseTaskDir", "baseUrl", "commandlinesFileName", "continueOnException", "dataflowApiVersion", "harnessCommand", "languageHint", "logDir", "logToSerialconsole", "logUploadLocation", "oauthScopes", "parallelWorkerSettings", "streamingWorkerMainClass", "taskGroup", "taskUser", "tempStoragePrefix", "vmId", "workflowFileName"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of TaskRunnerSettings from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of parallel_worker_settings
        if self.parallel_worker_settings:
            _dict['parallelWorkerSettings'] = self.parallel_worker_settings.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of TaskRunnerSettings from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "alsologtostderr": obj.get("alsologtostderr"),
            "baseTaskDir": obj.get("baseTaskDir"),
            "baseUrl": obj.get("baseUrl"),
            "commandlinesFileName": obj.get("commandlinesFileName"),
            "continueOnException": obj.get("continueOnException"),
            "dataflowApiVersion": obj.get("dataflowApiVersion"),
            "harnessCommand": obj.get("harnessCommand"),
            "languageHint": obj.get("languageHint"),
            "logDir": obj.get("logDir"),
            "logToSerialconsole": obj.get("logToSerialconsole"),
            "logUploadLocation": obj.get("logUploadLocation"),
            "oauthScopes": obj.get("oauthScopes"),
            "parallelWorkerSettings": WorkerSettings.from_dict(obj["parallelWorkerSettings"]) if obj.get("parallelWorkerSettings") is not None else None,
            "streamingWorkerMainClass": obj.get("streamingWorkerMainClass"),
            "taskGroup": obj.get("taskGroup"),
            "taskUser": obj.get("taskUser"),
            "tempStoragePrefix": obj.get("tempStoragePrefix"),
            "vmId": obj.get("vmId"),
            "workflowFileName": obj.get("workflowFileName")
        })
        return _obj


