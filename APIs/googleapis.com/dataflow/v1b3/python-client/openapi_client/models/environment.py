# coding: utf-8

"""
    Dataflow API

    Manages Google Cloud Dataflow projects on Google Cloud Platform.

    The version of the OpenAPI document: v1b3
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.debug_options import DebugOptions
from openapi_client.models.worker_pool import WorkerPool
from typing import Optional, Set
from typing_extensions import Self

class Environment(BaseModel):
    """
    Describes the environment in which a Dataflow Job runs.
    """ # noqa: E501
    cluster_manager_api_service: Optional[StrictStr] = Field(default=None, description="The type of cluster manager API to use. If unknown or unspecified, the service will attempt to choose a reasonable default. This should be in the form of the API service name, e.g. \"compute.googleapis.com\".", alias="clusterManagerApiService")
    dataset: Optional[StrictStr] = Field(default=None, description="The dataset for the current project where various workflow related tables are stored. The supported resource type is: Google BigQuery: bigquery.googleapis.com/{dataset}")
    debug_options: Optional[DebugOptions] = Field(default=None, alias="debugOptions")
    experiments: Optional[List[StrictStr]] = Field(default=None, description="The list of experiments to enable. This field should be used for SDK related experiments and not for service related experiments. The proper field for service related experiments is service_options.")
    flex_resource_scheduling_goal: Optional[StrictStr] = Field(default=None, description="Which Flexible Resource Scheduling mode to run in.", alias="flexResourceSchedulingGoal")
    internal_experiments: Optional[Dict[str, Any]] = Field(default=None, description="Experimental settings.", alias="internalExperiments")
    sdk_pipeline_options: Optional[Dict[str, Any]] = Field(default=None, description="The Cloud Dataflow SDK pipeline options specified by the user. These options are passed through the service and are used to recreate the SDK pipeline options on the worker in a language agnostic and platform independent way.", alias="sdkPipelineOptions")
    service_account_email: Optional[StrictStr] = Field(default=None, description="Identity to run virtual machines as. Defaults to the default account.", alias="serviceAccountEmail")
    service_kms_key_name: Optional[StrictStr] = Field(default=None, description="If set, contains the Cloud KMS key identifier used to encrypt data at rest, AKA a Customer Managed Encryption Key (CMEK). Format: projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY", alias="serviceKmsKeyName")
    service_options: Optional[List[StrictStr]] = Field(default=None, description="The list of service options to enable. This field should be used for service related experiments only. These experiments, when graduating to GA, should be replaced by dedicated fields or become default (i.e. always on).", alias="serviceOptions")
    shuffle_mode: Optional[StrictStr] = Field(default=None, description="Output only. The shuffle mode used for the job.", alias="shuffleMode")
    streaming_mode: Optional[StrictStr] = Field(default=None, description="Optional. Specifies the Streaming Engine message processing guarantees. Reduces cost and latency but might result in duplicate messages committed to storage. Designed to run simple mapping streaming ETL jobs at the lowest cost. For example, Change Data Capture (CDC) to BigQuery is a canonical use case.", alias="streamingMode")
    temp_storage_prefix: Optional[StrictStr] = Field(default=None, description="The prefix of the resources the system should use for temporary storage. The system will append the suffix \"/temp-{JOBNAME} to this resource prefix, where {JOBNAME} is the value of the job_name field. The resulting bucket and object prefix is used as the prefix of the resources used to store temporary data needed during the job execution. NOTE: This will override the value in taskrunner_settings. The supported resource type is: Google Cloud Storage: storage.googleapis.com/{bucket}/{object} bucket.storage.googleapis.com/{object}", alias="tempStoragePrefix")
    use_streaming_engine_resource_based_billing: Optional[StrictBool] = Field(default=None, description="Output only. Whether the job uses the Streaming Engine resource-based billing model.", alias="useStreamingEngineResourceBasedBilling")
    user_agent: Optional[Dict[str, Any]] = Field(default=None, description="A description of the process that generated the request.", alias="userAgent")
    version: Optional[Dict[str, Any]] = Field(default=None, description="A structure describing which components and their versions of the service are required in order to run the job.")
    worker_pools: Optional[List[WorkerPool]] = Field(default=None, description="The worker pools. At least one \"harness\" worker pool must be specified in order for the job to have workers.", alias="workerPools")
    worker_region: Optional[StrictStr] = Field(default=None, description="The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1\". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, default to the control plane's region.", alias="workerRegion")
    worker_zone: Optional[StrictStr] = Field(default=None, description="The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. \"us-west1-a\". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane's region is chosen based on available capacity.", alias="workerZone")
    __properties: ClassVar[List[str]] = ["clusterManagerApiService", "dataset", "debugOptions", "experiments", "flexResourceSchedulingGoal", "internalExperiments", "sdkPipelineOptions", "serviceAccountEmail", "serviceKmsKeyName", "serviceOptions", "shuffleMode", "streamingMode", "tempStoragePrefix", "useStreamingEngineResourceBasedBilling", "userAgent", "version", "workerPools", "workerRegion", "workerZone"]

    @field_validator('flex_resource_scheduling_goal')
    def flex_resource_scheduling_goal_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['FLEXRS_UNSPECIFIED', 'FLEXRS_SPEED_OPTIMIZED', 'FLEXRS_COST_OPTIMIZED']):
            raise ValueError("must be one of enum values ('FLEXRS_UNSPECIFIED', 'FLEXRS_SPEED_OPTIMIZED', 'FLEXRS_COST_OPTIMIZED')")
        return value

    @field_validator('shuffle_mode')
    def shuffle_mode_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['SHUFFLE_MODE_UNSPECIFIED', 'VM_BASED', 'SERVICE_BASED']):
            raise ValueError("must be one of enum values ('SHUFFLE_MODE_UNSPECIFIED', 'VM_BASED', 'SERVICE_BASED')")
        return value

    @field_validator('streaming_mode')
    def streaming_mode_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['STREAMING_MODE_UNSPECIFIED', 'STREAMING_MODE_EXACTLY_ONCE', 'STREAMING_MODE_AT_LEAST_ONCE']):
            raise ValueError("must be one of enum values ('STREAMING_MODE_UNSPECIFIED', 'STREAMING_MODE_EXACTLY_ONCE', 'STREAMING_MODE_AT_LEAST_ONCE')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of Environment from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "shuffle_mode",
            "use_streaming_engine_resource_based_billing",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of debug_options
        if self.debug_options:
            _dict['debugOptions'] = self.debug_options.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in worker_pools (list)
        _items = []
        if self.worker_pools:
            for _item_worker_pools in self.worker_pools:
                if _item_worker_pools:
                    _items.append(_item_worker_pools.to_dict())
            _dict['workerPools'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of Environment from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "clusterManagerApiService": obj.get("clusterManagerApiService"),
            "dataset": obj.get("dataset"),
            "debugOptions": DebugOptions.from_dict(obj["debugOptions"]) if obj.get("debugOptions") is not None else None,
            "experiments": obj.get("experiments"),
            "flexResourceSchedulingGoal": obj.get("flexResourceSchedulingGoal"),
            "internalExperiments": obj.get("internalExperiments"),
            "sdkPipelineOptions": obj.get("sdkPipelineOptions"),
            "serviceAccountEmail": obj.get("serviceAccountEmail"),
            "serviceKmsKeyName": obj.get("serviceKmsKeyName"),
            "serviceOptions": obj.get("serviceOptions"),
            "shuffleMode": obj.get("shuffleMode"),
            "streamingMode": obj.get("streamingMode"),
            "tempStoragePrefix": obj.get("tempStoragePrefix"),
            "useStreamingEngineResourceBasedBilling": obj.get("useStreamingEngineResourceBasedBilling"),
            "userAgent": obj.get("userAgent"),
            "version": obj.get("version"),
            "workerPools": [WorkerPool.from_dict(_item) for _item in obj["workerPools"]] if obj.get("workerPools") is not None else None,
            "workerRegion": obj.get("workerRegion"),
            "workerZone": obj.get("workerZone")
        })
        return _obj


