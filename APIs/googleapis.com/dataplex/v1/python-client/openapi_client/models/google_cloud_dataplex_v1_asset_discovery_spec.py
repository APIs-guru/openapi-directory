# coding: utf-8

"""
    Cloud Dataplex API

    Dataplex API is used to manage the lifecycle of data lakes.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_cloud_dataplex_v1_asset_discovery_spec_csv_options import GoogleCloudDataplexV1AssetDiscoverySpecCsvOptions
from openapi_client.models.google_cloud_dataplex_v1_asset_discovery_spec_json_options import GoogleCloudDataplexV1AssetDiscoverySpecJsonOptions
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudDataplexV1AssetDiscoverySpec(BaseModel):
    """
    Settings to manage the metadata discovery and publishing for an asset.
    """ # noqa: E501
    csv_options: Optional[GoogleCloudDataplexV1AssetDiscoverySpecCsvOptions] = Field(default=None, alias="csvOptions")
    enabled: Optional[StrictBool] = Field(default=None, description="Optional. Whether discovery is enabled.")
    exclude_patterns: Optional[List[StrictStr]] = Field(default=None, description="Optional. The list of patterns to apply for selecting data to exclude during discovery. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.", alias="excludePatterns")
    include_patterns: Optional[List[StrictStr]] = Field(default=None, description="Optional. The list of patterns to apply for selecting data to include during discovery if only a subset of the data should considered. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.", alias="includePatterns")
    json_options: Optional[GoogleCloudDataplexV1AssetDiscoverySpecJsonOptions] = Field(default=None, alias="jsonOptions")
    schedule: Optional[StrictStr] = Field(default=None, description="Optional. Cron schedule (https://en.wikipedia.org/wiki/Cron) for running discovery periodically. Successive discovery runs must be scheduled at least 60 minutes apart. The default value is to run discovery every 60 minutes. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: \"CRON_TZ=${IANA_TIME_ZONE}\" or TZ=${IANA_TIME_ZONE}\". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, CRON_TZ=America/New_York 1 * * * *, or TZ=America/New_York 1 * * * *.")
    __properties: ClassVar[List[str]] = ["csvOptions", "enabled", "excludePatterns", "includePatterns", "jsonOptions", "schedule"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudDataplexV1AssetDiscoverySpec from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of csv_options
        if self.csv_options:
            _dict['csvOptions'] = self.csv_options.to_dict()
        # override the default output from pydantic by calling `to_dict()` of json_options
        if self.json_options:
            _dict['jsonOptions'] = self.json_options.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudDataplexV1AssetDiscoverySpec from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "csvOptions": GoogleCloudDataplexV1AssetDiscoverySpecCsvOptions.from_dict(obj["csvOptions"]) if obj.get("csvOptions") is not None else None,
            "enabled": obj.get("enabled"),
            "excludePatterns": obj.get("excludePatterns"),
            "includePatterns": obj.get("includePatterns"),
            "jsonOptions": GoogleCloudDataplexV1AssetDiscoverySpecJsonOptions.from_dict(obj["jsonOptions"]) if obj.get("jsonOptions") is not None else None,
            "schedule": obj.get("schedule")
        })
        return _obj


