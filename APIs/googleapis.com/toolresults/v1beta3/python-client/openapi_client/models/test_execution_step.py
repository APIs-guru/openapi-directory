# coding: utf-8

"""
    Cloud Tool Results API

    API to publish and access results from developer tools.

    The version of the OpenAPI document: v1beta3
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.test_issue import TestIssue
from openapi_client.models.test_suite_overview import TestSuiteOverview
from openapi_client.models.test_timing import TestTiming
from openapi_client.models.tool_execution import ToolExecution
from typing import Optional, Set
from typing_extensions import Self

class TestExecutionStep(BaseModel):
    """
    A step that represents running tests. It accepts ant-junit xml files which will be parsed into structured test results by the service. Xml file paths are updated in order to append more files, however they can't be deleted. Users can also add test results manually by using the test_result field.
    """ # noqa: E501
    test_issues: Optional[List[TestIssue]] = Field(default=None, description="Issues observed during the test execution. For example, if the mobile app under test crashed during the test, the error message and the stack trace content can be recorded here to assist debugging. - In response: present if set by create or update - In create/update request: optional", alias="testIssues")
    test_suite_overviews: Optional[List[TestSuiteOverview]] = Field(default=None, description="List of test suite overview contents. This could be parsed from xUnit XML log by server, or uploaded directly by user. This references should only be called when test suites are fully parsed or uploaded. The maximum allowed number of test suite overviews per step is 1000. - In response: always set - In create request: optional - In update request: never (use publishXunitXmlFiles custom method instead)", alias="testSuiteOverviews")
    test_timing: Optional[TestTiming] = Field(default=None, alias="testTiming")
    tool_execution: Optional[ToolExecution] = Field(default=None, alias="toolExecution")
    __properties: ClassVar[List[str]] = ["testIssues", "testSuiteOverviews", "testTiming", "toolExecution"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of TestExecutionStep from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in test_issues (list)
        _items = []
        if self.test_issues:
            for _item_test_issues in self.test_issues:
                if _item_test_issues:
                    _items.append(_item_test_issues.to_dict())
            _dict['testIssues'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in test_suite_overviews (list)
        _items = []
        if self.test_suite_overviews:
            for _item_test_suite_overviews in self.test_suite_overviews:
                if _item_test_suite_overviews:
                    _items.append(_item_test_suite_overviews.to_dict())
            _dict['testSuiteOverviews'] = _items
        # override the default output from pydantic by calling `to_dict()` of test_timing
        if self.test_timing:
            _dict['testTiming'] = self.test_timing.to_dict()
        # override the default output from pydantic by calling `to_dict()` of tool_execution
        if self.tool_execution:
            _dict['toolExecution'] = self.tool_execution.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of TestExecutionStep from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "testIssues": [TestIssue.from_dict(_item) for _item in obj["testIssues"]] if obj.get("testIssues") is not None else None,
            "testSuiteOverviews": [TestSuiteOverview.from_dict(_item) for _item in obj["testSuiteOverviews"]] if obj.get("testSuiteOverviews") is not None else None,
            "testTiming": TestTiming.from_dict(obj["testTiming"]) if obj.get("testTiming") is not None else None,
            "toolExecution": ToolExecution.from_dict(obj["toolExecution"]) if obj.get("toolExecution") is not None else None
        })
        return _obj


