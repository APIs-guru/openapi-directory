# coding: utf-8

"""
    Sensitive Data Protection (DLP)

    Discover and protect your sensitive data. A fully managed service designed to help you discover, classify, and protect your valuable data assets with ease.

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_privacy_dlp_v2_big_query_table import GooglePrivacyDlpV2BigQueryTable
from openapi_client.models.google_privacy_dlp_v2_field_id import GooglePrivacyDlpV2FieldId
from typing import Optional, Set
from typing_extensions import Self

class GooglePrivacyDlpV2BigQueryOptions(BaseModel):
    """
    Options defining BigQuery table and row identifiers.
    """ # noqa: E501
    excluded_fields: Optional[List[GooglePrivacyDlpV2FieldId]] = Field(default=None, description="References to fields excluded from scanning. This allows you to skip inspection of entire columns which you know have no findings. When inspecting a table, we recommend that you inspect all columns. Otherwise, findings might be affected because hints from excluded columns will not be used.", alias="excludedFields")
    identifying_fields: Optional[List[GooglePrivacyDlpV2FieldId]] = Field(default=None, description="Table fields that may uniquely identify a row within the table. When `actions.saveFindings.outputConfig.table` is specified, the values of columns specified here are available in the output table under `location.content_locations.record_location.record_key.id_values`. Nested fields such as `person.birthdate.year` are allowed.", alias="identifyingFields")
    included_fields: Optional[List[GooglePrivacyDlpV2FieldId]] = Field(default=None, description="Limit scanning only to these fields. When inspecting a table, we recommend that you inspect all columns. Otherwise, findings might be affected because hints from excluded columns will not be used.", alias="includedFields")
    rows_limit: Optional[StrictStr] = Field(default=None, description="Max number of rows to scan. If the table has more rows than this value, the rest of the rows are omitted. If not set, or if set to 0, all rows will be scanned. Only one of rows_limit and rows_limit_percent can be specified. Cannot be used in conjunction with TimespanConfig.", alias="rowsLimit")
    rows_limit_percent: Optional[StrictInt] = Field(default=None, description="Max percentage of rows to scan. The rest are omitted. The number of rows scanned is rounded down. Must be between 0 and 100, inclusively. Both 0 and 100 means no limit. Defaults to 0. Only one of rows_limit and rows_limit_percent can be specified. Cannot be used in conjunction with TimespanConfig. Caution: A [known issue](https://cloud.google.com/sensitive-data-protection/docs/known-issues#bq-sampling) is causing the `rowsLimitPercent` field to behave unexpectedly. We recommend using `rowsLimit` instead.", alias="rowsLimitPercent")
    sample_method: Optional[StrictStr] = Field(default=None, description="How to sample the data.", alias="sampleMethod")
    table_reference: Optional[GooglePrivacyDlpV2BigQueryTable] = Field(default=None, alias="tableReference")
    __properties: ClassVar[List[str]] = ["excludedFields", "identifyingFields", "includedFields", "rowsLimit", "rowsLimitPercent", "sampleMethod", "tableReference"]

    @field_validator('sample_method')
    def sample_method_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['SAMPLE_METHOD_UNSPECIFIED', 'TOP', 'RANDOM_START']):
            raise ValueError("must be one of enum values ('SAMPLE_METHOD_UNSPECIFIED', 'TOP', 'RANDOM_START')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GooglePrivacyDlpV2BigQueryOptions from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in excluded_fields (list)
        _items = []
        if self.excluded_fields:
            for _item_excluded_fields in self.excluded_fields:
                if _item_excluded_fields:
                    _items.append(_item_excluded_fields.to_dict())
            _dict['excludedFields'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in identifying_fields (list)
        _items = []
        if self.identifying_fields:
            for _item_identifying_fields in self.identifying_fields:
                if _item_identifying_fields:
                    _items.append(_item_identifying_fields.to_dict())
            _dict['identifyingFields'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in included_fields (list)
        _items = []
        if self.included_fields:
            for _item_included_fields in self.included_fields:
                if _item_included_fields:
                    _items.append(_item_included_fields.to_dict())
            _dict['includedFields'] = _items
        # override the default output from pydantic by calling `to_dict()` of table_reference
        if self.table_reference:
            _dict['tableReference'] = self.table_reference.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GooglePrivacyDlpV2BigQueryOptions from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "excludedFields": [GooglePrivacyDlpV2FieldId.from_dict(_item) for _item in obj["excludedFields"]] if obj.get("excludedFields") is not None else None,
            "identifyingFields": [GooglePrivacyDlpV2FieldId.from_dict(_item) for _item in obj["identifyingFields"]] if obj.get("identifyingFields") is not None else None,
            "includedFields": [GooglePrivacyDlpV2FieldId.from_dict(_item) for _item in obj["includedFields"]] if obj.get("includedFields") is not None else None,
            "rowsLimit": obj.get("rowsLimit"),
            "rowsLimitPercent": obj.get("rowsLimitPercent"),
            "sampleMethod": obj.get("sampleMethod"),
            "tableReference": GooglePrivacyDlpV2BigQueryTable.from_dict(obj["tableReference"]) if obj.get("tableReference") is not None else None
        })
        return _obj


