# coding: utf-8

"""
    Data Labeling API

    Public API for Google Cloud AI Data Labeling Service.

    The version of the OpenAPI document: v1beta1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_cloud_datalabeling_v1beta1_attempt import GoogleCloudDatalabelingV1beta1Attempt
from openapi_client.models.google_cloud_datalabeling_v1beta1_evaluation_job_config import GoogleCloudDatalabelingV1beta1EvaluationJobConfig
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudDatalabelingV1beta1EvaluationJob(BaseModel):
    """
    Defines an evaluation job that runs periodically to generate Evaluations. [Creating an evaluation job](/ml-engine/docs/continuous-evaluation/create-job) is the starting point for using continuous evaluation.
    """ # noqa: E501
    annotation_spec_set: Optional[StrictStr] = Field(default=None, description="Required. Name of the AnnotationSpecSet describing all the labels that your machine learning model outputs. You must create this resource before you create an evaluation job and provide its name in the following format: \"projects/{project_id}/annotationSpecSets/{annotation_spec_set_id}\"", alias="annotationSpecSet")
    attempts: Optional[List[GoogleCloudDatalabelingV1beta1Attempt]] = Field(default=None, description="Output only. Every time the evaluation job runs and an error occurs, the failed attempt is appended to this array.")
    create_time: Optional[StrictStr] = Field(default=None, description="Output only. Timestamp of when this evaluation job was created.", alias="createTime")
    description: Optional[StrictStr] = Field(default=None, description="Required. Description of the job. The description can be up to 25,000 characters long.")
    evaluation_job_config: Optional[GoogleCloudDatalabelingV1beta1EvaluationJobConfig] = Field(default=None, alias="evaluationJobConfig")
    label_missing_ground_truth: Optional[StrictBool] = Field(default=None, description="Required. Whether you want Data Labeling Service to provide ground truth labels for prediction input. If you want the service to assign human labelers to annotate your data, set this to `true`. If you want to provide your own ground truth labels in the evaluation job's BigQuery table, set this to `false`.", alias="labelMissingGroundTruth")
    model_version: Optional[StrictStr] = Field(default=None, description="Required. The [AI Platform Prediction model version](/ml-engine/docs/prediction-overview) to be evaluated. Prediction input and output is sampled from this model version. When creating an evaluation job, specify the model version in the following format: \"projects/{project_id}/models/{model_name}/versions/{version_name}\" There can only be one evaluation job per model version.", alias="modelVersion")
    name: Optional[StrictStr] = Field(default=None, description="Output only. After you create a job, Data Labeling Service assigns a name to the job with the following format: \"projects/{project_id}/evaluationJobs/ {evaluation_job_id}\"")
    schedule: Optional[StrictStr] = Field(default=None, description="Required. Describes the interval at which the job runs. This interval must be at least 1 day, and it is rounded to the nearest day. For example, if you specify a 50-hour interval, the job runs every 2 days. You can provide the schedule in [crontab format](/scheduler/docs/configuring/cron-job-schedules) or in an [English-like format](/appengine/docs/standard/python/config/cronref#schedule_format). Regardless of what you specify, the job will run at 10:00 AM UTC. Only the interval from this schedule is used, not the specific time of day.")
    state: Optional[StrictStr] = Field(default=None, description="Output only. Describes the current state of the job.")
    __properties: ClassVar[List[str]] = ["annotationSpecSet", "attempts", "createTime", "description", "evaluationJobConfig", "labelMissingGroundTruth", "modelVersion", "name", "schedule", "state"]

    @field_validator('state')
    def state_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['STATE_UNSPECIFIED', 'SCHEDULED', 'RUNNING', 'PAUSED', 'STOPPED']):
            raise ValueError("must be one of enum values ('STATE_UNSPECIFIED', 'SCHEDULED', 'RUNNING', 'PAUSED', 'STOPPED')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudDatalabelingV1beta1EvaluationJob from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in attempts (list)
        _items = []
        if self.attempts:
            for _item_attempts in self.attempts:
                if _item_attempts:
                    _items.append(_item_attempts.to_dict())
            _dict['attempts'] = _items
        # override the default output from pydantic by calling `to_dict()` of evaluation_job_config
        if self.evaluation_job_config:
            _dict['evaluationJobConfig'] = self.evaluation_job_config.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudDatalabelingV1beta1EvaluationJob from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "annotationSpecSet": obj.get("annotationSpecSet"),
            "attempts": [GoogleCloudDatalabelingV1beta1Attempt.from_dict(_item) for _item in obj["attempts"]] if obj.get("attempts") is not None else None,
            "createTime": obj.get("createTime"),
            "description": obj.get("description"),
            "evaluationJobConfig": GoogleCloudDatalabelingV1beta1EvaluationJobConfig.from_dict(obj["evaluationJobConfig"]) if obj.get("evaluationJobConfig") is not None else None,
            "labelMissingGroundTruth": obj.get("labelMissingGroundTruth"),
            "modelVersion": obj.get("modelVersion"),
            "name": obj.get("name"),
            "schedule": obj.get("schedule"),
            "state": obj.get("state")
        })
        return _obj


