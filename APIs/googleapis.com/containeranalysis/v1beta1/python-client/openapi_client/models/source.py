# coding: utf-8

"""
    Container Analysis API

    This API is a prerequisite for leveraging Artifact Analysis scanning capabilities in both Artifact Registry and with Advanced Vulnerability Insights (runtime scanning) in GKE. In addition, the Container Analysis API is an implementation of the Grafeas API, which enables storing, querying, and retrieval of critical metadata about all of your software artifacts.

    The version of the OpenAPI document: v1beta1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.file_hashes import FileHashes
from openapi_client.models.source_context import SourceContext
from typing import Optional, Set
from typing_extensions import Self

class Source(BaseModel):
    """
    Source describes the location of the source used for the build.
    """ # noqa: E501
    additional_contexts: Optional[List[SourceContext]] = Field(default=None, description="If provided, some of the source code used for the build may be found in these locations, in the case where the source repository had multiple remotes or submodules. This list will not include the context specified in the context field.", alias="additionalContexts")
    artifact_storage_source_uri: Optional[StrictStr] = Field(default=None, description="If provided, the input binary artifacts for the build came from this location.", alias="artifactStorageSourceUri")
    context: Optional[SourceContext] = None
    file_hashes: Optional[Dict[str, FileHashes]] = Field(default=None, description="Hash(es) of the build source, which can be used to verify that the original source integrity was maintained in the build. The keys to this map are file paths used as build source and the values contain the hash values for those files. If the build source came in a single package such as a gzipped tarfile (.tar.gz), the FileHash will be for the single path to that file.", alias="fileHashes")
    __properties: ClassVar[List[str]] = ["additionalContexts", "artifactStorageSourceUri", "context", "fileHashes"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of Source from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in additional_contexts (list)
        _items = []
        if self.additional_contexts:
            for _item_additional_contexts in self.additional_contexts:
                if _item_additional_contexts:
                    _items.append(_item_additional_contexts.to_dict())
            _dict['additionalContexts'] = _items
        # override the default output from pydantic by calling `to_dict()` of context
        if self.context:
            _dict['context'] = self.context.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each value in file_hashes (dict)
        _field_dict = {}
        if self.file_hashes:
            for _key_file_hashes in self.file_hashes:
                if self.file_hashes[_key_file_hashes]:
                    _field_dict[_key_file_hashes] = self.file_hashes[_key_file_hashes].to_dict()
            _dict['fileHashes'] = _field_dict
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of Source from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "additionalContexts": [SourceContext.from_dict(_item) for _item in obj["additionalContexts"]] if obj.get("additionalContexts") is not None else None,
            "artifactStorageSourceUri": obj.get("artifactStorageSourceUri"),
            "context": SourceContext.from_dict(obj["context"]) if obj.get("context") is not None else None,
            "fileHashes": dict(
                (_k, FileHashes.from_dict(_v))
                for _k, _v in obj["fileHashes"].items()
            )
            if obj.get("fileHashes") is not None
            else None
        })
        return _obj


