# coding: utf-8

"""
    BigQuery Data Transfer API

    Schedule queries or transfer external data from SaaS applications to Google BigQuery on a regular basis.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.data_source_parameter import DataSourceParameter
from typing import Optional, Set
from typing_extensions import Self

class DataSource(BaseModel):
    """
    Defines the properties and custom parameters for a data source.
    """ # noqa: E501
    authorization_type: Optional[StrictStr] = Field(default=None, description="Indicates the type of authorization.", alias="authorizationType")
    client_id: Optional[StrictStr] = Field(default=None, description="Data source client id which should be used to receive refresh token.", alias="clientId")
    data_refresh_type: Optional[StrictStr] = Field(default=None, description="Specifies whether the data source supports automatic data refresh for the past few days, and how it's supported. For some data sources, data might not be complete until a few days later, so it's useful to refresh data automatically.", alias="dataRefreshType")
    data_source_id: Optional[StrictStr] = Field(default=None, description="Data source id.", alias="dataSourceId")
    default_data_refresh_window_days: Optional[StrictInt] = Field(default=None, description="Default data refresh window on days. Only meaningful when `data_refresh_type` = `SLIDING_WINDOW`.", alias="defaultDataRefreshWindowDays")
    default_schedule: Optional[StrictStr] = Field(default=None, description="Default data transfer schedule. Examples of valid schedules include: `1st,3rd monday of month 15:30`, `every wed,fri of jan,jun 13:15`, and `first sunday of quarter 00:00`.", alias="defaultSchedule")
    description: Optional[StrictStr] = Field(default=None, description="User friendly data source description string.")
    display_name: Optional[StrictStr] = Field(default=None, description="User friendly data source name.", alias="displayName")
    help_url: Optional[StrictStr] = Field(default=None, description="Url for the help document for this data source.", alias="helpUrl")
    manual_runs_disabled: Optional[StrictBool] = Field(default=None, description="Disables backfilling and manual run scheduling for the data source.", alias="manualRunsDisabled")
    minimum_schedule_interval: Optional[StrictStr] = Field(default=None, description="The minimum interval for scheduler to schedule runs.", alias="minimumScheduleInterval")
    name: Optional[StrictStr] = Field(default=None, description="Output only. Data source resource name.")
    parameters: Optional[List[DataSourceParameter]] = Field(default=None, description="Data source parameters.")
    scopes: Optional[List[StrictStr]] = Field(default=None, description="Api auth scopes for which refresh token needs to be obtained. These are scopes needed by a data source to prepare data and ingest them into BigQuery, e.g., https://www.googleapis.com/auth/bigquery")
    supports_custom_schedule: Optional[StrictBool] = Field(default=None, description="Specifies whether the data source supports a user defined schedule, or operates on the default schedule. When set to `true`, user can override default schedule.", alias="supportsCustomSchedule")
    supports_multiple_transfers: Optional[StrictBool] = Field(default=None, description="Deprecated. This field has no effect.", alias="supportsMultipleTransfers")
    transfer_type: Optional[StrictStr] = Field(default=None, description="Deprecated. This field has no effect.", alias="transferType")
    update_deadline_seconds: Optional[StrictInt] = Field(default=None, description="The number of seconds to wait for an update from the data source before the Data Transfer Service marks the transfer as FAILED.", alias="updateDeadlineSeconds")
    __properties: ClassVar[List[str]] = ["authorizationType", "clientId", "dataRefreshType", "dataSourceId", "defaultDataRefreshWindowDays", "defaultSchedule", "description", "displayName", "helpUrl", "manualRunsDisabled", "minimumScheduleInterval", "name", "parameters", "scopes", "supportsCustomSchedule", "supportsMultipleTransfers", "transferType", "updateDeadlineSeconds"]

    @field_validator('authorization_type')
    def authorization_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['AUTHORIZATION_TYPE_UNSPECIFIED', 'AUTHORIZATION_CODE', 'GOOGLE_PLUS_AUTHORIZATION_CODE', 'FIRST_PARTY_OAUTH']):
            raise ValueError("must be one of enum values ('AUTHORIZATION_TYPE_UNSPECIFIED', 'AUTHORIZATION_CODE', 'GOOGLE_PLUS_AUTHORIZATION_CODE', 'FIRST_PARTY_OAUTH')")
        return value

    @field_validator('data_refresh_type')
    def data_refresh_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['DATA_REFRESH_TYPE_UNSPECIFIED', 'SLIDING_WINDOW', 'CUSTOM_SLIDING_WINDOW']):
            raise ValueError("must be one of enum values ('DATA_REFRESH_TYPE_UNSPECIFIED', 'SLIDING_WINDOW', 'CUSTOM_SLIDING_WINDOW')")
        return value

    @field_validator('transfer_type')
    def transfer_type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['TRANSFER_TYPE_UNSPECIFIED', 'BATCH', 'STREAMING']):
            raise ValueError("must be one of enum values ('TRANSFER_TYPE_UNSPECIFIED', 'BATCH', 'STREAMING')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of DataSource from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "name",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in parameters (list)
        _items = []
        if self.parameters:
            for _item_parameters in self.parameters:
                if _item_parameters:
                    _items.append(_item_parameters.to_dict())
            _dict['parameters'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of DataSource from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "authorizationType": obj.get("authorizationType"),
            "clientId": obj.get("clientId"),
            "dataRefreshType": obj.get("dataRefreshType"),
            "dataSourceId": obj.get("dataSourceId"),
            "defaultDataRefreshWindowDays": obj.get("defaultDataRefreshWindowDays"),
            "defaultSchedule": obj.get("defaultSchedule"),
            "description": obj.get("description"),
            "displayName": obj.get("displayName"),
            "helpUrl": obj.get("helpUrl"),
            "manualRunsDisabled": obj.get("manualRunsDisabled"),
            "minimumScheduleInterval": obj.get("minimumScheduleInterval"),
            "name": obj.get("name"),
            "parameters": [DataSourceParameter.from_dict(_item) for _item in obj["parameters"]] if obj.get("parameters") is not None else None,
            "scopes": obj.get("scopes"),
            "supportsCustomSchedule": obj.get("supportsCustomSchedule"),
            "supportsMultipleTransfers": obj.get("supportsMultipleTransfers"),
            "transferType": obj.get("transferType"),
            "updateDeadlineSeconds": obj.get("updateDeadlineSeconds")
        })
        return _obj


