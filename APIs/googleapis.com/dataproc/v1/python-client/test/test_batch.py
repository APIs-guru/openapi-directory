# coding: utf-8

"""
    Cloud Dataproc API

    Manages Hadoop-based clusters and jobs on Google Cloud Platform.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


import unittest

from openapi_client.models.batch import Batch

class TestBatch(unittest.TestCase):
    """Batch unit test stubs"""

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def make_instance(self, include_optional) -> Batch:
        """Test Batch
            include_optional is a boolean, when False only required
            params are included, when True both required and
            optional params are included """
        # uncomment below to create an instance of `Batch`
        """
        model = Batch()
        if include_optional:
            return Batch(
                create_time = '',
                creator = '',
                environment_config = openapi_client.models.environment_config.EnvironmentConfig(
                    execution_config = openapi_client.models.execution_config.ExecutionConfig(
                        idle_ttl = '', 
                        kms_key = '', 
                        network_tags = [
                            ''
                            ], 
                        network_uri = '', 
                        service_account = '', 
                        staging_bucket = '', 
                        subnetwork_uri = '', 
                        ttl = '', ), 
                    peripherals_config = openapi_client.models.peripherals_config.PeripheralsConfig(
                        metastore_service = '', 
                        spark_history_server_config = openapi_client.models.spark_history_server_config.SparkHistoryServerConfig(
                            dataproc_cluster = '', ), ), ),
                labels = {
                    'key' : ''
                    },
                name = '',
                operation = '',
                pyspark_batch = openapi_client.models.py_spark_batch.PySparkBatch(
                    archive_uris = [
                        ''
                        ], 
                    args = [
                        ''
                        ], 
                    file_uris = [
                        ''
                        ], 
                    jar_file_uris = [
                        ''
                        ], 
                    main_python_file_uri = '', 
                    python_file_uris = [
                        ''
                        ], ),
                runtime_config = openapi_client.models.runtime_config.RuntimeConfig(
                    container_image = '', 
                    properties = {
                        'key' : ''
                        }, 
                    repository_config = openapi_client.models.repository_config.RepositoryConfig(
                        pypi_repository_config = openapi_client.models.py_pi_repository_config.PyPiRepositoryConfig(
                            pypi_repository = '', ), ), 
                    version = '', ),
                runtime_info = openapi_client.models.runtime_info.RuntimeInfo(
                    approximate_usage = openapi_client.models.usage_metrics.UsageMetrics(
                        accelerator_type = '', 
                        milli_accelerator_seconds = '', 
                        milli_dcu_seconds = '', 
                        shuffle_storage_gb_seconds = '', ), 
                    current_usage = openapi_client.models.usage_snapshot.UsageSnapshot(
                        accelerator_type = '', 
                        milli_accelerator = '', 
                        milli_dcu = '', 
                        milli_dcu_premium = '', 
                        shuffle_storage_gb = '', 
                        shuffle_storage_gb_premium = '', 
                        snapshot_time = '', ), 
                    diagnostic_output_uri = '', 
                    endpoints = {
                        'key' : ''
                        }, 
                    output_uri = '', ),
                spark_batch = openapi_client.models.spark_batch.SparkBatch(
                    archive_uris = [
                        ''
                        ], 
                    args = [
                        ''
                        ], 
                    file_uris = [
                        ''
                        ], 
                    jar_file_uris = [
                        ''
                        ], 
                    main_class = '', 
                    main_jar_file_uri = '', ),
                spark_r_batch = openapi_client.models.spark_r_batch.SparkRBatch(
                    archive_uris = [
                        ''
                        ], 
                    args = [
                        ''
                        ], 
                    file_uris = [
                        ''
                        ], 
                    main_r_file_uri = '', ),
                spark_sql_batch = openapi_client.models.spark_sql_batch.SparkSqlBatch(
                    jar_file_uris = [
                        ''
                        ], 
                    query_file_uri = '', 
                    query_variables = {
                        'key' : ''
                        }, ),
                state = 'STATE_UNSPECIFIED',
                state_history = [
                    openapi_client.models.state_history.StateHistory(
                        state = 'STATE_UNSPECIFIED', 
                        state_message = '', 
                        state_start_time = '', )
                    ],
                state_message = '',
                state_time = '',
                uuid = ''
            )
        else:
            return Batch(
        )
        """

    def testBatch(self):
        """Test Batch"""
        # inst_req_only = self.make_instance(include_optional=False)
        # inst_req_and_optional = self.make_instance(include_optional=True)

if __name__ == '__main__':
    unittest.main()
