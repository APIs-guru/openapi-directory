# coding: utf-8

"""
    Cloud Dataproc API

    Manages Hadoop-based clusters and jobs on Google Cloud Platform.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.flink_job import FlinkJob
from openapi_client.models.hadoop_job import HadoopJob
from openapi_client.models.hive_job import HiveJob
from openapi_client.models.job_scheduling import JobScheduling
from openapi_client.models.pig_job import PigJob
from openapi_client.models.presto_job import PrestoJob
from openapi_client.models.py_spark_job import PySparkJob
from openapi_client.models.spark_job import SparkJob
from openapi_client.models.spark_r_job import SparkRJob
from openapi_client.models.spark_sql_job import SparkSqlJob
from openapi_client.models.trino_job import TrinoJob
from typing import Optional, Set
from typing_extensions import Self

class OrderedJob(BaseModel):
    """
    A job executed by the workflow.
    """ # noqa: E501
    flink_job: Optional[FlinkJob] = Field(default=None, alias="flinkJob")
    hadoop_job: Optional[HadoopJob] = Field(default=None, alias="hadoopJob")
    hive_job: Optional[HiveJob] = Field(default=None, alias="hiveJob")
    labels: Optional[Dict[str, StrictStr]] = Field(default=None, description="Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \\p{Ll}\\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \\p{Ll}\\p{Lo}\\p{N}_-{0,63}No more than 32 labels can be associated with a given job.")
    pig_job: Optional[PigJob] = Field(default=None, alias="pigJob")
    prerequisite_step_ids: Optional[List[StrictStr]] = Field(default=None, description="Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.", alias="prerequisiteStepIds")
    presto_job: Optional[PrestoJob] = Field(default=None, alias="prestoJob")
    pyspark_job: Optional[PySparkJob] = Field(default=None, alias="pysparkJob")
    scheduling: Optional[JobScheduling] = None
    spark_job: Optional[SparkJob] = Field(default=None, alias="sparkJob")
    spark_r_job: Optional[SparkRJob] = Field(default=None, alias="sparkRJob")
    spark_sql_job: Optional[SparkSqlJob] = Field(default=None, alias="sparkSqlJob")
    step_id: Optional[StrictStr] = Field(default=None, description="Required. The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.", alias="stepId")
    trino_job: Optional[TrinoJob] = Field(default=None, alias="trinoJob")
    __properties: ClassVar[List[str]] = ["flinkJob", "hadoopJob", "hiveJob", "labels", "pigJob", "prerequisiteStepIds", "prestoJob", "pysparkJob", "scheduling", "sparkJob", "sparkRJob", "sparkSqlJob", "stepId", "trinoJob"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of OrderedJob from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of flink_job
        if self.flink_job:
            _dict['flinkJob'] = self.flink_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of hadoop_job
        if self.hadoop_job:
            _dict['hadoopJob'] = self.hadoop_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of hive_job
        if self.hive_job:
            _dict['hiveJob'] = self.hive_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of pig_job
        if self.pig_job:
            _dict['pigJob'] = self.pig_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of presto_job
        if self.presto_job:
            _dict['prestoJob'] = self.presto_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of pyspark_job
        if self.pyspark_job:
            _dict['pysparkJob'] = self.pyspark_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of scheduling
        if self.scheduling:
            _dict['scheduling'] = self.scheduling.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark_job
        if self.spark_job:
            _dict['sparkJob'] = self.spark_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark_r_job
        if self.spark_r_job:
            _dict['sparkRJob'] = self.spark_r_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark_sql_job
        if self.spark_sql_job:
            _dict['sparkSqlJob'] = self.spark_sql_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of trino_job
        if self.trino_job:
            _dict['trinoJob'] = self.trino_job.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of OrderedJob from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "flinkJob": FlinkJob.from_dict(obj["flinkJob"]) if obj.get("flinkJob") is not None else None,
            "hadoopJob": HadoopJob.from_dict(obj["hadoopJob"]) if obj.get("hadoopJob") is not None else None,
            "hiveJob": HiveJob.from_dict(obj["hiveJob"]) if obj.get("hiveJob") is not None else None,
            "labels": obj.get("labels"),
            "pigJob": PigJob.from_dict(obj["pigJob"]) if obj.get("pigJob") is not None else None,
            "prerequisiteStepIds": obj.get("prerequisiteStepIds"),
            "prestoJob": PrestoJob.from_dict(obj["prestoJob"]) if obj.get("prestoJob") is not None else None,
            "pysparkJob": PySparkJob.from_dict(obj["pysparkJob"]) if obj.get("pysparkJob") is not None else None,
            "scheduling": JobScheduling.from_dict(obj["scheduling"]) if obj.get("scheduling") is not None else None,
            "sparkJob": SparkJob.from_dict(obj["sparkJob"]) if obj.get("sparkJob") is not None else None,
            "sparkRJob": SparkRJob.from_dict(obj["sparkRJob"]) if obj.get("sparkRJob") is not None else None,
            "sparkSqlJob": SparkSqlJob.from_dict(obj["sparkSqlJob"]) if obj.get("sparkSqlJob") is not None else None,
            "stepId": obj.get("stepId"),
            "trinoJob": TrinoJob.from_dict(obj["trinoJob"]) if obj.get("trinoJob") is not None else None
        })
        return _obj


