# coding: utf-8

"""
    Cloud Dataproc API

    Manages Hadoop-based clusters and jobs on Google Cloud Platform.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.driver_scheduling_config import DriverSchedulingConfig
from openapi_client.models.flink_job import FlinkJob
from openapi_client.models.hadoop_job import HadoopJob
from openapi_client.models.hive_job import HiveJob
from openapi_client.models.job_placement import JobPlacement
from openapi_client.models.job_reference import JobReference
from openapi_client.models.job_scheduling import JobScheduling
from openapi_client.models.job_status import JobStatus
from openapi_client.models.pig_job import PigJob
from openapi_client.models.presto_job import PrestoJob
from openapi_client.models.py_spark_job import PySparkJob
from openapi_client.models.spark_job import SparkJob
from openapi_client.models.spark_r_job import SparkRJob
from openapi_client.models.spark_sql_job import SparkSqlJob
from openapi_client.models.trino_job import TrinoJob
from openapi_client.models.yarn_application import YarnApplication
from typing import Optional, Set
from typing_extensions import Self

class Job(BaseModel):
    """
    A Dataproc job resource.
    """ # noqa: E501
    done: Optional[StrictBool] = Field(default=None, description="Output only. Indicates whether the job is completed. If the value is false, the job is still in progress. If true, the job is completed, and status.state field will indicate if it was successful, failed, or cancelled.")
    driver_control_files_uri: Optional[StrictStr] = Field(default=None, description="Output only. If present, the location of miscellaneous control files which can be used as part of job setup and handling. If not present, control files might be placed in the same location as driver_output_uri.", alias="driverControlFilesUri")
    driver_output_resource_uri: Optional[StrictStr] = Field(default=None, description="Output only. A URI pointing to the location of the stdout of the job's driver program.", alias="driverOutputResourceUri")
    driver_scheduling_config: Optional[DriverSchedulingConfig] = Field(default=None, alias="driverSchedulingConfig")
    flink_job: Optional[FlinkJob] = Field(default=None, alias="flinkJob")
    hadoop_job: Optional[HadoopJob] = Field(default=None, alias="hadoopJob")
    hive_job: Optional[HiveJob] = Field(default=None, alias="hiveJob")
    job_uuid: Optional[StrictStr] = Field(default=None, description="Output only. A UUID that uniquely identifies a job within the project over time. This is in contrast to a user-settable reference.job_id that might be reused over time.", alias="jobUuid")
    labels: Optional[Dict[str, StrictStr]] = Field(default=None, description="Optional. The labels to associate with this job. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values can be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a job.")
    pig_job: Optional[PigJob] = Field(default=None, alias="pigJob")
    placement: Optional[JobPlacement] = None
    presto_job: Optional[PrestoJob] = Field(default=None, alias="prestoJob")
    pyspark_job: Optional[PySparkJob] = Field(default=None, alias="pysparkJob")
    reference: Optional[JobReference] = None
    scheduling: Optional[JobScheduling] = None
    spark_job: Optional[SparkJob] = Field(default=None, alias="sparkJob")
    spark_r_job: Optional[SparkRJob] = Field(default=None, alias="sparkRJob")
    spark_sql_job: Optional[SparkSqlJob] = Field(default=None, alias="sparkSqlJob")
    status: Optional[JobStatus] = None
    status_history: Optional[List[JobStatus]] = Field(default=None, description="Output only. The previous job status.", alias="statusHistory")
    trino_job: Optional[TrinoJob] = Field(default=None, alias="trinoJob")
    yarn_applications: Optional[List[YarnApplication]] = Field(default=None, description="Output only. The collection of YARN applications spun up by this job.Beta Feature: This report is available for testing purposes only. It might be changed before final release.", alias="yarnApplications")
    __properties: ClassVar[List[str]] = ["done", "driverControlFilesUri", "driverOutputResourceUri", "driverSchedulingConfig", "flinkJob", "hadoopJob", "hiveJob", "jobUuid", "labels", "pigJob", "placement", "prestoJob", "pysparkJob", "reference", "scheduling", "sparkJob", "sparkRJob", "sparkSqlJob", "status", "statusHistory", "trinoJob", "yarnApplications"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of Job from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "done",
            "driver_control_files_uri",
            "driver_output_resource_uri",
            "job_uuid",
            "status_history",
            "yarn_applications",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of driver_scheduling_config
        if self.driver_scheduling_config:
            _dict['driverSchedulingConfig'] = self.driver_scheduling_config.to_dict()
        # override the default output from pydantic by calling `to_dict()` of flink_job
        if self.flink_job:
            _dict['flinkJob'] = self.flink_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of hadoop_job
        if self.hadoop_job:
            _dict['hadoopJob'] = self.hadoop_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of hive_job
        if self.hive_job:
            _dict['hiveJob'] = self.hive_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of pig_job
        if self.pig_job:
            _dict['pigJob'] = self.pig_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of placement
        if self.placement:
            _dict['placement'] = self.placement.to_dict()
        # override the default output from pydantic by calling `to_dict()` of presto_job
        if self.presto_job:
            _dict['prestoJob'] = self.presto_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of pyspark_job
        if self.pyspark_job:
            _dict['pysparkJob'] = self.pyspark_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of reference
        if self.reference:
            _dict['reference'] = self.reference.to_dict()
        # override the default output from pydantic by calling `to_dict()` of scheduling
        if self.scheduling:
            _dict['scheduling'] = self.scheduling.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark_job
        if self.spark_job:
            _dict['sparkJob'] = self.spark_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark_r_job
        if self.spark_r_job:
            _dict['sparkRJob'] = self.spark_r_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark_sql_job
        if self.spark_sql_job:
            _dict['sparkSqlJob'] = self.spark_sql_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of status
        if self.status:
            _dict['status'] = self.status.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in status_history (list)
        _items = []
        if self.status_history:
            for _item_status_history in self.status_history:
                if _item_status_history:
                    _items.append(_item_status_history.to_dict())
            _dict['statusHistory'] = _items
        # override the default output from pydantic by calling `to_dict()` of trino_job
        if self.trino_job:
            _dict['trinoJob'] = self.trino_job.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in yarn_applications (list)
        _items = []
        if self.yarn_applications:
            for _item_yarn_applications in self.yarn_applications:
                if _item_yarn_applications:
                    _items.append(_item_yarn_applications.to_dict())
            _dict['yarnApplications'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of Job from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "done": obj.get("done"),
            "driverControlFilesUri": obj.get("driverControlFilesUri"),
            "driverOutputResourceUri": obj.get("driverOutputResourceUri"),
            "driverSchedulingConfig": DriverSchedulingConfig.from_dict(obj["driverSchedulingConfig"]) if obj.get("driverSchedulingConfig") is not None else None,
            "flinkJob": FlinkJob.from_dict(obj["flinkJob"]) if obj.get("flinkJob") is not None else None,
            "hadoopJob": HadoopJob.from_dict(obj["hadoopJob"]) if obj.get("hadoopJob") is not None else None,
            "hiveJob": HiveJob.from_dict(obj["hiveJob"]) if obj.get("hiveJob") is not None else None,
            "jobUuid": obj.get("jobUuid"),
            "labels": obj.get("labels"),
            "pigJob": PigJob.from_dict(obj["pigJob"]) if obj.get("pigJob") is not None else None,
            "placement": JobPlacement.from_dict(obj["placement"]) if obj.get("placement") is not None else None,
            "prestoJob": PrestoJob.from_dict(obj["prestoJob"]) if obj.get("prestoJob") is not None else None,
            "pysparkJob": PySparkJob.from_dict(obj["pysparkJob"]) if obj.get("pysparkJob") is not None else None,
            "reference": JobReference.from_dict(obj["reference"]) if obj.get("reference") is not None else None,
            "scheduling": JobScheduling.from_dict(obj["scheduling"]) if obj.get("scheduling") is not None else None,
            "sparkJob": SparkJob.from_dict(obj["sparkJob"]) if obj.get("sparkJob") is not None else None,
            "sparkRJob": SparkRJob.from_dict(obj["sparkRJob"]) if obj.get("sparkRJob") is not None else None,
            "sparkSqlJob": SparkSqlJob.from_dict(obj["sparkSqlJob"]) if obj.get("sparkSqlJob") is not None else None,
            "status": JobStatus.from_dict(obj["status"]) if obj.get("status") is not None else None,
            "statusHistory": [JobStatus.from_dict(_item) for _item in obj["statusHistory"]] if obj.get("statusHistory") is not None else None,
            "trinoJob": TrinoJob.from_dict(obj["trinoJob"]) if obj.get("trinoJob") is not None else None,
            "yarnApplications": [YarnApplication.from_dict(_item) for _item in obj["yarnApplications"]] if obj.get("yarnApplications") is not None else None
        })
        return _obj


