# coding: utf-8

"""
    Cloud Dataproc API

    Manages Hadoop-based clusters and jobs on Google Cloud Platform.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from typing import Optional, Set
from typing_extensions import Self

class ExecutionConfig(BaseModel):
    """
    Execution configuration for a workload.
    """ # noqa: E501
    idle_ttl: Optional[StrictStr] = Field(default=None, description="Optional. Applies to sessions only. The duration to keep the session alive while it's idling. Exceeding this threshold causes the session to terminate. This field cannot be set on a batch workload. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 1 hour if not set. If both ttl and idle_ttl are specified for an interactive session, the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.", alias="idleTtl")
    kms_key: Optional[StrictStr] = Field(default=None, description="Optional. The Cloud KMS key to use for encryption.", alias="kmsKey")
    network_tags: Optional[List[StrictStr]] = Field(default=None, description="Optional. Tags used for network traffic control.", alias="networkTags")
    network_uri: Optional[StrictStr] = Field(default=None, description="Optional. Network URI to connect workload to.", alias="networkUri")
    service_account: Optional[StrictStr] = Field(default=None, description="Optional. Service account that used to execute workload.", alias="serviceAccount")
    staging_bucket: Optional[StrictStr] = Field(default=None, description="Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.", alias="stagingBucket")
    subnetwork_uri: Optional[StrictStr] = Field(default=None, description="Optional. Subnetwork URI to connect workload to.", alias="subnetworkUri")
    ttl: Optional[StrictStr] = Field(default=None, description="Optional. The duration after which the workload will be terminated, specified as the JSON representation for Duration (https://protobuf.dev/programming-guides/proto3/#json). When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing work to finish. If ttl is not specified for a batch workload, the workload will be allowed to run until it exits naturally (or run forever without exiting). If ttl is not specified for an interactive session, it defaults to 24 hours. If ttl is not specified for a batch that uses 2.1+ runtime version, it defaults to 4 hours. Minimum value is 10 minutes; maximum value is 14 days. If both ttl and idle_ttl are specified (for an interactive session), the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.")
    __properties: ClassVar[List[str]] = ["idleTtl", "kmsKey", "networkTags", "networkUri", "serviceAccount", "stagingBucket", "subnetworkUri", "ttl"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of ExecutionConfig from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of ExecutionConfig from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "idleTtl": obj.get("idleTtl"),
            "kmsKey": obj.get("kmsKey"),
            "networkTags": obj.get("networkTags"),
            "networkUri": obj.get("networkUri"),
            "serviceAccount": obj.get("serviceAccount"),
            "stagingBucket": obj.get("stagingBucket"),
            "subnetworkUri": obj.get("subnetworkUri"),
            "ttl": obj.get("ttl")
        })
        return _obj


