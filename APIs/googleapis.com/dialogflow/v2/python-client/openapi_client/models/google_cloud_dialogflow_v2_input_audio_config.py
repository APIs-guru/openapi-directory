# coding: utf-8

"""
    Dialogflow API

    Builds conversational interfaces (for example, chatbots, and voice-powered apps and devices).

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_cloud_dialogflow_v2_speech_context import GoogleCloudDialogflowV2SpeechContext
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudDialogflowV2InputAudioConfig(BaseModel):
    """
    Instructs the speech recognizer how to process the audio content.
    """ # noqa: E501
    audio_encoding: Optional[StrictStr] = Field(default=None, description="Required. Audio encoding of the audio content to process.", alias="audioEncoding")
    disable_no_speech_recognized_event: Optional[StrictBool] = Field(default=None, description="Only used in Participants.AnalyzeContent and Participants.StreamingAnalyzeContent. If `false` and recognition doesn't return any result, trigger `NO_SPEECH_RECOGNIZED` event to Dialogflow agent.", alias="disableNoSpeechRecognizedEvent")
    enable_automatic_punctuation: Optional[StrictBool] = Field(default=None, description="Enable automatic punctuation option at the speech backend.", alias="enableAutomaticPunctuation")
    enable_word_info: Optional[StrictBool] = Field(default=None, description="If `true`, Dialogflow returns SpeechWordInfo in StreamingRecognitionResult with information about the recognized speech words, e.g. start and end time offsets. If false or unspecified, Speech doesn't return any word-level information.", alias="enableWordInfo")
    language_code: Optional[StrictStr] = Field(default=None, description="Required. The language of the supplied audio. Dialogflow does not do translations. See [Language Support](https://cloud.google.com/dialogflow/docs/reference/language) for a list of the currently supported language codes. Note that queries in the same session do not necessarily need to specify the same language.", alias="languageCode")
    model: Optional[StrictStr] = Field(default=None, description="Optional. Which Speech model to select for the given request. For more information, see [Speech models](https://cloud.google.com/dialogflow/es/docs/speech-models).")
    model_variant: Optional[StrictStr] = Field(default=None, description="Which variant of the Speech model to use.", alias="modelVariant")
    opt_out_conformer_model_migration: Optional[StrictBool] = Field(default=None, description="If `true`, the request will opt out for STT conformer model migration. This field will be deprecated once force migration takes place in June 2024. Please refer to [Dialogflow ES Speech model migration](https://cloud.google.com/dialogflow/es/docs/speech-model-migration).", alias="optOutConformerModelMigration")
    phrase_hints: Optional[List[StrictStr]] = Field(default=None, description="A list of strings containing words and phrases that the speech recognizer should recognize with higher likelihood. See [the Cloud Speech documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints) for more details. This field is deprecated. Please use [`speech_contexts`]() instead. If you specify both [`phrase_hints`]() and [`speech_contexts`](), Dialogflow will treat the [`phrase_hints`]() as a single additional [`SpeechContext`]().", alias="phraseHints")
    sample_rate_hertz: Optional[StrictInt] = Field(default=None, description="Required. Sample rate (in Hertz) of the audio content sent in the query. Refer to [Cloud Speech API documentation](https://cloud.google.com/speech-to-text/docs/basics) for more details.", alias="sampleRateHertz")
    single_utterance: Optional[StrictBool] = Field(default=None, description="If `false` (default), recognition does not cease until the client closes the stream. If `true`, the recognizer will detect a single spoken utterance in input audio. Recognition ceases when it detects the audio's voice has stopped or paused. In this case, once a detected intent is received, the client should close the stream and start a new request with a new stream as needed. Note: This setting is relevant only for streaming methods. Note: When specified, InputAudioConfig.single_utterance takes precedence over StreamingDetectIntentRequest.single_utterance.", alias="singleUtterance")
    speech_contexts: Optional[List[GoogleCloudDialogflowV2SpeechContext]] = Field(default=None, description="Context information to assist speech recognition. See [the Cloud Speech documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints) for more details.", alias="speechContexts")
    __properties: ClassVar[List[str]] = ["audioEncoding", "disableNoSpeechRecognizedEvent", "enableAutomaticPunctuation", "enableWordInfo", "languageCode", "model", "modelVariant", "optOutConformerModelMigration", "phraseHints", "sampleRateHertz", "singleUtterance", "speechContexts"]

    @field_validator('audio_encoding')
    def audio_encoding_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['AUDIO_ENCODING_UNSPECIFIED', 'AUDIO_ENCODING_LINEAR_16', 'AUDIO_ENCODING_FLAC', 'AUDIO_ENCODING_MULAW', 'AUDIO_ENCODING_AMR', 'AUDIO_ENCODING_AMR_WB', 'AUDIO_ENCODING_OGG_OPUS', 'AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE']):
            raise ValueError("must be one of enum values ('AUDIO_ENCODING_UNSPECIFIED', 'AUDIO_ENCODING_LINEAR_16', 'AUDIO_ENCODING_FLAC', 'AUDIO_ENCODING_MULAW', 'AUDIO_ENCODING_AMR', 'AUDIO_ENCODING_AMR_WB', 'AUDIO_ENCODING_OGG_OPUS', 'AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE')")
        return value

    @field_validator('model_variant')
    def model_variant_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['SPEECH_MODEL_VARIANT_UNSPECIFIED', 'USE_BEST_AVAILABLE', 'USE_STANDARD', 'USE_ENHANCED']):
            raise ValueError("must be one of enum values ('SPEECH_MODEL_VARIANT_UNSPECIFIED', 'USE_BEST_AVAILABLE', 'USE_STANDARD', 'USE_ENHANCED')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudDialogflowV2InputAudioConfig from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in speech_contexts (list)
        _items = []
        if self.speech_contexts:
            for _item_speech_contexts in self.speech_contexts:
                if _item_speech_contexts:
                    _items.append(_item_speech_contexts.to_dict())
            _dict['speechContexts'] = _items
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudDialogflowV2InputAudioConfig from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "audioEncoding": obj.get("audioEncoding"),
            "disableNoSpeechRecognizedEvent": obj.get("disableNoSpeechRecognizedEvent"),
            "enableAutomaticPunctuation": obj.get("enableAutomaticPunctuation"),
            "enableWordInfo": obj.get("enableWordInfo"),
            "languageCode": obj.get("languageCode"),
            "model": obj.get("model"),
            "modelVariant": obj.get("modelVariant"),
            "optOutConformerModelMigration": obj.get("optOutConformerModelMigration"),
            "phraseHints": obj.get("phraseHints"),
            "sampleRateHertz": obj.get("sampleRateHertz"),
            "singleUtterance": obj.get("singleUtterance"),
            "speechContexts": [GoogleCloudDialogflowV2SpeechContext.from_dict(_item) for _item in obj["speechContexts"]] if obj.get("speechContexts") is not None else None
        })
        return _obj


