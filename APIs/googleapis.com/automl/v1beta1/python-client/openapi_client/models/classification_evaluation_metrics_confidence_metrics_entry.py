# coding: utf-8

"""
    Cloud AutoML API

    Train high-quality custom machine learning models with minimum effort and machine learning expertise.

    The version of the OpenAPI document: v1beta1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictFloat, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional, Union
from typing import Optional, Set
from typing_extensions import Self

class ClassificationEvaluationMetricsConfidenceMetricsEntry(BaseModel):
    """
    Metrics for a single confidence threshold.
    """ # noqa: E501
    confidence_threshold: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Output only. Metrics are computed with an assumption that the model never returns predictions with score lower than this value.", alias="confidenceThreshold")
    f1_score: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Output only. The harmonic mean of recall and precision.", alias="f1Score")
    f1_score_at1: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Output only. The harmonic mean of recall_at1 and precision_at1.", alias="f1ScoreAt1")
    false_negative_count: Optional[StrictStr] = Field(default=None, description="Output only. The number of ground truth labels that are not matched by a model created label.", alias="falseNegativeCount")
    false_positive_count: Optional[StrictStr] = Field(default=None, description="Output only. The number of model created labels that do not match a ground truth label.", alias="falsePositiveCount")
    false_positive_rate: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Output only. False Positive Rate for the given confidence threshold.", alias="falsePositiveRate")
    false_positive_rate_at1: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Output only. The False Positive Rate when only considering the label that has the highest prediction score and not below the confidence threshold for each example.", alias="falsePositiveRateAt1")
    position_threshold: Optional[StrictInt] = Field(default=None, description="Output only. Metrics are computed with an assumption that the model always returns at most this many predictions (ordered by their score, descendingly), but they all still need to meet the confidence_threshold.", alias="positionThreshold")
    precision: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Output only. Precision for the given confidence threshold.")
    precision_at1: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Output only. The precision when only considering the label that has the highest prediction score and not below the confidence threshold for each example.", alias="precisionAt1")
    recall: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Output only. Recall (True Positive Rate) for the given confidence threshold.")
    recall_at1: Optional[Union[StrictFloat, StrictInt]] = Field(default=None, description="Output only. The Recall (True Positive Rate) when only considering the label that has the highest prediction score and not below the confidence threshold for each example.", alias="recallAt1")
    true_negative_count: Optional[StrictStr] = Field(default=None, description="Output only. The number of labels that were not created by the model, but if they would, they would not match a ground truth label.", alias="trueNegativeCount")
    true_positive_count: Optional[StrictStr] = Field(default=None, description="Output only. The number of model created labels that match a ground truth label.", alias="truePositiveCount")
    __properties: ClassVar[List[str]] = ["confidenceThreshold", "f1Score", "f1ScoreAt1", "falseNegativeCount", "falsePositiveCount", "falsePositiveRate", "falsePositiveRateAt1", "positionThreshold", "precision", "precisionAt1", "recall", "recallAt1", "trueNegativeCount", "truePositiveCount"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of ClassificationEvaluationMetricsConfidenceMetricsEntry from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of ClassificationEvaluationMetricsConfidenceMetricsEntry from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "confidenceThreshold": obj.get("confidenceThreshold"),
            "f1Score": obj.get("f1Score"),
            "f1ScoreAt1": obj.get("f1ScoreAt1"),
            "falseNegativeCount": obj.get("falseNegativeCount"),
            "falsePositiveCount": obj.get("falsePositiveCount"),
            "falsePositiveRate": obj.get("falsePositiveRate"),
            "falsePositiveRateAt1": obj.get("falsePositiveRateAt1"),
            "positionThreshold": obj.get("positionThreshold"),
            "precision": obj.get("precision"),
            "precisionAt1": obj.get("precisionAt1"),
            "recall": obj.get("recall"),
            "recallAt1": obj.get("recallAt1"),
            "trueNegativeCount": obj.get("trueNegativeCount"),
            "truePositiveCount": obj.get("truePositiveCount")
        })
        return _obj


