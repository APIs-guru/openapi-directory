# coding: utf-8

"""
    Cloud AutoML API

    Train high-quality custom machine learning models with minimum effort and machine learning expertise.

    The version of the OpenAPI document: v1beta1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.classification_evaluation_metrics import ClassificationEvaluationMetrics
from openapi_client.models.image_object_detection_evaluation_metrics import ImageObjectDetectionEvaluationMetrics
from openapi_client.models.regression_evaluation_metrics import RegressionEvaluationMetrics
from openapi_client.models.text_extraction_evaluation_metrics import TextExtractionEvaluationMetrics
from openapi_client.models.text_sentiment_evaluation_metrics import TextSentimentEvaluationMetrics
from openapi_client.models.translation_evaluation_metrics import TranslationEvaluationMetrics
from openapi_client.models.video_object_tracking_evaluation_metrics import VideoObjectTrackingEvaluationMetrics
from typing import Optional, Set
from typing_extensions import Self

class ModelEvaluation(BaseModel):
    """
    Evaluation results of a model.
    """ # noqa: E501
    annotation_spec_id: Optional[StrictStr] = Field(default=None, description="Output only. The ID of the annotation spec that the model evaluation applies to. The The ID is empty for the overall model evaluation. For Tables annotation specs in the dataset do not exist and this ID is always not set, but for CLASSIFICATION prediction_type-s the display_name field is used.", alias="annotationSpecId")
    classification_evaluation_metrics: Optional[ClassificationEvaluationMetrics] = Field(default=None, alias="classificationEvaluationMetrics")
    create_time: Optional[StrictStr] = Field(default=None, description="Output only. Timestamp when this model evaluation was created.", alias="createTime")
    display_name: Optional[StrictStr] = Field(default=None, description="Output only. The value of display_name at the moment when the model was trained. Because this field returns a value at model training time, for different models trained from the same dataset, the values may differ, since display names could had been changed between the two model's trainings. For Tables CLASSIFICATION prediction_type-s distinct values of the target column at the moment of the model evaluation are populated here. The display_name is empty for the overall model evaluation.", alias="displayName")
    evaluated_example_count: Optional[StrictInt] = Field(default=None, description="Output only. The number of examples used for model evaluation, i.e. for which ground truth from time of model creation is compared against the predicted annotations created by the model. For overall ModelEvaluation (i.e. with annotation_spec_id not set) this is the total number of all examples used for evaluation. Otherwise, this is the count of examples that according to the ground truth were annotated by the annotation_spec_id.", alias="evaluatedExampleCount")
    image_object_detection_evaluation_metrics: Optional[ImageObjectDetectionEvaluationMetrics] = Field(default=None, alias="imageObjectDetectionEvaluationMetrics")
    name: Optional[StrictStr] = Field(default=None, description="Output only. Resource name of the model evaluation. Format: `projects/{project_id}/locations/{location_id}/models/{model_id}/modelEvaluations/{model_evaluation_id}`")
    regression_evaluation_metrics: Optional[RegressionEvaluationMetrics] = Field(default=None, alias="regressionEvaluationMetrics")
    text_extraction_evaluation_metrics: Optional[TextExtractionEvaluationMetrics] = Field(default=None, alias="textExtractionEvaluationMetrics")
    text_sentiment_evaluation_metrics: Optional[TextSentimentEvaluationMetrics] = Field(default=None, alias="textSentimentEvaluationMetrics")
    translation_evaluation_metrics: Optional[TranslationEvaluationMetrics] = Field(default=None, alias="translationEvaluationMetrics")
    video_object_tracking_evaluation_metrics: Optional[VideoObjectTrackingEvaluationMetrics] = Field(default=None, alias="videoObjectTrackingEvaluationMetrics")
    __properties: ClassVar[List[str]] = ["annotationSpecId", "classificationEvaluationMetrics", "createTime", "displayName", "evaluatedExampleCount", "imageObjectDetectionEvaluationMetrics", "name", "regressionEvaluationMetrics", "textExtractionEvaluationMetrics", "textSentimentEvaluationMetrics", "translationEvaluationMetrics", "videoObjectTrackingEvaluationMetrics"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of ModelEvaluation from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of classification_evaluation_metrics
        if self.classification_evaluation_metrics:
            _dict['classificationEvaluationMetrics'] = self.classification_evaluation_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of image_object_detection_evaluation_metrics
        if self.image_object_detection_evaluation_metrics:
            _dict['imageObjectDetectionEvaluationMetrics'] = self.image_object_detection_evaluation_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of regression_evaluation_metrics
        if self.regression_evaluation_metrics:
            _dict['regressionEvaluationMetrics'] = self.regression_evaluation_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of text_extraction_evaluation_metrics
        if self.text_extraction_evaluation_metrics:
            _dict['textExtractionEvaluationMetrics'] = self.text_extraction_evaluation_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of text_sentiment_evaluation_metrics
        if self.text_sentiment_evaluation_metrics:
            _dict['textSentimentEvaluationMetrics'] = self.text_sentiment_evaluation_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of translation_evaluation_metrics
        if self.translation_evaluation_metrics:
            _dict['translationEvaluationMetrics'] = self.translation_evaluation_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of video_object_tracking_evaluation_metrics
        if self.video_object_tracking_evaluation_metrics:
            _dict['videoObjectTrackingEvaluationMetrics'] = self.video_object_tracking_evaluation_metrics.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of ModelEvaluation from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "annotationSpecId": obj.get("annotationSpecId"),
            "classificationEvaluationMetrics": ClassificationEvaluationMetrics.from_dict(obj["classificationEvaluationMetrics"]) if obj.get("classificationEvaluationMetrics") is not None else None,
            "createTime": obj.get("createTime"),
            "displayName": obj.get("displayName"),
            "evaluatedExampleCount": obj.get("evaluatedExampleCount"),
            "imageObjectDetectionEvaluationMetrics": ImageObjectDetectionEvaluationMetrics.from_dict(obj["imageObjectDetectionEvaluationMetrics"]) if obj.get("imageObjectDetectionEvaluationMetrics") is not None else None,
            "name": obj.get("name"),
            "regressionEvaluationMetrics": RegressionEvaluationMetrics.from_dict(obj["regressionEvaluationMetrics"]) if obj.get("regressionEvaluationMetrics") is not None else None,
            "textExtractionEvaluationMetrics": TextExtractionEvaluationMetrics.from_dict(obj["textExtractionEvaluationMetrics"]) if obj.get("textExtractionEvaluationMetrics") is not None else None,
            "textSentimentEvaluationMetrics": TextSentimentEvaluationMetrics.from_dict(obj["textSentimentEvaluationMetrics"]) if obj.get("textSentimentEvaluationMetrics") is not None else None,
            "translationEvaluationMetrics": TranslationEvaluationMetrics.from_dict(obj["translationEvaluationMetrics"]) if obj.get("translationEvaluationMetrics") is not None else None,
            "videoObjectTrackingEvaluationMetrics": VideoObjectTrackingEvaluationMetrics.from_dict(obj["videoObjectTrackingEvaluationMetrics"]) if obj.get("videoObjectTrackingEvaluationMetrics") is not None else None
        })
        return _obj


