# coding: utf-8

"""
    Data pipelines API

    Data Pipelines provides an interface for creating, updating, and managing recurring Data Analytics jobs.

    The version of the OpenAPI document: v1
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.google_cloud_datapipelines_v1_schedule_spec import GoogleCloudDatapipelinesV1ScheduleSpec
from openapi_client.models.google_cloud_datapipelines_v1_workload import GoogleCloudDatapipelinesV1Workload
from typing import Optional, Set
from typing_extensions import Self

class GoogleCloudDatapipelinesV1Pipeline(BaseModel):
    """
    The main pipeline entity and all the necessary metadata for launching and managing linked jobs.
    """ # noqa: E501
    create_time: Optional[StrictStr] = Field(default=None, description="Output only. Immutable. The timestamp when the pipeline was initially created. Set by the Data Pipelines service.", alias="createTime")
    display_name: Optional[StrictStr] = Field(default=None, description="Required. The display name of the pipeline. It can contain only letters ([A-Za-z]), numbers ([0-9]), hyphens (-), and underscores (_).", alias="displayName")
    job_count: Optional[StrictInt] = Field(default=None, description="Output only. Number of jobs.", alias="jobCount")
    last_update_time: Optional[StrictStr] = Field(default=None, description="Output only. Immutable. The timestamp when the pipeline was last modified. Set by the Data Pipelines service.", alias="lastUpdateTime")
    name: Optional[StrictStr] = Field(default=None, description="The pipeline name. For example: `projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID`. * `PROJECT_ID` can contain letters ([A-Za-z]), numbers ([0-9]), hyphens (-), colons (:), and periods (.). For more information, see [Identifying projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects). * `LOCATION_ID` is the canonical ID for the pipeline's location. The list of available locations can be obtained by calling `google.cloud.location.Locations.ListLocations`. Note that the Data Pipelines service is not available in all regions. It depends on Cloud Scheduler, an App Engine application, so it's only available in [App Engine regions](https://cloud.google.com/about/locations#region). * `PIPELINE_ID` is the ID of the pipeline. Must be unique for the selected project and location.")
    pipeline_sources: Optional[Dict[str, StrictStr]] = Field(default=None, description="Immutable. The sources of the pipeline (for example, Dataplex). The keys and values are set by the corresponding sources during pipeline creation.", alias="pipelineSources")
    schedule_info: Optional[GoogleCloudDatapipelinesV1ScheduleSpec] = Field(default=None, alias="scheduleInfo")
    scheduler_service_account_email: Optional[StrictStr] = Field(default=None, description="Optional. A service account email to be used with the Cloud Scheduler job. If not specified, the default compute engine service account will be used.", alias="schedulerServiceAccountEmail")
    state: Optional[StrictStr] = Field(default=None, description="Required. The state of the pipeline. When the pipeline is created, the state is set to 'PIPELINE_STATE_ACTIVE' by default. State changes can be requested by setting the state to stopping, paused, or resuming. State cannot be changed through UpdatePipeline requests.")
    type: Optional[StrictStr] = Field(default=None, description="Required. The type of the pipeline. This field affects the scheduling of the pipeline and the type of metrics to show for the pipeline.")
    workload: Optional[GoogleCloudDatapipelinesV1Workload] = None
    __properties: ClassVar[List[str]] = ["createTime", "displayName", "jobCount", "lastUpdateTime", "name", "pipelineSources", "scheduleInfo", "schedulerServiceAccountEmail", "state", "type", "workload"]

    @field_validator('state')
    def state_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['STATE_UNSPECIFIED', 'STATE_RESUMING', 'STATE_ACTIVE', 'STATE_STOPPING', 'STATE_ARCHIVED', 'STATE_PAUSED']):
            raise ValueError("must be one of enum values ('STATE_UNSPECIFIED', 'STATE_RESUMING', 'STATE_ACTIVE', 'STATE_STOPPING', 'STATE_ARCHIVED', 'STATE_PAUSED')")
        return value

    @field_validator('type')
    def type_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['PIPELINE_TYPE_UNSPECIFIED', 'PIPELINE_TYPE_BATCH', 'PIPELINE_TYPE_STREAMING']):
            raise ValueError("must be one of enum values ('PIPELINE_TYPE_UNSPECIFIED', 'PIPELINE_TYPE_BATCH', 'PIPELINE_TYPE_STREAMING')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of GoogleCloudDatapipelinesV1Pipeline from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "create_time",
            "job_count",
            "last_update_time",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of schedule_info
        if self.schedule_info:
            _dict['scheduleInfo'] = self.schedule_info.to_dict()
        # override the default output from pydantic by calling `to_dict()` of workload
        if self.workload:
            _dict['workload'] = self.workload.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of GoogleCloudDatapipelinesV1Pipeline from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "createTime": obj.get("createTime"),
            "displayName": obj.get("displayName"),
            "jobCount": obj.get("jobCount"),
            "lastUpdateTime": obj.get("lastUpdateTime"),
            "name": obj.get("name"),
            "pipelineSources": obj.get("pipelineSources"),
            "scheduleInfo": GoogleCloudDatapipelinesV1ScheduleSpec.from_dict(obj["scheduleInfo"]) if obj.get("scheduleInfo") is not None else None,
            "schedulerServiceAccountEmail": obj.get("schedulerServiceAccountEmail"),
            "state": obj.get("state"),
            "type": obj.get("type"),
            "workload": GoogleCloudDatapipelinesV1Workload.from_dict(obj["workload"]) if obj.get("workload") is not None else None
        })
        return _obj


