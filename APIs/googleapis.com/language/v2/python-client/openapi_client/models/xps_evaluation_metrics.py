# coding: utf-8

"""
    Cloud Natural Language API

    Provides natural language understanding technologies, such as sentiment analysis, entity recognition, entity sentiment analysis, and other text annotations, to developers.

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.xps_classification_evaluation_metrics import XPSClassificationEvaluationMetrics
from openapi_client.models.xps_image_object_detection_evaluation_metrics import XPSImageObjectDetectionEvaluationMetrics
from openapi_client.models.xps_image_segmentation_evaluation_metrics import XPSImageSegmentationEvaluationMetrics
from openapi_client.models.xps_regression_evaluation_metrics import XPSRegressionEvaluationMetrics
from openapi_client.models.xps_tables_evaluation_metrics import XPSTablesEvaluationMetrics
from openapi_client.models.xps_text_extraction_evaluation_metrics import XPSTextExtractionEvaluationMetrics
from openapi_client.models.xps_text_sentiment_evaluation_metrics import XPSTextSentimentEvaluationMetrics
from openapi_client.models.xps_translation_evaluation_metrics import XPSTranslationEvaluationMetrics
from openapi_client.models.xps_video_action_recognition_evaluation_metrics import XPSVideoActionRecognitionEvaluationMetrics
from openapi_client.models.xps_video_object_tracking_evaluation_metrics import XPSVideoObjectTrackingEvaluationMetrics
from typing import Optional, Set
from typing_extensions import Self

class XPSEvaluationMetrics(BaseModel):
    """
    Contains xPS-specific model evaluation metrics either for a single annotation spec (label), or for the model overall. Next tag: 18.
    """ # noqa: E501
    annotation_spec_id_token: Optional[StrictStr] = Field(default=None, description="The annotation_spec for which this evaluation metrics instance had been created. Empty iff this is an overall model evaluation (like Tables evaluation metrics), i.e. aggregated across all labels. The value comes from the input annotations in AnnotatedExample. For MVP product or for text sentiment models where annotation_spec_id_token is not available, set label instead.", alias="annotationSpecIdToken")
    category: Optional[StrictInt] = Field(default=None, description="The integer category label for which this evaluation metric instance had been created. Valid categories are 0 or higher. Overall model evaluation should set this to negative values (rather than implicit zero). Only used for Image Segmentation (prefer to set annotation_spec_id_token instead). Note: uCAIP Image Segmentation should use annotation_spec_id_token.")
    evaluated_example_count: Optional[StrictInt] = Field(default=None, description="The number of examples used to create this evaluation metrics instance.", alias="evaluatedExampleCount")
    image_classification_eval_metrics: Optional[XPSClassificationEvaluationMetrics] = Field(default=None, alias="imageClassificationEvalMetrics")
    image_object_detection_eval_metrics: Optional[XPSImageObjectDetectionEvaluationMetrics] = Field(default=None, alias="imageObjectDetectionEvalMetrics")
    image_segmentation_eval_metrics: Optional[XPSImageSegmentationEvaluationMetrics] = Field(default=None, alias="imageSegmentationEvalMetrics")
    label: Optional[StrictStr] = Field(default=None, description="The label for which this evaluation metrics instance had been created. Empty iff this is an overall model evaluation (like Tables evaluation metrics), i.e. aggregated across all labels. The label maps to AnnotationSpec.display_name in Public API protos. Only used by MVP implementation and text sentiment FULL implementation.")
    regression_eval_metrics: Optional[XPSRegressionEvaluationMetrics] = Field(default=None, alias="regressionEvalMetrics")
    tables_classification_eval_metrics: Optional[XPSClassificationEvaluationMetrics] = Field(default=None, alias="tablesClassificationEvalMetrics")
    tables_eval_metrics: Optional[XPSTablesEvaluationMetrics] = Field(default=None, alias="tablesEvalMetrics")
    text_classification_eval_metrics: Optional[XPSClassificationEvaluationMetrics] = Field(default=None, alias="textClassificationEvalMetrics")
    text_extraction_eval_metrics: Optional[XPSTextExtractionEvaluationMetrics] = Field(default=None, alias="textExtractionEvalMetrics")
    text_sentiment_eval_metrics: Optional[XPSTextSentimentEvaluationMetrics] = Field(default=None, alias="textSentimentEvalMetrics")
    translation_eval_metrics: Optional[XPSTranslationEvaluationMetrics] = Field(default=None, alias="translationEvalMetrics")
    video_action_recognition_eval_metrics: Optional[XPSVideoActionRecognitionEvaluationMetrics] = Field(default=None, alias="videoActionRecognitionEvalMetrics")
    video_classification_eval_metrics: Optional[XPSClassificationEvaluationMetrics] = Field(default=None, alias="videoClassificationEvalMetrics")
    video_object_tracking_eval_metrics: Optional[XPSVideoObjectTrackingEvaluationMetrics] = Field(default=None, alias="videoObjectTrackingEvalMetrics")
    __properties: ClassVar[List[str]] = ["annotationSpecIdToken", "category", "evaluatedExampleCount", "imageClassificationEvalMetrics", "imageObjectDetectionEvalMetrics", "imageSegmentationEvalMetrics", "label", "regressionEvalMetrics", "tablesClassificationEvalMetrics", "tablesEvalMetrics", "textClassificationEvalMetrics", "textExtractionEvalMetrics", "textSentimentEvalMetrics", "translationEvalMetrics", "videoActionRecognitionEvalMetrics", "videoClassificationEvalMetrics", "videoObjectTrackingEvalMetrics"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of XPSEvaluationMetrics from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of image_classification_eval_metrics
        if self.image_classification_eval_metrics:
            _dict['imageClassificationEvalMetrics'] = self.image_classification_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of image_object_detection_eval_metrics
        if self.image_object_detection_eval_metrics:
            _dict['imageObjectDetectionEvalMetrics'] = self.image_object_detection_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of image_segmentation_eval_metrics
        if self.image_segmentation_eval_metrics:
            _dict['imageSegmentationEvalMetrics'] = self.image_segmentation_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of regression_eval_metrics
        if self.regression_eval_metrics:
            _dict['regressionEvalMetrics'] = self.regression_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of tables_classification_eval_metrics
        if self.tables_classification_eval_metrics:
            _dict['tablesClassificationEvalMetrics'] = self.tables_classification_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of tables_eval_metrics
        if self.tables_eval_metrics:
            _dict['tablesEvalMetrics'] = self.tables_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of text_classification_eval_metrics
        if self.text_classification_eval_metrics:
            _dict['textClassificationEvalMetrics'] = self.text_classification_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of text_extraction_eval_metrics
        if self.text_extraction_eval_metrics:
            _dict['textExtractionEvalMetrics'] = self.text_extraction_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of text_sentiment_eval_metrics
        if self.text_sentiment_eval_metrics:
            _dict['textSentimentEvalMetrics'] = self.text_sentiment_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of translation_eval_metrics
        if self.translation_eval_metrics:
            _dict['translationEvalMetrics'] = self.translation_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of video_action_recognition_eval_metrics
        if self.video_action_recognition_eval_metrics:
            _dict['videoActionRecognitionEvalMetrics'] = self.video_action_recognition_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of video_classification_eval_metrics
        if self.video_classification_eval_metrics:
            _dict['videoClassificationEvalMetrics'] = self.video_classification_eval_metrics.to_dict()
        # override the default output from pydantic by calling `to_dict()` of video_object_tracking_eval_metrics
        if self.video_object_tracking_eval_metrics:
            _dict['videoObjectTrackingEvalMetrics'] = self.video_object_tracking_eval_metrics.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of XPSEvaluationMetrics from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "annotationSpecIdToken": obj.get("annotationSpecIdToken"),
            "category": obj.get("category"),
            "evaluatedExampleCount": obj.get("evaluatedExampleCount"),
            "imageClassificationEvalMetrics": XPSClassificationEvaluationMetrics.from_dict(obj["imageClassificationEvalMetrics"]) if obj.get("imageClassificationEvalMetrics") is not None else None,
            "imageObjectDetectionEvalMetrics": XPSImageObjectDetectionEvaluationMetrics.from_dict(obj["imageObjectDetectionEvalMetrics"]) if obj.get("imageObjectDetectionEvalMetrics") is not None else None,
            "imageSegmentationEvalMetrics": XPSImageSegmentationEvaluationMetrics.from_dict(obj["imageSegmentationEvalMetrics"]) if obj.get("imageSegmentationEvalMetrics") is not None else None,
            "label": obj.get("label"),
            "regressionEvalMetrics": XPSRegressionEvaluationMetrics.from_dict(obj["regressionEvalMetrics"]) if obj.get("regressionEvalMetrics") is not None else None,
            "tablesClassificationEvalMetrics": XPSClassificationEvaluationMetrics.from_dict(obj["tablesClassificationEvalMetrics"]) if obj.get("tablesClassificationEvalMetrics") is not None else None,
            "tablesEvalMetrics": XPSTablesEvaluationMetrics.from_dict(obj["tablesEvalMetrics"]) if obj.get("tablesEvalMetrics") is not None else None,
            "textClassificationEvalMetrics": XPSClassificationEvaluationMetrics.from_dict(obj["textClassificationEvalMetrics"]) if obj.get("textClassificationEvalMetrics") is not None else None,
            "textExtractionEvalMetrics": XPSTextExtractionEvaluationMetrics.from_dict(obj["textExtractionEvalMetrics"]) if obj.get("textExtractionEvalMetrics") is not None else None,
            "textSentimentEvalMetrics": XPSTextSentimentEvaluationMetrics.from_dict(obj["textSentimentEvalMetrics"]) if obj.get("textSentimentEvalMetrics") is not None else None,
            "translationEvalMetrics": XPSTranslationEvaluationMetrics.from_dict(obj["translationEvalMetrics"]) if obj.get("translationEvalMetrics") is not None else None,
            "videoActionRecognitionEvalMetrics": XPSVideoActionRecognitionEvaluationMetrics.from_dict(obj["videoActionRecognitionEvalMetrics"]) if obj.get("videoActionRecognitionEvalMetrics") is not None else None,
            "videoClassificationEvalMetrics": XPSClassificationEvaluationMetrics.from_dict(obj["videoClassificationEvalMetrics"]) if obj.get("videoClassificationEvalMetrics") is not None else None,
            "videoObjectTrackingEvalMetrics": XPSVideoObjectTrackingEvaluationMetrics.from_dict(obj["videoObjectTrackingEvalMetrics"]) if obj.get("videoObjectTrackingEvalMetrics") is not None else None
        })
        return _obj


