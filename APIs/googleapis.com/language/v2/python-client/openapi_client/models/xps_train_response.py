# coding: utf-8

"""
    Cloud Natural Language API

    Provides natural language understanding technologies, such as sentiment analysis, entity recognition, entity sentiment analysis, and other text annotations, to developers.

    The version of the OpenAPI document: v2
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBytes, StrictStr
from typing import Any, ClassVar, Dict, List, Optional, Union
from openapi_client.models.xps_evaluation_metrics_set import XPSEvaluationMetricsSet
from openapi_client.models.xps_example_set import XPSExampleSet
from openapi_client.models.xps_image_classification_train_response import XPSImageClassificationTrainResponse
from openapi_client.models.xps_image_object_detection_model_spec import XPSImageObjectDetectionModelSpec
from openapi_client.models.xps_image_segmentation_train_response import XPSImageSegmentationTrainResponse
from openapi_client.models.xps_response_explanation_spec import XPSResponseExplanationSpec
from openapi_client.models.xps_speech_model_spec import XPSSpeechModelSpec
from openapi_client.models.xps_tables_train_response import XPSTablesTrainResponse
from openapi_client.models.xps_text_train_response import XPSTextTrainResponse
from openapi_client.models.xps_translation_train_response import XPSTranslationTrainResponse
from openapi_client.models.xps_video_action_recognition_train_response import XPSVideoActionRecognitionTrainResponse
from openapi_client.models.xps_video_classification_train_response import XPSVideoClassificationTrainResponse
from openapi_client.models.xps_video_object_tracking_train_response import XPSVideoObjectTrackingTrainResponse
from openapi_client.models.xps_vision_error_analysis_config import XPSVisionErrorAnalysisConfig
from typing import Optional, Set
from typing_extensions import Self

class XPSTrainResponse(BaseModel):
    """
    Next ID: 18
    """ # noqa: E501
    deployed_model_size_bytes: Optional[StrictStr] = Field(default=None, description="Estimated model size in bytes once deployed.", alias="deployedModelSizeBytes")
    error_analysis_configs: Optional[List[XPSVisionErrorAnalysisConfig]] = Field(default=None, description="Optional vision model error analysis configuration. The field is set when model error analysis is enabled in the training request. The results of error analysis will be binded together with evaluation results (in the format of AnnotatedExample).", alias="errorAnalysisConfigs")
    evaluated_example_set: Optional[XPSExampleSet] = Field(default=None, alias="evaluatedExampleSet")
    evaluation_metrics_set: Optional[XPSEvaluationMetricsSet] = Field(default=None, alias="evaluationMetricsSet")
    explanation_configs: Optional[List[XPSResponseExplanationSpec]] = Field(default=None, description="VisionExplanationConfig for XAI on test set. Optional for when XAI is enable in training request.", alias="explanationConfigs")
    image_classification_train_resp: Optional[XPSImageClassificationTrainResponse] = Field(default=None, alias="imageClassificationTrainResp")
    image_object_detection_train_resp: Optional[XPSImageObjectDetectionModelSpec] = Field(default=None, alias="imageObjectDetectionTrainResp")
    image_segmentation_train_resp: Optional[XPSImageSegmentationTrainResponse] = Field(default=None, alias="imageSegmentationTrainResp")
    model_token: Optional[Union[StrictBytes, StrictStr]] = Field(default=None, description="Token that represents the trained model. This is considered immutable and is persisted in AutoML. xPS can put their own proto in the byte string, to e.g. point to the model checkpoints. The token is passed to other xPS APIs to refer to the model.", alias="modelToken")
    speech_train_resp: Optional[XPSSpeechModelSpec] = Field(default=None, alias="speechTrainResp")
    tables_train_resp: Optional[XPSTablesTrainResponse] = Field(default=None, alias="tablesTrainResp")
    text_to_speech_train_resp: Optional[Dict[str, Any]] = Field(default=None, description="TextToSpeech train response", alias="textToSpeechTrainResp")
    text_train_resp: Optional[XPSTextTrainResponse] = Field(default=None, alias="textTrainResp")
    translation_train_resp: Optional[XPSTranslationTrainResponse] = Field(default=None, alias="translationTrainResp")
    video_action_recognition_train_resp: Optional[XPSVideoActionRecognitionTrainResponse] = Field(default=None, alias="videoActionRecognitionTrainResp")
    video_classification_train_resp: Optional[XPSVideoClassificationTrainResponse] = Field(default=None, alias="videoClassificationTrainResp")
    video_object_tracking_train_resp: Optional[XPSVideoObjectTrackingTrainResponse] = Field(default=None, alias="videoObjectTrackingTrainResp")
    __properties: ClassVar[List[str]] = ["deployedModelSizeBytes", "errorAnalysisConfigs", "evaluatedExampleSet", "evaluationMetricsSet", "explanationConfigs", "imageClassificationTrainResp", "imageObjectDetectionTrainResp", "imageSegmentationTrainResp", "modelToken", "speechTrainResp", "tablesTrainResp", "textToSpeechTrainResp", "textTrainResp", "translationTrainResp", "videoActionRecognitionTrainResp", "videoClassificationTrainResp", "videoObjectTrackingTrainResp"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of XPSTrainResponse from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in error_analysis_configs (list)
        _items = []
        if self.error_analysis_configs:
            for _item_error_analysis_configs in self.error_analysis_configs:
                if _item_error_analysis_configs:
                    _items.append(_item_error_analysis_configs.to_dict())
            _dict['errorAnalysisConfigs'] = _items
        # override the default output from pydantic by calling `to_dict()` of evaluated_example_set
        if self.evaluated_example_set:
            _dict['evaluatedExampleSet'] = self.evaluated_example_set.to_dict()
        # override the default output from pydantic by calling `to_dict()` of evaluation_metrics_set
        if self.evaluation_metrics_set:
            _dict['evaluationMetricsSet'] = self.evaluation_metrics_set.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in explanation_configs (list)
        _items = []
        if self.explanation_configs:
            for _item_explanation_configs in self.explanation_configs:
                if _item_explanation_configs:
                    _items.append(_item_explanation_configs.to_dict())
            _dict['explanationConfigs'] = _items
        # override the default output from pydantic by calling `to_dict()` of image_classification_train_resp
        if self.image_classification_train_resp:
            _dict['imageClassificationTrainResp'] = self.image_classification_train_resp.to_dict()
        # override the default output from pydantic by calling `to_dict()` of image_object_detection_train_resp
        if self.image_object_detection_train_resp:
            _dict['imageObjectDetectionTrainResp'] = self.image_object_detection_train_resp.to_dict()
        # override the default output from pydantic by calling `to_dict()` of image_segmentation_train_resp
        if self.image_segmentation_train_resp:
            _dict['imageSegmentationTrainResp'] = self.image_segmentation_train_resp.to_dict()
        # override the default output from pydantic by calling `to_dict()` of speech_train_resp
        if self.speech_train_resp:
            _dict['speechTrainResp'] = self.speech_train_resp.to_dict()
        # override the default output from pydantic by calling `to_dict()` of tables_train_resp
        if self.tables_train_resp:
            _dict['tablesTrainResp'] = self.tables_train_resp.to_dict()
        # override the default output from pydantic by calling `to_dict()` of text_train_resp
        if self.text_train_resp:
            _dict['textTrainResp'] = self.text_train_resp.to_dict()
        # override the default output from pydantic by calling `to_dict()` of translation_train_resp
        if self.translation_train_resp:
            _dict['translationTrainResp'] = self.translation_train_resp.to_dict()
        # override the default output from pydantic by calling `to_dict()` of video_action_recognition_train_resp
        if self.video_action_recognition_train_resp:
            _dict['videoActionRecognitionTrainResp'] = self.video_action_recognition_train_resp.to_dict()
        # override the default output from pydantic by calling `to_dict()` of video_classification_train_resp
        if self.video_classification_train_resp:
            _dict['videoClassificationTrainResp'] = self.video_classification_train_resp.to_dict()
        # override the default output from pydantic by calling `to_dict()` of video_object_tracking_train_resp
        if self.video_object_tracking_train_resp:
            _dict['videoObjectTrackingTrainResp'] = self.video_object_tracking_train_resp.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of XPSTrainResponse from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "deployedModelSizeBytes": obj.get("deployedModelSizeBytes"),
            "errorAnalysisConfigs": [XPSVisionErrorAnalysisConfig.from_dict(_item) for _item in obj["errorAnalysisConfigs"]] if obj.get("errorAnalysisConfigs") is not None else None,
            "evaluatedExampleSet": XPSExampleSet.from_dict(obj["evaluatedExampleSet"]) if obj.get("evaluatedExampleSet") is not None else None,
            "evaluationMetricsSet": XPSEvaluationMetricsSet.from_dict(obj["evaluationMetricsSet"]) if obj.get("evaluationMetricsSet") is not None else None,
            "explanationConfigs": [XPSResponseExplanationSpec.from_dict(_item) for _item in obj["explanationConfigs"]] if obj.get("explanationConfigs") is not None else None,
            "imageClassificationTrainResp": XPSImageClassificationTrainResponse.from_dict(obj["imageClassificationTrainResp"]) if obj.get("imageClassificationTrainResp") is not None else None,
            "imageObjectDetectionTrainResp": XPSImageObjectDetectionModelSpec.from_dict(obj["imageObjectDetectionTrainResp"]) if obj.get("imageObjectDetectionTrainResp") is not None else None,
            "imageSegmentationTrainResp": XPSImageSegmentationTrainResponse.from_dict(obj["imageSegmentationTrainResp"]) if obj.get("imageSegmentationTrainResp") is not None else None,
            "modelToken": obj.get("modelToken"),
            "speechTrainResp": XPSSpeechModelSpec.from_dict(obj["speechTrainResp"]) if obj.get("speechTrainResp") is not None else None,
            "tablesTrainResp": XPSTablesTrainResponse.from_dict(obj["tablesTrainResp"]) if obj.get("tablesTrainResp") is not None else None,
            "textToSpeechTrainResp": obj.get("textToSpeechTrainResp"),
            "textTrainResp": XPSTextTrainResponse.from_dict(obj["textTrainResp"]) if obj.get("textTrainResp") is not None else None,
            "translationTrainResp": XPSTranslationTrainResponse.from_dict(obj["translationTrainResp"]) if obj.get("translationTrainResp") is not None else None,
            "videoActionRecognitionTrainResp": XPSVideoActionRecognitionTrainResponse.from_dict(obj["videoActionRecognitionTrainResp"]) if obj.get("videoActionRecognitionTrainResp") is not None else None,
            "videoClassificationTrainResp": XPSVideoClassificationTrainResponse.from_dict(obj["videoClassificationTrainResp"]) if obj.get("videoClassificationTrainResp") is not None else None,
            "videoObjectTrackingTrainResp": XPSVideoObjectTrackingTrainResponse.from_dict(obj["videoObjectTrackingTrainResp"]) if obj.get("videoObjectTrackingTrainResp") is not None else None
        })
        return _obj


