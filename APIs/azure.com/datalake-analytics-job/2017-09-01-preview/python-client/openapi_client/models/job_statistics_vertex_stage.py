# coding: utf-8

"""
    DataLakeAnalyticsJobManagementClient

    Creates an Azure Data Lake Analytics job client.

    The version of the OpenAPI document: 2017-09-01-preview
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.job_statistics_vertex import JobStatisticsVertex
from openapi_client.models.resource_usage_statistics import ResourceUsageStatistics
from typing import Optional, Set
from typing_extensions import Self

class JobStatisticsVertexStage(BaseModel):
    """
    The Data Lake Analytics job statistics vertex stage information.
    """ # noqa: E501
    allocated_container_cpu_core_count: Optional[ResourceUsageStatistics] = Field(default=None, alias="allocatedContainerCpuCoreCount")
    allocated_container_mem_size: Optional[ResourceUsageStatistics] = Field(default=None, alias="allocatedContainerMemSize")
    data_read: Optional[StrictInt] = Field(default=None, description="The amount of data read, in bytes.", alias="dataRead")
    data_read_cross_pod: Optional[StrictInt] = Field(default=None, description="The amount of data read across multiple pods, in bytes.", alias="dataReadCrossPod")
    data_read_intra_pod: Optional[StrictInt] = Field(default=None, description="The amount of data read in one pod, in bytes.", alias="dataReadIntraPod")
    data_to_read: Optional[StrictInt] = Field(default=None, description="The amount of data remaining to be read, in bytes.", alias="dataToRead")
    data_written: Optional[StrictInt] = Field(default=None, description="The amount of data written, in bytes.", alias="dataWritten")
    duplicate_discard_count: Optional[StrictInt] = Field(default=None, description="The number of duplicates that were discarded.", alias="duplicateDiscardCount")
    estimated_vertex_cpu_core_count: Optional[StrictInt] = Field(default=None, description="The estimated vertex CPU core count.", alias="estimatedVertexCpuCoreCount")
    estimated_vertex_mem_size: Optional[StrictInt] = Field(default=None, description="The estimated vertex memory size, in bytes.", alias="estimatedVertexMemSize")
    estimated_vertex_peak_cpu_core_count: Optional[StrictInt] = Field(default=None, description="The estimated vertex peak CPU core count.", alias="estimatedVertexPeakCpuCoreCount")
    failed_count: Optional[StrictInt] = Field(default=None, description="The number of failures that occurred in this stage.", alias="failedCount")
    max_data_read_vertex: Optional[JobStatisticsVertex] = Field(default=None, alias="maxDataReadVertex")
    max_execution_time_vertex: Optional[JobStatisticsVertex] = Field(default=None, alias="maxExecutionTimeVertex")
    max_peak_mem_usage_vertex: Optional[JobStatisticsVertex] = Field(default=None, alias="maxPeakMemUsageVertex")
    max_vertex_data_read: Optional[StrictInt] = Field(default=None, description="The maximum amount of data read in a single vertex, in bytes.", alias="maxVertexDataRead")
    min_vertex_data_read: Optional[StrictInt] = Field(default=None, description="The minimum amount of data read in a single vertex, in bytes.", alias="minVertexDataRead")
    read_failure_count: Optional[StrictInt] = Field(default=None, description="The number of read failures in this stage.", alias="readFailureCount")
    revocation_count: Optional[StrictInt] = Field(default=None, description="The number of vertices that were revoked during this stage.", alias="revocationCount")
    running_count: Optional[StrictInt] = Field(default=None, description="The number of currently running vertices in this stage.", alias="runningCount")
    scheduled_count: Optional[StrictInt] = Field(default=None, description="The number of currently scheduled vertices in this stage.", alias="scheduledCount")
    stage_name: Optional[StrictStr] = Field(default=None, description="The name of this stage in job execution.", alias="stageName")
    succeeded_count: Optional[StrictInt] = Field(default=None, description="The number of vertices that succeeded in this stage.", alias="succeededCount")
    temp_data_written: Optional[StrictInt] = Field(default=None, description="The amount of temporary data written, in bytes.", alias="tempDataWritten")
    total_count: Optional[StrictInt] = Field(default=None, description="The total vertex count for this stage.", alias="totalCount")
    total_execution_time: Optional[StrictStr] = Field(default=None, description="The sum of the total execution time of all the vertices in the stage.", alias="totalExecutionTime")
    total_failed_time: Optional[StrictStr] = Field(default=None, description="The amount of time that failed vertices took up in this stage.", alias="totalFailedTime")
    total_peak_mem_usage: Optional[StrictInt] = Field(default=None, description="The sum of the peak memory usage of all the vertices in the stage, in bytes.", alias="totalPeakMemUsage")
    total_progress: Optional[StrictInt] = Field(default=None, description="The current progress of this stage, as a percentage.", alias="totalProgress")
    total_succeeded_time: Optional[StrictStr] = Field(default=None, description="The amount of time all successful vertices took in this stage.", alias="totalSucceededTime")
    used_vertex_cpu_core_count: Optional[ResourceUsageStatistics] = Field(default=None, alias="usedVertexCpuCoreCount")
    used_vertex_peak_mem_size: Optional[ResourceUsageStatistics] = Field(default=None, alias="usedVertexPeakMemSize")
    __properties: ClassVar[List[str]] = ["allocatedContainerCpuCoreCount", "allocatedContainerMemSize", "dataRead", "dataReadCrossPod", "dataReadIntraPod", "dataToRead", "dataWritten", "duplicateDiscardCount", "estimatedVertexCpuCoreCount", "estimatedVertexMemSize", "estimatedVertexPeakCpuCoreCount", "failedCount", "maxDataReadVertex", "maxExecutionTimeVertex", "maxPeakMemUsageVertex", "maxVertexDataRead", "minVertexDataRead", "readFailureCount", "revocationCount", "runningCount", "scheduledCount", "stageName", "succeededCount", "tempDataWritten", "totalCount", "totalExecutionTime", "totalFailedTime", "totalPeakMemUsage", "totalProgress", "totalSucceededTime", "usedVertexCpuCoreCount", "usedVertexPeakMemSize"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of JobStatisticsVertexStage from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set([
            "data_read",
            "data_read_cross_pod",
            "data_read_intra_pod",
            "data_to_read",
            "data_written",
            "duplicate_discard_count",
            "estimated_vertex_cpu_core_count",
            "estimated_vertex_mem_size",
            "estimated_vertex_peak_cpu_core_count",
            "failed_count",
            "max_vertex_data_read",
            "min_vertex_data_read",
            "read_failure_count",
            "revocation_count",
            "running_count",
            "scheduled_count",
            "stage_name",
            "succeeded_count",
            "temp_data_written",
            "total_count",
            "total_execution_time",
            "total_failed_time",
            "total_peak_mem_usage",
            "total_progress",
            "total_succeeded_time",
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of allocated_container_cpu_core_count
        if self.allocated_container_cpu_core_count:
            _dict['allocatedContainerCpuCoreCount'] = self.allocated_container_cpu_core_count.to_dict()
        # override the default output from pydantic by calling `to_dict()` of allocated_container_mem_size
        if self.allocated_container_mem_size:
            _dict['allocatedContainerMemSize'] = self.allocated_container_mem_size.to_dict()
        # override the default output from pydantic by calling `to_dict()` of max_data_read_vertex
        if self.max_data_read_vertex:
            _dict['maxDataReadVertex'] = self.max_data_read_vertex.to_dict()
        # override the default output from pydantic by calling `to_dict()` of max_execution_time_vertex
        if self.max_execution_time_vertex:
            _dict['maxExecutionTimeVertex'] = self.max_execution_time_vertex.to_dict()
        # override the default output from pydantic by calling `to_dict()` of max_peak_mem_usage_vertex
        if self.max_peak_mem_usage_vertex:
            _dict['maxPeakMemUsageVertex'] = self.max_peak_mem_usage_vertex.to_dict()
        # override the default output from pydantic by calling `to_dict()` of used_vertex_cpu_core_count
        if self.used_vertex_cpu_core_count:
            _dict['usedVertexCpuCoreCount'] = self.used_vertex_cpu_core_count.to_dict()
        # override the default output from pydantic by calling `to_dict()` of used_vertex_peak_mem_size
        if self.used_vertex_peak_mem_size:
            _dict['usedVertexPeakMemSize'] = self.used_vertex_peak_mem_size.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of JobStatisticsVertexStage from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "allocatedContainerCpuCoreCount": ResourceUsageStatistics.from_dict(obj["allocatedContainerCpuCoreCount"]) if obj.get("allocatedContainerCpuCoreCount") is not None else None,
            "allocatedContainerMemSize": ResourceUsageStatistics.from_dict(obj["allocatedContainerMemSize"]) if obj.get("allocatedContainerMemSize") is not None else None,
            "dataRead": obj.get("dataRead"),
            "dataReadCrossPod": obj.get("dataReadCrossPod"),
            "dataReadIntraPod": obj.get("dataReadIntraPod"),
            "dataToRead": obj.get("dataToRead"),
            "dataWritten": obj.get("dataWritten"),
            "duplicateDiscardCount": obj.get("duplicateDiscardCount"),
            "estimatedVertexCpuCoreCount": obj.get("estimatedVertexCpuCoreCount"),
            "estimatedVertexMemSize": obj.get("estimatedVertexMemSize"),
            "estimatedVertexPeakCpuCoreCount": obj.get("estimatedVertexPeakCpuCoreCount"),
            "failedCount": obj.get("failedCount"),
            "maxDataReadVertex": JobStatisticsVertex.from_dict(obj["maxDataReadVertex"]) if obj.get("maxDataReadVertex") is not None else None,
            "maxExecutionTimeVertex": JobStatisticsVertex.from_dict(obj["maxExecutionTimeVertex"]) if obj.get("maxExecutionTimeVertex") is not None else None,
            "maxPeakMemUsageVertex": JobStatisticsVertex.from_dict(obj["maxPeakMemUsageVertex"]) if obj.get("maxPeakMemUsageVertex") is not None else None,
            "maxVertexDataRead": obj.get("maxVertexDataRead"),
            "minVertexDataRead": obj.get("minVertexDataRead"),
            "readFailureCount": obj.get("readFailureCount"),
            "revocationCount": obj.get("revocationCount"),
            "runningCount": obj.get("runningCount"),
            "scheduledCount": obj.get("scheduledCount"),
            "stageName": obj.get("stageName"),
            "succeededCount": obj.get("succeededCount"),
            "tempDataWritten": obj.get("tempDataWritten"),
            "totalCount": obj.get("totalCount"),
            "totalExecutionTime": obj.get("totalExecutionTime"),
            "totalFailedTime": obj.get("totalFailedTime"),
            "totalPeakMemUsage": obj.get("totalPeakMemUsage"),
            "totalProgress": obj.get("totalProgress"),
            "totalSucceededTime": obj.get("totalSucceededTime"),
            "usedVertexCpuCoreCount": ResourceUsageStatistics.from_dict(obj["usedVertexCpuCoreCount"]) if obj.get("usedVertexCpuCoreCount") is not None else None,
            "usedVertexPeakMemSize": ResourceUsageStatistics.from_dict(obj["usedVertexPeakMemSize"]) if obj.get("usedVertexPeakMemSize") is not None else None
        })
        return _obj


