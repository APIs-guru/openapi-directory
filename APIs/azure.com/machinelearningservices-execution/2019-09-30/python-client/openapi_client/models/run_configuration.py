# coding: utf-8

"""
    Execution Service

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: 2019-09-30
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictInt, StrictStr, field_validator
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.data_reference_configuration import DataReferenceConfiguration
from openapi_client.models.environment_definition import EnvironmentDefinition
from openapi_client.models.hdi_configuration import HdiConfiguration
from openapi_client.models.history_configuration import HistoryConfiguration
from openapi_client.models.mpi_configuration import MpiConfiguration
from openapi_client.models.spark_configuration import SparkConfiguration
from openapi_client.models.tensorflow_configuration import TensorflowConfiguration
from typing import Optional, Set
from typing_extensions import Self

class RunConfiguration(BaseModel):
    """
    RunConfiguration
    """ # noqa: E501
    arguments: Optional[List[StrictStr]] = Field(default=None, description="Command line arguments for the python script file.")
    communicator: Optional[StrictStr] = Field(default=None, description="The supported communicators are None, ParameterServer, OpenMpi, and IntelMpi Keep in mind that OpenMpi requires a custom image with OpenMpi installed.  Use ParameterServer or OpenMpi for AmlCompute clusters. Use IntelMpi for distributed training jobs.")
    data_references: Optional[Dict[str, DataReferenceConfiguration]] = Field(default=None, description="All the data sources are made available to the run during execution based on each configuration.", alias="dataReferences")
    environment: Optional[EnvironmentDefinition] = None
    framework: Optional[StrictStr] = Field(default=None, description="The supported frameworks are Python, PySpark, CNTK, TensorFlow, and PyTorch. Use Tensorflow for AmlCompute clusters, and Python for distributed training jobs.")
    hdi: Optional[HdiConfiguration] = None
    history: Optional[HistoryConfiguration] = None
    job_name: Optional[StrictStr] = Field(default=None, description="This is primarily intended for notebooks to override the default job name.  Defaults to ArgumentVector[0] if not specified.", alias="jobName")
    max_run_duration_seconds: Optional[StrictInt] = Field(default=None, description="Maximum allowed time for the run. The system will attempt to automatically cancel the run if it took longer than this value.  MaxRunDurationSeconds=null means infinite duration.", alias="maxRunDurationSeconds")
    mpi: Optional[MpiConfiguration] = None
    node_count: Optional[StrictInt] = Field(default=None, description="Number of compute nodes to run the job on. Only applies to AMLCompute.", alias="nodeCount")
    script: Optional[StrictStr] = Field(default=None, description="The relative path to the python script file. The file path is relative to the source_directory passed to submit run.")
    spark: Optional[SparkConfiguration] = None
    target: Optional[StrictStr] = Field(default=None, description="Target refers to compute where the job is scheduled for execution. The default target is \"local\" referring to the local machine.")
    tensorflow: Optional[TensorflowConfiguration] = None
    __properties: ClassVar[List[str]] = ["arguments", "communicator", "dataReferences", "environment", "framework", "hdi", "history", "jobName", "maxRunDurationSeconds", "mpi", "nodeCount", "script", "spark", "target", "tensorflow"]

    @field_validator('communicator')
    def communicator_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['None', 'ParameterServer', 'Gloo', 'Mpi', 'Nccl']):
            raise ValueError("must be one of enum values ('None', 'ParameterServer', 'Gloo', 'Mpi', 'Nccl')")
        return value

    @field_validator('framework')
    def framework_validate_enum(cls, value):
        """Validates the enum"""
        if value is None:
            return value

        if value not in set(['Python', 'PySpark', 'Cntk', 'TensorFlow', 'PyTorch']):
            raise ValueError("must be one of enum values ('Python', 'PySpark', 'Cntk', 'TensorFlow', 'PyTorch')")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of RunConfiguration from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each value in data_references (dict)
        _field_dict = {}
        if self.data_references:
            for _key_data_references in self.data_references:
                if self.data_references[_key_data_references]:
                    _field_dict[_key_data_references] = self.data_references[_key_data_references].to_dict()
            _dict['dataReferences'] = _field_dict
        # override the default output from pydantic by calling `to_dict()` of environment
        if self.environment:
            _dict['environment'] = self.environment.to_dict()
        # override the default output from pydantic by calling `to_dict()` of hdi
        if self.hdi:
            _dict['hdi'] = self.hdi.to_dict()
        # override the default output from pydantic by calling `to_dict()` of history
        if self.history:
            _dict['history'] = self.history.to_dict()
        # override the default output from pydantic by calling `to_dict()` of mpi
        if self.mpi:
            _dict['mpi'] = self.mpi.to_dict()
        # override the default output from pydantic by calling `to_dict()` of spark
        if self.spark:
            _dict['spark'] = self.spark.to_dict()
        # override the default output from pydantic by calling `to_dict()` of tensorflow
        if self.tensorflow:
            _dict['tensorflow'] = self.tensorflow.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of RunConfiguration from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "arguments": obj.get("arguments"),
            "communicator": obj.get("communicator"),
            "dataReferences": dict(
                (_k, DataReferenceConfiguration.from_dict(_v))
                for _k, _v in obj["dataReferences"].items()
            )
            if obj.get("dataReferences") is not None
            else None,
            "environment": EnvironmentDefinition.from_dict(obj["environment"]) if obj.get("environment") is not None else None,
            "framework": obj.get("framework"),
            "hdi": HdiConfiguration.from_dict(obj["hdi"]) if obj.get("hdi") is not None else None,
            "history": HistoryConfiguration.from_dict(obj["history"]) if obj.get("history") is not None else None,
            "jobName": obj.get("jobName"),
            "maxRunDurationSeconds": obj.get("maxRunDurationSeconds"),
            "mpi": MpiConfiguration.from_dict(obj["mpi"]) if obj.get("mpi") is not None else None,
            "nodeCount": obj.get("nodeCount"),
            "script": obj.get("script"),
            "spark": SparkConfiguration.from_dict(obj["spark"]) if obj.get("spark") is not None else None,
            "target": obj.get("target"),
            "tensorflow": TensorflowConfiguration.from_dict(obj["tensorflow"]) if obj.get("tensorflow") is not None else None
        })
        return _obj


