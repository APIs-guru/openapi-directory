# coding: utf-8

"""
    BatchService

    A client for issuing REST requests to the Azure Batch service.

    The version of the OpenAPI document: 2019-08-01.10.0
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictBool, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from openapi_client.models.environment_setting import EnvironmentSetting
from openapi_client.models.resource_file import ResourceFile
from openapi_client.models.task_constraints import TaskConstraints
from openapi_client.models.task_container_settings import TaskContainerSettings
from openapi_client.models.user_identity import UserIdentity
from typing import Optional, Set
from typing_extensions import Self

class JobPreparationTask(BaseModel):
    """
    You can use Job Preparation to prepare a Node to run Tasks for the Job. Activities commonly performed in Job Preparation include: Downloading common resource files used by all the Tasks in the Job. The Job Preparation Task can download these common resource files to the shared location on the Node. (AZ_BATCH_NODE_ROOT_DIR\\shared), or starting a local service on the Node so that all Tasks of that Job can communicate with it. If the Job Preparation Task fails (that is, exhausts its retry count before exiting with exit code 0), Batch will not run Tasks of this Job on the Node. The Compute Node remains ineligible to run Tasks of this Job until it is reimaged. The Compute Node remains active and can be used for other Jobs. The Job Preparation Task can run multiple times on the same Node. Therefore, you should write the Job Preparation Task to handle re-execution. If the Node is rebooted, the Job Preparation Task is run again on the Compute Node before scheduling any other Task of the Job, if rerunOnNodeRebootAfterSuccess is true or if the Job Preparation Task did not previously complete. If the Node is reimaged, the Job Preparation Task is run again before scheduling any Task of the Job. Batch will retry Tasks when a recovery operation is triggered on a Node. Examples of recovery operations include (but are not limited to) when an unhealthy Node is rebooted or a Compute Node disappeared due to host failure. Retries due to recovery operations are independent of and are not counted against the maxTaskRetryCount. Even if the maxTaskRetryCount is 0, an internal retry due to a recovery operation may occur. Because of this, all Tasks should be idempotent. This means Tasks need to tolerate being interrupted and restarted without causing any corruption or duplicate data. The best practice for long running Tasks is to use some form of checkpointing.
    """ # noqa: E501
    command_line: StrictStr = Field(description="The command line does not run under a shell, and therefore cannot take advantage of shell features such as environment variable expansion. If you want to take advantage of such features, you should invoke the shell in the command line, for example using \"cmd /c MyCommand\" in Windows or \"/bin/sh -c MyCommand\" in Linux. If the command line refers to file paths, it should use a relative path (relative to the Task working directory), or use the Batch provided environment variable (https://docs.microsoft.com/en-us/azure/batch/batch-compute-node-environment-variables).", alias="commandLine")
    constraints: Optional[TaskConstraints] = None
    container_settings: Optional[TaskContainerSettings] = Field(default=None, alias="containerSettings")
    environment_settings: Optional[List[EnvironmentSetting]] = Field(default=None, alias="environmentSettings")
    id: Optional[StrictStr] = Field(default=None, description="The ID can contain any combination of alphanumeric characters including hyphens and underscores and cannot contain more than 64 characters. If you do not specify this property, the Batch service assigns a default value of 'jobpreparation'. No other Task in the Job can have the same ID as the Job Preparation Task. If you try to submit a Task with the same id, the Batch service rejects the request with error code TaskIdSameAsJobPreparationTask; if you are calling the REST API directly, the HTTP status code is 409 (Conflict).")
    rerun_on_node_reboot_after_success: Optional[StrictBool] = Field(default=None, description="The Job Preparation Task is always rerun if a Compute Node is reimaged, or if the Job Preparation Task did not complete (e.g. because the reboot occurred while the Task was running). Therefore, you should always write a Job Preparation Task to be idempotent and to behave correctly if run multiple times. The default value is true.", alias="rerunOnNodeRebootAfterSuccess")
    resource_files: Optional[List[ResourceFile]] = Field(default=None, description="Files listed under this element are located in the Task's working directory.  There is a maximum size for the list of resource files.  When the max size is exceeded, the request will fail and the response error code will be RequestEntityTooLarge. If this occurs, the collection of ResourceFiles must be reduced in size. This can be achieved using .zip files, Application Packages, or Docker Containers.", alias="resourceFiles")
    user_identity: Optional[UserIdentity] = Field(default=None, alias="userIdentity")
    wait_for_success: Optional[StrictBool] = Field(default=None, description="If true and the Job Preparation Task fails on a Node, the Batch service retries the Job Preparation Task up to its maximum retry count (as specified in the constraints element). If the Task has still not completed successfully after all retries, then the Batch service will not schedule Tasks of the Job to the Node. The Node remains active and eligible to run Tasks of other Jobs. If false, the Batch service will not wait for the Job Preparation Task to complete. In this case, other Tasks of the Job can start executing on the Compute Node while the Job Preparation Task is still running; and even if the Job Preparation Task fails, new Tasks will continue to be scheduled on the Compute Node. The default value is true.", alias="waitForSuccess")
    __properties: ClassVar[List[str]] = ["commandLine", "constraints", "containerSettings", "environmentSettings", "id", "rerunOnNodeRebootAfterSuccess", "resourceFiles", "userIdentity", "waitForSuccess"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of JobPreparationTask from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of constraints
        if self.constraints:
            _dict['constraints'] = self.constraints.to_dict()
        # override the default output from pydantic by calling `to_dict()` of container_settings
        if self.container_settings:
            _dict['containerSettings'] = self.container_settings.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in environment_settings (list)
        _items = []
        if self.environment_settings:
            for _item_environment_settings in self.environment_settings:
                if _item_environment_settings:
                    _items.append(_item_environment_settings.to_dict())
            _dict['environmentSettings'] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in resource_files (list)
        _items = []
        if self.resource_files:
            for _item_resource_files in self.resource_files:
                if _item_resource_files:
                    _items.append(_item_resource_files.to_dict())
            _dict['resourceFiles'] = _items
        # override the default output from pydantic by calling `to_dict()` of user_identity
        if self.user_identity:
            _dict['userIdentity'] = self.user_identity.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of JobPreparationTask from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "commandLine": obj.get("commandLine"),
            "constraints": TaskConstraints.from_dict(obj["constraints"]) if obj.get("constraints") is not None else None,
            "containerSettings": TaskContainerSettings.from_dict(obj["containerSettings"]) if obj.get("containerSettings") is not None else None,
            "environmentSettings": [EnvironmentSetting.from_dict(_item) for _item in obj["environmentSettings"]] if obj.get("environmentSettings") is not None else None,
            "id": obj.get("id"),
            "rerunOnNodeRebootAfterSuccess": obj.get("rerunOnNodeRebootAfterSuccess"),
            "resourceFiles": [ResourceFile.from_dict(_item) for _item in obj["resourceFiles"]] if obj.get("resourceFiles") is not None else None,
            "userIdentity": UserIdentity.from_dict(obj["userIdentity"]) if obj.get("userIdentity") is not None else None,
            "waitForSuccess": obj.get("waitForSuccess")
        })
        return _obj


